{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align:center; font-size:45px; color: teal; letter-spacing: .1em;\">\n    MOVIES RECOMMENDATION SYSTEMS\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='introduction'>\n    <h2> INTRODUCTION </h2>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"Intelligent algorithms can help viewers sift through tens of thousands of titles to find the best ones. Recommender systems are both socially and economically important in ensuring that people can make informed decisions about the content they consume on a daily basis. This is particularly true in the case of movie recommendations.\n\nProviding an accurate and robust solution to this challenge has enormous economic potential, with users of the system receiving personalized recommendations, thereby enhancing platform affinity for the streaming services that best facilitate their audience's viewing.","metadata":{}},{"cell_type":"markdown","source":"### RECOMMENDER SYSTEM","metadata":{}},{"cell_type":"markdown","source":"A recommendation system is an information filtering system whose main goal is to predict the rating or preference a user might give to an item. This helps create personalized content and better product search experience. One popular use is recommending to users which movie to watch. This is because significant dependencies exist between users and item centric activity. For example a user who is interested in s historical documentary is more likely to be interested in another historical documentary or an educational program, rather than in an action movie.\n\nA recommendation system can use either of these two techniques:\n\n- Content based filtering\n- Collaborative filtering\n\nIn content based filtering, the algorithm seeks to make recommendations based on how similar the properties or features of an item are to other items.\n\nIn collaborative filtering, we use similarities between users and items simultaneously to provide recommendations. This allows for serendipitous recommendations; that is, collaborative filtering models can recommend an item to user A based on the interests of a similar user B.\n\nHere we are going to explore both methods and assess which recommendation system gives us the best results. Increasing sales is the primary goal of a recommender system. By recommending carefully selected items to users, recommender systems bring relevant items to the attention of users. This increases the sales volumes and profits to the merchants.","metadata":{}},{"cell_type":"markdown","source":"<div id='tbl-contents'>\n    <h2>TABLE OF CONTENTS</h2>\n    <ol>\n        <li>\n            <h4>\n                <a href='#introduction'>Introduction</a>\n            </h4>\n        </li>\n        <li>\n            <h4>\n                <a href='#load-dependencies'>Load Dependencies</a>\n            </h4>\n        </li>\n        <li>\n            <h4>\n                <a href='#load-data'>Load Data</a>\n            </h4>\n        </li> \n        <li>\n            <h4>\n                <a href='#sneak-peek'>Sneak Peek into Loaded Data</a>\n            </h4>\n        </li> \n        <li>\n            <h4>\n                <a href='#data-cleaning'>Data Cleaning</a>\n            </h4>\n        </li> \n        <li>\n            <h4>\n                <a href='#eda'>Exploratory Data Analysis</a>\n            </h4>\n        </li>  \n        <li>\n            <h4>\n                <a href='#FE'>Feature Engineering</a>\n            </h4>\n        </li>  \n        <li>\n            <h4>\n                <a href='#recommender'>Recommender Systems</a>\n            </h4>\n        </li>  \n    </ol>\n</div>\n\n","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='load-dependencies'>\n    <h2>LOAD DEPENDENCIES</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"import os # for os operations on kaggle\n\n# for pattern searching and extraction \nimport re\n\n# libraries for data analysis and manipulation\nimport pandas as pd\nimport numpy as np\n\n# libraries for numerical efficiencies\nimport scipy as sp\nfrom scipy import stats\n\n# libraries for string matching\nfrom fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process\n\n# libraries for data visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import figure\nfrom wordcloud import WordCloud, STOPWORDS\n\n# library to evaluate strings containing python literals\nfrom ast import literal_eval\n\n# libraries for natural language processing\nfrom nltk.corpus import wordnet\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# libraries for building and analyzing recommender systems that deal with explicit rating data.\nfrom surprise import Reader, Dataset, SVD\nfrom surprise import KNNBasic, BaselineOnly, NMF\n\n# libraries for entity featurization and similarity computation\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom surprise.model_selection import cross_validate, train_test_split\n\n# to ignore whatever warnings that may arise\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (15, 10) # defaulting all plots to a fixed size\nsns.set(rc={'figure.figsize':(15,10)})\nplt.style.use('ggplot')\nsns.set_palette(\"Blues_r\")\n# sns.set_palette(sns.dark_palette(\"#69d\"))\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"id":"jJoKzyWPDpVd","execution":{"iopub.status.busy":"2022-05-04T20:34:30.072390Z","iopub.execute_input":"2022-05-04T20:34:30.072646Z","iopub.status.idle":"2022-05-04T20:34:30.100371Z","shell.execute_reply.started":"2022-05-04T20:34:30.072614Z","shell.execute_reply":"2022-05-04T20:34:30.099670Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='load-data'>\n    <h2>LOAD DATASETS</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"def convert_columns(data):\n    \"\"\"\n    This function takes in a dataset and converts the \n    dtype of each column to a lesser version to reduce\n    the size of the dataset for further operations.\n    \"\"\"\n    \n    for col in data.columns: # iterate over the columns in the dataset\n        \n        if data[col].dtype == 'object':\n            data[col] = data[col].astype('category') # convert objects to categories\n        \n        if data[col].dtype == 'int64':\n            data[col] = data[col].astype('int32') # convert int64 to int32\n        \n        if data[col].dtype == 'float64':\n            data[col] = data[col].astype('float32') # convert float64 to float32\n        \n    return data # return converted data\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:34:34.292668Z","iopub.execute_input":"2022-05-04T20:34:34.293022Z","iopub.status.idle":"2022-05-04T20:34:34.301933Z","shell.execute_reply.started":"2022-05-04T20:34:34.292981Z","shell.execute_reply":"2022-05-04T20:34:34.300947Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Loading datasets. Change paths when working on a different platform or personal computer\n\nimdb = pd.read_csv('/kaggle/input/movie-recommendation-data/imdb_data.csv')\nmovies = pd.read_csv('/kaggle/input/movie-recommendation-data/movies.csv')\nmeta_data = pd.read_csv('/kaggle/input/movies-metadata/movies_metadata.csv')\ngenome_scores = pd.read_csv('/kaggle/input/movie-recommendation-data/genome_scores.csv')\ngenome_scores = convert_columns(genome_scores)\ngenome_tags = pd.read_csv('/kaggle/input/movie-recommendation-data/genome_tags.csv')\ngenome_tags= convert_columns(genome_tags)\ntrain = pd.read_csv('/kaggle/input/movie-recommendation-data/train.csv')\ntrain = convert_columns(train)\ntest = pd.read_csv('/kaggle/input/movie-recommendation-data/test.csv')\ntest = convert_columns(test)\nlinks = pd.read_csv('/kaggle/input/movie-recommendation-data/links.csv')\nlinks = convert_columns(links)\ntags = pd.read_csv('/kaggle/input/movie-recommendation-data/tags.csv')\ntags = convert_columns(tags)\nscores_n_tags = pd.read_csv('/kaggle/input/genome-scores-and-tags/scores_tags.csv')\nsample_submission = pd.read_csv('/kaggle/input/movie-recommendation-data/sample_submission.csv')","metadata":{"id":"AX_ZbyJ6DyQG","execution":{"iopub.status.busy":"2022-05-04T20:34:39.721174Z","iopub.execute_input":"2022-05-04T20:34:39.721514Z","iopub.status.idle":"2022-05-04T20:35:09.536426Z","shell.execute_reply.started":"2022-05-04T20:34:39.721480Z","shell.execute_reply":"2022-05-04T20:35:09.535432Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='sneak-peek'>\n    <h2 style='text-transform: uppercase;'>Sneak Peak into Loaded Data</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### 1. IMDB DATA","metadata":{}},{"cell_type":"code","source":"# imdb data\nimdb.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.669242Z","iopub.execute_input":"2022-05-04T19:14:52.669586Z","iopub.status.idle":"2022-05-04T19:14:52.693792Z","shell.execute_reply.started":"2022-05-04T19:14:52.669547Z","shell.execute_reply":"2022-05-04T19:14:52.692811Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"From the above output, we see that the IMDB Data is made up of __6__ columns - __movieId__, __title_cast__, __director__, __runtime__, __budget__, __plot_keywords__.\n\nWe can observe that the __title_cast__ and __plot_keywords__ columns are separated by a pipe - '|'. This makes each row in these columns seem to be one(1) long complicated word, which will make further analysis difficult. We will treat this problem in the `Data Cleaning` section.\n\nAlso, data in the __budget__ column, which is meant to be a numerical column, are prepended with currency symbols and separated by commas(,). This is bad format and needs to be taken care of in the `Data Cleaning` section.","metadata":{}},{"cell_type":"code","source":"# get the dimensions of the data\nimdb.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.695292Z","iopub.execute_input":"2022-05-04T19:14:52.695609Z","iopub.status.idle":"2022-05-04T19:14:52.702620Z","shell.execute_reply.started":"2022-05-04T19:14:52.695563Z","shell.execute_reply":"2022-05-04T19:14:52.701588Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"IMDB data is made up of __27,278__ rows and __6__ columns.\n\nHow about some information about the data? ","metadata":{}},{"cell_type":"code","source":"imdb.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.704106Z","iopub.execute_input":"2022-05-04T19:14:52.704464Z","iopub.status.idle":"2022-05-04T19:14:52.739300Z","shell.execute_reply.started":"2022-05-04T19:14:52.704426Z","shell.execute_reply":"2022-05-04T19:14:52.738316Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"The IMDB data has __1__ column of dtype `float64` - __runtime__, __1__ column of dtype `int64` - __movieId__ and __4__ columns of dtype `object` - __title_cast, director, budget & plot_keywords__. \n\nThe __budget__ column is meant to be numerical to aid aggregation. This will be taken care of in the `Data Cleaning` section.\n\nWe also have a case of missing data in all columns bar __movieId__. Let's see by how much.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\nimdb_missing_data = pd.concat([imdb.isnull().sum(), round(imdb.isnull().sum()/imdb.shape[0] * 100)], axis=1)\nimdb_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\nimdb_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.740625Z","iopub.execute_input":"2022-05-04T19:14:52.741347Z","iopub.status.idle":"2022-05-04T19:14:52.777049Z","shell.execute_reply.started":"2022-05-04T19:14:52.741307Z","shell.execute_reply":"2022-05-04T19:14:52.776337Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"`Budget` has the highest missing data with __19,372__! rows of missing data, making up __71%__! of the entire column - that is a huge amount!.\n\n`Runtime` has the second highest missing values at __44%__, followed closely by `plot_keywords` at __41%__. `title_cast` and `director` also record missing values at __37%__ and __36%__ respectively.\n\nThese look like a lot of missing data and have to come up with creative ways to deal with this problem in `Feature Engineering` section.","metadata":{}},{"cell_type":"markdown","source":"Next, Movies data","metadata":{}},{"cell_type":"markdown","source":"#### 2. MOVIES DATA","metadata":{}},{"cell_type":"code","source":"# movies data\nmovies.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.778172Z","iopub.execute_input":"2022-05-04T19:14:52.778815Z","iopub.status.idle":"2022-05-04T19:14:52.788637Z","shell.execute_reply.started":"2022-05-04T19:14:52.778781Z","shell.execute_reply":"2022-05-04T19:14:52.787615Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that the Movies data is made up of __3__ columns - __movieId__, __title__, __genres__.\n\nSimilar to IMDB's __title_cast__ and __plot_keywords__, data in the __genres__ column are separated distinctly by a '|' symbol. As stated earlier, this will need to be taken care of in the `Data Cleaning` section.\n\nThe __title__ column holds both the _title_ of the movie and the _year of release_, like co-joined twins they need to be separated in the theatre of `Feature Engineering`.\n\n\nNext, we will look at the dimensions of the data using `.shape` attribute of a Dataframe","metadata":{}},{"cell_type":"code","source":"# get the dimensions of the data\nmovies.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.790095Z","iopub.execute_input":"2022-05-04T19:14:52.790964Z","iopub.status.idle":"2022-05-04T19:14:52.802805Z","shell.execute_reply.started":"2022-05-04T19:14:52.790884Z","shell.execute_reply":"2022-05-04T19:14:52.801886Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The movies dataset has __62,423__ rows of data and __3__ columns of features.\n\nFine, let's drill down a bit on the data by columns to gain a slightly better understanding using `.info` ","metadata":{}},{"cell_type":"code","source":"# get more information about the data\nmovies.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.806491Z","iopub.execute_input":"2022-05-04T19:14:52.806819Z","iopub.status.idle":"2022-05-04T19:14:52.835236Z","shell.execute_reply.started":"2022-05-04T19:14:52.806772Z","shell.execute_reply":"2022-05-04T19:14:52.834261Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"There are __2__ objects columns - __title__ and __genres__ and __1__ numerical(_int64_) column.\n\nWe can safely say there are no missing values in any of the columns, judging from the shape of the dataset and the number of _non-null count_ for each column.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\nmovies_missing_data = pd.concat([movies.isnull().sum(), round(movies.isnull().sum()/movies.shape[0] * 100)], axis=1)\nmovies_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\nmovies_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.836452Z","iopub.execute_input":"2022-05-04T19:14:52.836817Z","iopub.status.idle":"2022-05-04T19:14:52.882605Z","shell.execute_reply.started":"2022-05-04T19:14:52.836784Z","shell.execute_reply":"2022-05-04T19:14:52.881671Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Our intuition was right afterall...\n\nNext on the list, Meta_data!","metadata":{}},{"cell_type":"markdown","source":"#### 3. META_DATA DATA","metadata":{}},{"cell_type":"code","source":"meta_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.884117Z","iopub.execute_input":"2022-05-04T19:14:52.884610Z","iopub.status.idle":"2022-05-04T19:14:52.918492Z","shell.execute_reply.started":"2022-05-04T19:14:52.884559Z","shell.execute_reply":"2022-05-04T19:14:52.917248Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Wow! the meta_data dataset seem to be a more robust, upgraded and an agglomerated version of the IMDB dataset and Movies dataset with a lot more information about a movie. This will be very useful for our recommendation systems.\n\nA drawback of note is that the meta_data does not have a \"movieId\", while we may use the \"id\" column instead, it doesn't map correctly with \"movieId\" of other datasets. \n\nWhat to do? let's keep that pending while we continue exploring the dataset.\n\nLet's look at the dimensions of the data next.","metadata":{}},{"cell_type":"code","source":"# get the dimension of the data\nmeta_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.920375Z","iopub.execute_input":"2022-05-04T19:14:52.920752Z","iopub.status.idle":"2022-05-04T19:14:52.927409Z","shell.execute_reply.started":"2022-05-04T19:14:52.920700Z","shell.execute_reply":"2022-05-04T19:14:52.926786Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"This dataset has __45,466__ rows and __24__ columns.\n\nMore information please! `.info`","metadata":{}},{"cell_type":"code","source":"# get more information about the dataset\nmeta_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:52.928594Z","iopub.execute_input":"2022-05-04T19:14:52.929031Z","iopub.status.idle":"2022-05-04T19:14:53.042104Z","shell.execute_reply.started":"2022-05-04T19:14:52.928994Z","shell.execute_reply":"2022-05-04T19:14:53.041178Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We have a lot more columns than we do for both `movies` and `IMDB` datasets combined. \n\nQuestion is, are they all useful for what we are trying to achieve? \n\nThere are columns with missing values. It is difficult to know by how much, so let's break it down.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\nmeta_data_missing_data = pd.concat([meta_data.isnull().sum(), round(meta_data.isnull().sum()/meta_data.shape[0] * 100)], axis=1)\nmeta_data_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\nmeta_data_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.043652Z","iopub.execute_input":"2022-05-04T19:14:53.044171Z","iopub.status.idle":"2022-05-04T19:14:53.248167Z","shell.execute_reply.started":"2022-05-04T19:14:53.044126Z","shell.execute_reply":"2022-05-04T19:14:53.247313Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"An astonishing __90%__! of data are missing in the __belongs_to_collection__ column, but that can be very misleading because not all the movies are part of a franchise or collection, meaning they don't have sequels. Also, __budget__ column appears to not have any missing value but from the initial sneak peek, we can see that there are movies with **Zero (0)** budget. This is practically not possible and need to be dealt with.\n\n__homepage__ on the other hand, which has __83%__ of its data missing is not useful to us in the particular context of a recommender system. Therefore, it will be removed during `Feature engineering`\n\n__tagline__, while have approximately half of its data missing may be of value us and cannot be discarded so easily.\n\nHonorable mentions in the missing data category include; __overview__ - __2%__, __poster_path__ - __1%__ and __runtime__ - __1%__.","metadata":{}},{"cell_type":"code","source":"# check for duplicated values\n\nmeta_data.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.249603Z","iopub.execute_input":"2022-05-04T19:14:53.250026Z","iopub.status.idle":"2022-05-04T19:14:53.467323Z","shell.execute_reply.started":"2022-05-04T19:14:53.249981Z","shell.execute_reply":"2022-05-04T19:14:53.466446Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Meta data contains __13__ duplicated rows. We will handle this in `Data Cleaning`\n\nUp Next, Genome_scores.","metadata":{}},{"cell_type":"markdown","source":"#### 4. GENOME SCORES DATA","metadata":{}},{"cell_type":"code","source":"genome_scores.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.468721Z","iopub.execute_input":"2022-05-04T19:14:53.469429Z","iopub.status.idle":"2022-05-04T19:14:53.479436Z","shell.execute_reply.started":"2022-05-04T19:14:53.469385Z","shell.execute_reply":"2022-05-04T19:14:53.478086Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"This dataset is made up of __3__ columns, namely; __movieId__, __tagId__ and __relevance__.\n\nRight now, we can only assume that __relevance__ indicates by how much a tag is of importance to a movie.\n\nLet's look at the shape of the dataset","metadata":{}},{"cell_type":"code","source":"genome_scores.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.480886Z","iopub.execute_input":"2022-05-04T19:14:53.481298Z","iopub.status.idle":"2022-05-04T19:14:53.491796Z","shell.execute_reply.started":"2022-05-04T19:14:53.481245Z","shell.execute_reply":"2022-05-04T19:14:53.491201Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"There are over __15 Million__ rows of data.","metadata":{}},{"cell_type":"code","source":"genome_scores.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.492998Z","iopub.execute_input":"2022-05-04T19:14:53.493244Z","iopub.status.idle":"2022-05-04T19:14:53.509995Z","shell.execute_reply.started":"2022-05-04T19:14:53.493214Z","shell.execute_reply":"2022-05-04T19:14:53.508966Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"There's no information on the number of non-null rows.\n\nTher are __2__ *int32* columns and __1__ *float32* column, indicating it's an all-numeric dataset\n\nLet's see if there are any missing data","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\ngs_missing_data = pd.concat([genome_scores.isnull().sum(), round(genome_scores.isnull().sum()/genome_scores.shape[0] * 100)], axis=1)\ngs_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\ngs_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.511276Z","iopub.execute_input":"2022-05-04T19:14:53.511526Z","iopub.status.idle":"2022-05-04T19:14:53.674751Z","shell.execute_reply.started":"2022-05-04T19:14:53.511496Z","shell.execute_reply":"2022-05-04T19:14:53.673940Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"There are no missing data.\n\nNext, we look at Genome Tags.","metadata":{}},{"cell_type":"markdown","source":"#### 5. GENOME TAGS DATA ","metadata":{}},{"cell_type":"code","source":"genome_tags.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.676156Z","iopub.execute_input":"2022-05-04T19:14:53.676395Z","iopub.status.idle":"2022-05-04T19:14:53.686938Z","shell.execute_reply.started":"2022-05-04T19:14:53.676362Z","shell.execute_reply":"2022-05-04T19:14:53.686147Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"This dataset has __2__ columns; __tagId__ and __tag__. \n\nMovie tags are another way to relate movies to each other.\n\nNext, The dimensions of the dataset","metadata":{}},{"cell_type":"code","source":"genome_tags.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.688287Z","iopub.execute_input":"2022-05-04T19:14:53.688763Z","iopub.status.idle":"2022-05-04T19:14:53.695213Z","shell.execute_reply.started":"2022-05-04T19:14:53.688720Z","shell.execute_reply":"2022-05-04T19:14:53.694551Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"There are __1,128__ rows of data.","metadata":{}},{"cell_type":"code","source":"genome_tags.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.696745Z","iopub.execute_input":"2022-05-04T19:14:53.697275Z","iopub.status.idle":"2022-05-04T19:14:53.720752Z","shell.execute_reply.started":"2022-05-04T19:14:53.697239Z","shell.execute_reply":"2022-05-04T19:14:53.719791Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"The dataset consists of __1__ categorical or text data column, much like 'object' and __1__ numerical column ('int32')\n\nAnd there are no missing values, but let's double check.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\ngt_missing_data = pd.concat([genome_tags.isnull().sum(), round(genome_tags.isnull().sum()/genome_tags.shape[0] * 100)], axis=1)\ngt_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\ngt_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.722229Z","iopub.execute_input":"2022-05-04T19:14:53.722456Z","iopub.status.idle":"2022-05-04T19:14:53.736896Z","shell.execute_reply.started":"2022-05-04T19:14:53.722428Z","shell.execute_reply":"2022-05-04T19:14:53.736256Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Our suspicion was correct afterall. Let's trust our gut feelings next time. :-)\n\nUp next, we will be sneak peaking into the train dataset. stay tuned!","metadata":{}},{"cell_type":"markdown","source":"#### 6. TRAIN DATA","metadata":{}},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.738073Z","iopub.execute_input":"2022-05-04T19:14:53.738893Z","iopub.status.idle":"2022-05-04T19:14:53.755044Z","shell.execute_reply.started":"2022-05-04T19:14:53.738847Z","shell.execute_reply":"2022-05-04T19:14:53.754296Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"At a glance, we see that the train dataset is made up of __4__ columns; __userId__, __movieId__, __rating__ and __timestamp__.\n\nHere we have the rating each user gives a movie and also a timestamp of when such rating occured.\n\nWe will look at the shape of the data next","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.756165Z","iopub.execute_input":"2022-05-04T19:14:53.756705Z","iopub.status.idle":"2022-05-04T19:14:53.768983Z","shell.execute_reply.started":"2022-05-04T19:14:53.756669Z","shell.execute_reply":"2022-05-04T19:14:53.768326Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"The dataset is made up of about __10 Million__ rows of data. Pretty large.\n\nLet's extract more information.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.770256Z","iopub.execute_input":"2022-05-04T19:14:53.770869Z","iopub.status.idle":"2022-05-04T19:14:53.784969Z","shell.execute_reply.started":"2022-05-04T19:14:53.770836Z","shell.execute_reply":"2022-05-04T19:14:53.784207Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"There are __4__ columns, all of which are numerical, consisting of __3__ columns of dtype *int32* and __1__ column of dtype *float32*. I am tempted to say there are no missing values and trust my gut feelings, but just to double check again...","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\ntrain_missing_data = pd.concat([train.isnull().sum(), round(train.isnull().sum()/train.shape[0] * 100)], axis=1)\ntrain_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\ntrain_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.791348Z","iopub.execute_input":"2022-05-04T19:14:53.792019Z","iopub.status.idle":"2022-05-04T19:14:53.918834Z","shell.execute_reply.started":"2022-05-04T19:14:53.791978Z","shell.execute_reply":"2022-05-04T19:14:53.917945Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Sorry gut feelings!","metadata":{}},{"cell_type":"markdown","source":"#### 7. LINKS DATA","metadata":{}},{"cell_type":"code","source":"links.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.920349Z","iopub.execute_input":"2022-05-04T19:14:53.920588Z","iopub.status.idle":"2022-05-04T19:14:53.931467Z","shell.execute_reply.started":"2022-05-04T19:14:53.920557Z","shell.execute_reply":"2022-05-04T19:14:53.930063Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Links, as the name suggests, contains only primary keys (columns with unique identity for each data point)  to other datasets. \n\nIt is made up of **3** columns; __movieId__, __imdbId__ & __tmdbId__","metadata":{}},{"cell_type":"code","source":"links.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.933334Z","iopub.execute_input":"2022-05-04T19:14:53.933599Z","iopub.status.idle":"2022-05-04T19:14:53.944470Z","shell.execute_reply.started":"2022-05-04T19:14:53.933566Z","shell.execute_reply":"2022-05-04T19:14:53.943587Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"There are __62,423__ rows in the dataset","metadata":{}},{"cell_type":"code","source":"links.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.945671Z","iopub.execute_input":"2022-05-04T19:14:53.945962Z","iopub.status.idle":"2022-05-04T19:14:53.962328Z","shell.execute_reply.started":"2022-05-04T19:14:53.945920Z","shell.execute_reply":"2022-05-04T19:14:53.961600Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"All columns are numeric columns; __2__ _int32_ and __1__ _float32_ column(s) respectively. \n\n__tmdbId__ seems to have missing values, let's check by how much","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\nlinks_missing_data = pd.concat([links.isnull().sum(), round(links.isnull().sum()/links.shape[0] * 100, 3)], axis=1)\nlinks_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\nlinks_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.963468Z","iopub.execute_input":"2022-05-04T19:14:53.963951Z","iopub.status.idle":"2022-05-04T19:14:53.982290Z","shell.execute_reply.started":"2022-05-04T19:14:53.963892Z","shell.execute_reply":"2022-05-04T19:14:53.980986Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"The number of missing data here is very negligible at __0.171%__, which I believe won't create significant problems","metadata":{}},{"cell_type":"markdown","source":"For our final show in this section, we will take a sneak peek into the tags dataset","metadata":{}},{"cell_type":"markdown","source":"#### 8. TAGS DATA","metadata":{}},{"cell_type":"code","source":"tags.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.983708Z","iopub.execute_input":"2022-05-04T19:14:53.984118Z","iopub.status.idle":"2022-05-04T19:14:53.995600Z","shell.execute_reply.started":"2022-05-04T19:14:53.984077Z","shell.execute_reply":"2022-05-04T19:14:53.994341Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"This dataset also has __4__ columns; __userId__, __movieId__, __tag__ and __timestamp__.\n\n__tag__ also features here as it did in `genome_tags` dataset. Is there a difference? or are they the same? This we will explore in the `Exploratory Data Analysis` section.\n\nNext, we will take a look at the dimensions of the dataset","metadata":{}},{"cell_type":"code","source":"tags.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:53.997362Z","iopub.execute_input":"2022-05-04T19:14:53.998394Z","iopub.status.idle":"2022-05-04T19:14:54.010033Z","shell.execute_reply.started":"2022-05-04T19:14:53.998258Z","shell.execute_reply":"2022-05-04T19:14:54.009332Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"There are over __1 Million__ rows of data in this dataset.\n\nLet's get some more information.","metadata":{}},{"cell_type":"code","source":"tags.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.012018Z","iopub.execute_input":"2022-05-04T19:14:54.012236Z","iopub.status.idle":"2022-05-04T19:14:54.089613Z","shell.execute_reply.started":"2022-05-04T19:14:54.012210Z","shell.execute_reply":"2022-05-04T19:14:54.088978Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"There are __3__ numerical columns and __1__ categorical column which is the __tag__ column. \n\nThe tag column also seem to be missing some values. Let's confirm this.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\ntags_missing_data = pd.concat([tags.isnull().sum(), round(tags.isnull().sum()/tags.shape[0] * 100, 3)], axis=1)\ntags_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\ntags_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.090756Z","iopub.execute_input":"2022-05-04T19:14:54.091134Z","iopub.status.idle":"2022-05-04T19:14:54.117624Z","shell.execute_reply.started":"2022-05-04T19:14:54.091089Z","shell.execute_reply":"2022-05-04T19:14:54.116810Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"There's a very very negligible number of missing data at __0.001%__. That won't do us much harm.","metadata":{}},{"cell_type":"markdown","source":"And with that, we've come to the end of the `Sneak Peek into Loaded Data Section`. \n\nHere, we had a brief overview of the datasets we intend to work with and what needs to be done to get our data ready for further analysis and modeling.\n\n\nDuring the sneak peeking, we noticed that there are some columns that aren't just right. In the next section, we will be making use of a bunch of techniques to prepare the data into the right and useable formats in a process known as `Data Cleaning`","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='data-cleaning'>\n    <h2 style='text-transform: uppercase;'>data cleaning</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### WHAT IS DATA CLEANING?\n\nThe process of repairing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data from a dataset is known as `Data Cleaning`. There are numerous opportunities for data to be duplicated or mislabeled when combining multiple data sources. If the data is incorrect, the results and algorithms are untrustworthy, even if they appear to be correct.\n\nThis will be done for every datasets we intend to work with.","metadata":{}},{"cell_type":"markdown","source":"#### 1. IMDB DATA","metadata":{}},{"cell_type":"markdown","source":"First, let's make a copy of the dataset","metadata":{}},{"cell_type":"code","source":"# make a copy of the dataset\nimdb_copy = imdb.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.119011Z","iopub.execute_input":"2022-05-04T19:14:54.119256Z","iopub.status.idle":"2022-05-04T19:14:54.124161Z","shell.execute_reply.started":"2022-05-04T19:14:54.119226Z","shell.execute_reply":"2022-05-04T19:14:54.123354Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"...then let's remind ourselves what messy data we have on our hands","metadata":{}},{"cell_type":"code","source":"imdb_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.125432Z","iopub.execute_input":"2022-05-04T19:14:54.126065Z","iopub.status.idle":"2022-05-04T19:14:54.143350Z","shell.execute_reply.started":"2022-05-04T19:14:54.126026Z","shell.execute_reply":"2022-05-04T19:14:54.142744Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"For the *title_cast* column, we will split each row on '|' and we want to keep the firstname and lastname of the actors together so we join the first names and last names with an underscore('_'), same for the directors and make them lowercase\n\nFor *plot_keyword*, we will replace '|' with a space ' ', remove stopwords and we lemmatize or stem the word or both.\n\nFor the *budget* column, we will remove the commas(',') and extract the digits into a separate column and the currency symbols into another","metadata":{}},{"cell_type":"code","source":"# check for duplicated data\n\nimdb_copy.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.144667Z","iopub.execute_input":"2022-05-04T19:14:54.145051Z","iopub.status.idle":"2022-05-04T19:14:54.183915Z","shell.execute_reply.started":"2022-05-04T19:14:54.145013Z","shell.execute_reply":"2022-05-04T19:14:54.183264Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"There are no duplicated data in the IMDB Dataset","metadata":{}},{"cell_type":"markdown","source":"#### Agenda\n1. Fill null values with ''(blank) for both *title_cast*, *director* and *plot_keywords*\n2. Split *title_cast* on '|', join the first and last names of each actor with '_' and convert to lowercase letters\n3. Replace '|' with ' '(a space) in *plot_keywords*\n4. Remove stopwords in plot_keywords\n5. Replace ',' in _budget_ column with ''(nothing)\n6. Extract currency symbol into another column called 'symbol' and amount into 'budget_amount'\n7. Convert the dtype of budget to 'float32'","metadata":{}},{"cell_type":"code","source":"# Agenda 1: Fill null values with ''(blank)\n\nimdb_copy['title_cast'] = imdb_copy['title_cast'].fillna('')\nimdb_copy['director'] = imdb_copy['director'].fillna('')\nimdb_copy['plot_keywords'] = imdb_copy['plot_keywords'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.184893Z","iopub.execute_input":"2022-05-04T19:14:54.185396Z","iopub.status.idle":"2022-05-04T19:14:54.203384Z","shell.execute_reply.started":"2022-05-04T19:14:54.185362Z","shell.execute_reply":"2022-05-04T19:14:54.202444Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Agenda 2: Split title_cast on '|', join the first and last names of each actor with '_'\n\ndef clean_text(text):\n    \n    # split text on '|'\n    text_split = text.split('|')\n    \n    # replace the space between the actors first name and\n    # lastname with an underscore, convert to lowercase\n    # and then join into a string.\n    text_replace = ' '.join([x.replace(' ', '_') if len(x) > 0 else '' for x in text_split]).lower()\n    \n    # return transformed text\n    return text_replace\n\n# apply clean_text function to each row\nimdb_copy['title_cast'] = imdb_copy['title_cast'].apply(clean_text) \n\n# replace the space between the directors' first name and \n# last names with an underscore, and convert to lowercase\nimdb_copy['director'] = imdb_copy['director'].apply(lambda row: row.replace(' ', '_').lower())","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.204738Z","iopub.execute_input":"2022-05-04T19:14:54.205523Z","iopub.status.idle":"2022-05-04T19:14:54.370436Z","shell.execute_reply.started":"2022-05-04T19:14:54.205471Z","shell.execute_reply":"2022-05-04T19:14:54.369329Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Agenda 3: Replace '|' with ' '(a space) in plot_keywords\n\n# pick column and use .apply() with the lambda function to replace \"|\" character\n# with a space.\nimdb_copy['plot_keywords'] = imdb_copy['plot_keywords']\\\n                            .apply(lambda row: row.replace('|', ' '))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.371528Z","iopub.execute_input":"2022-05-04T19:14:54.372264Z","iopub.status.idle":"2022-05-04T19:14:54.390435Z","shell.execute_reply.started":"2022-05-04T19:14:54.372228Z","shell.execute_reply":"2022-05-04T19:14:54.389355Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Agenda 4: Remove stopwords from *plot_keywords*\nstopwords = set(STOPWORDS)\n\ndef remove_stopwords(text):\n    split_text = text.split()\n    text = ' '.join([x for x in split_text if x not in stopwords])  \n    return text    \n\nimdb_copy['plot_keywords'] = imdb_copy['plot_keywords'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.391922Z","iopub.execute_input":"2022-05-04T19:14:54.392159Z","iopub.status.idle":"2022-05-04T19:14:54.449776Z","shell.execute_reply.started":"2022-05-04T19:14:54.392131Z","shell.execute_reply":"2022-05-04T19:14:54.449054Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Agenda 5: Replace ',' in budget column with ''(nothing)\n\n# replace commas in budget amount with blanks\n# excluding rows with values np.Nan\nimdb_copy['budget'] = imdb_copy['budget']\\\n                                .apply(lambda row: row.replace(',', '')\\\n                                      if type(row) == str else row)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.450789Z","iopub.execute_input":"2022-05-04T19:14:54.451493Z","iopub.status.idle":"2022-05-04T19:14:54.469400Z","shell.execute_reply.started":"2022-05-04T19:14:54.451456Z","shell.execute_reply":"2022-05-04T19:14:54.468644Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Removing the commas before using regex to extract amounts and currency symbols is important because trying to extract the digits first before removing commas will result in extracting only the first few digits found before a comma, leaving us with only the million figure without the zeros (i.e 35 instead of 35000000)","metadata":{}},{"cell_type":"code","source":"# Agenda 6: Extract currency symbol into another column called 'symbol' and amount into 'budget_amount'\n\npattern = '[0-9]+' # pattern to search for digits\nsymbol = '[$A-Za-z]+' # patter to search for alpha characters\n\n# extract the budget amount from the budget column \n# and put it in `budget_amount column \n# excluding rows with values np.Nan\nimdb_copy['budget_amount'] = imdb_copy['budget']\\\n                            .apply(lambda row: re.search(pattern, row)\\\n                                   .group() if type(row) == str else row)\n\n# extract the currency symbol from the budget column\n# and put it in `symbol` column\n# excluding rows with values np.Nan\nimdb_copy['symbol'] = imdb_copy['budget']\\\n                            .apply(lambda row: re.search(symbol, row)\\\n                                   .group() if type(row) == str else row)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.470648Z","iopub.execute_input":"2022-05-04T19:14:54.470923Z","iopub.status.idle":"2022-05-04T19:14:54.528435Z","shell.execute_reply.started":"2022-05-04T19:14:54.470872Z","shell.execute_reply":"2022-05-04T19:14:54.527563Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Agenda 7: Convert the dtype of budget to 'float32'\n\nimdb_copy['budget_amount'] = imdb_copy['budget_amount'].astype('float32')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.529745Z","iopub.execute_input":"2022-05-04T19:14:54.530236Z","iopub.status.idle":"2022-05-04T19:14:54.539466Z","shell.execute_reply.started":"2022-05-04T19:14:54.530188Z","shell.execute_reply":"2022-05-04T19:14:54.538572Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"imdb_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.540661Z","iopub.execute_input":"2022-05-04T19:14:54.540975Z","iopub.status.idle":"2022-05-04T19:14:54.559602Z","shell.execute_reply.started":"2022-05-04T19:14:54.540941Z","shell.execute_reply":"2022-05-04T19:14:54.559001Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"So far, we have been able to clean the imdb data to a certain degree and I am currently happy with the result. \n\nThe redundant columns will be handled during `Feature Engineering`\n\nNext we will be cleaning the Movies Dataset","metadata":{}},{"cell_type":"markdown","source":"#### 2. MOVIES DATA","metadata":{}},{"cell_type":"code","source":"# make a copy\n\nmovies_copy = movies.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.560982Z","iopub.execute_input":"2022-05-04T19:14:54.561423Z","iopub.status.idle":"2022-05-04T19:14:54.566346Z","shell.execute_reply.started":"2022-05-04T19:14:54.561389Z","shell.execute_reply":"2022-05-04T19:14:54.565575Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"movies_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.567743Z","iopub.execute_input":"2022-05-04T19:14:54.568000Z","iopub.status.idle":"2022-05-04T19:14:54.584199Z","shell.execute_reply.started":"2022-05-04T19:14:54.567969Z","shell.execute_reply":"2022-05-04T19:14:54.583340Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# check for duplicated data\n\nmovies_copy.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.585334Z","iopub.execute_input":"2022-05-04T19:14:54.585559Z","iopub.status.idle":"2022-05-04T19:14:54.622942Z","shell.execute_reply.started":"2022-05-04T19:14:54.585531Z","shell.execute_reply":"2022-05-04T19:14:54.622118Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"There are no duplicated data","metadata":{}},{"cell_type":"markdown","source":"The Agenda for this dataset will be;\n\n1. Split the title into 2 columns, title and year_released and convert the title column to lowercase\n2. Replace '|' with ' '(space) in the genres column and convert to lowercase","metadata":{}},{"cell_type":"code","source":"# Agenda 1: Split the title into 2 columns, title and year_released\n\nmovies_copy['year'] = movies_copy['title'].apply(lambda x: x[-7:].replace('(', '').replace(')', ''))\n\n#convert year to a numeric column\nmovies_copy['year'] = pd.to_numeric(movies_copy['year'], errors='coerce', downcast='float')\n\nmovies_copy['title'] = movies_copy['title'].apply(lambda x: x[:-7].strip().lower())","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.624606Z","iopub.execute_input":"2022-05-04T19:14:54.625207Z","iopub.status.idle":"2022-05-04T19:14:54.775235Z","shell.execute_reply.started":"2022-05-04T19:14:54.625162Z","shell.execute_reply":"2022-05-04T19:14:54.774418Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Agenda 2: Replace '|' with ' '(space) in the genres column\n\nmovies_copy['genres'] = movies_copy['genres'].apply(lambda row: row.replace('|', ' ').lower())","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.776227Z","iopub.execute_input":"2022-05-04T19:14:54.776443Z","iopub.status.idle":"2022-05-04T19:14:54.819196Z","shell.execute_reply.started":"2022-05-04T19:14:54.776417Z","shell.execute_reply":"2022-05-04T19:14:54.818361Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"movies_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.820443Z","iopub.execute_input":"2022-05-04T19:14:54.820683Z","iopub.status.idle":"2022-05-04T19:14:54.832295Z","shell.execute_reply.started":"2022-05-04T19:14:54.820654Z","shell.execute_reply":"2022-05-04T19:14:54.831356Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"Everything seems to have worked out fine, and I am happy with the results\n\n\nNext, Meta_data","metadata":{}},{"cell_type":"markdown","source":"#### 3. META DATA","metadata":{}},{"cell_type":"markdown","source":"Recall that meta_data is a robust agglomerated version of the IMDB and Movies Datasets. The meta_data meanwhile, does not contain values for some of the data points that are available in Movies and IMDB datasets, for instance, some movie budgets data available in the IMDB dataset are not available in the Meta dataset.\n\nAfter cleaning, we will take a look at how to maximally utilise all datasets involved","metadata":{}},{"cell_type":"code","source":"# make a copy\nmeta_copy = meta_data.copy(deep=True)\n\n# drop possible duplicates\nmeta_copy.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:54.833371Z","iopub.execute_input":"2022-05-04T19:14:54.833637Z","iopub.status.idle":"2022-05-04T19:14:55.061363Z","shell.execute_reply.started":"2022-05-04T19:14:54.833602Z","shell.execute_reply":"2022-05-04T19:14:55.060411Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"meta_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:55.062565Z","iopub.execute_input":"2022-05-04T19:14:55.062810Z","iopub.status.idle":"2022-05-04T19:14:55.091502Z","shell.execute_reply.started":"2022-05-04T19:14:55.062779Z","shell.execute_reply":"2022-05-04T19:14:55.090867Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# displaying the truncated columns\nmeta_copy.loc[:, 'overview': 'release_date'].head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:55.092445Z","iopub.execute_input":"2022-05-04T19:14:55.093119Z","iopub.status.idle":"2022-05-04T19:14:55.115220Z","shell.execute_reply.started":"2022-05-04T19:14:55.093082Z","shell.execute_reply":"2022-05-04T19:14:55.114223Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"This is a very messy data. Let's get to work","metadata":{}},{"cell_type":"markdown","source":"Let's set our cleaning agenda for this dataset;\n\n1. Column *Belongs_to_collection* should be converted to a boolean field (True or False)\n2. Extract the genres from column _genres_ as they are store in dictionaries in a list\n3. Extract the digits from the *imdb_id* column and rename the column to _imdbId_\n4. Convert *popularity* to 'float32'\n5. Extract *production_companies* and *production_countries*\n6. Extract _year_ from *release_date*\n7. Extract _language_ from *spoken_languages*\n8. Replace Zeros in budget with np.nan and make column a numerical column\n9. convert _title_ column to lowercase\n","metadata":{}},{"cell_type":"code","source":"# Agenda 1: Column Belongs_to_collection should be converted to a boolean field (True or False)\n\nmeta_copy['belongs_to_collection'] = meta_copy['belongs_to_collection']\\\n                                        .apply(lambda row: True if type(row) != float else False )","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:55.116372Z","iopub.execute_input":"2022-05-04T19:14:55.116587Z","iopub.status.idle":"2022-05-04T19:14:55.148379Z","shell.execute_reply.started":"2022-05-04T19:14:55.116561Z","shell.execute_reply":"2022-05-04T19:14:55.147432Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"Moving on to the second agenda. The __Genres__ column is a list of objects or dictionaries made of 'id' - referring to the genre's id and the 'name' - referring to the genre itself. This is really poor formatting if we are going to make use of this dataset. **Production_companies**, **production_countries** and **spoken_languages** are formatted In a similar fashion.\n\nTo deal with this problem, we will be applying a custom function we will call `decompose` to help us extract what we need from these columns. But first, since each row in these columns is stored as a 'string', we will apply 'literal_eval' which evaluates a string containing a python literal, such as a list, dictionary etc...\n\nLet's see an example","metadata":{}},{"cell_type":"code","source":"x = \"[{'id': 12, 'name': 'Adventure'},{'id': 14, 'name': 'Fantasy'},{'id': 10751, 'name': 'Family'}]\"\n\nprint(f\"Data type of x before literal_eval: {type(x)}\")\n\nx = literal_eval(x)\n\nprint(f\"Data type of x after literal_eval: {type(x)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:55.149757Z","iopub.execute_input":"2022-05-04T19:14:55.150141Z","iopub.status.idle":"2022-05-04T19:14:55.157618Z","shell.execute_reply.started":"2022-05-04T19:14:55.150101Z","shell.execute_reply":"2022-05-04T19:14:55.156578Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"Literal_eval takes the text and returns its true form. Let's apply this in cleaning our dataset","metadata":{}},{"cell_type":"code","source":"# Agenda 2: Extract the genres from column genres as they are store in dictionaries in a list\n\ndef decompose(text, key='name'):\n    \n    try:\n        # check if text is np.nan \n        if type(text) == float:\n            decomposed_text = text\n            \n        #apply literal_exal to each row of data\n        eval_text = literal_eval(text)\n\n        # get the name key of each dictionary in the list\n        # store extracted name in a list\n        # join each item in the list into a string\n        decomposed_text = ' '.join([dictionary[key].replace(' ', '_') for dictionary in eval_text]).lower()\n    \n    except (ValueError, TypeError):\n        decomposed_text = np.nan\n        \n    return decomposed_text","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:55.159267Z","iopub.execute_input":"2022-05-04T19:14:55.159670Z","iopub.status.idle":"2022-05-04T19:14:55.170755Z","shell.execute_reply.started":"2022-05-04T19:14:55.159622Z","shell.execute_reply":"2022-05-04T19:14:55.169746Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# apply the decompose function on the genres column.\nmeta_copy['genres'] = meta_copy['genres'].apply(decompose)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:55.172324Z","iopub.execute_input":"2022-05-04T19:14:55.173412Z","iopub.status.idle":"2022-05-04T19:14:56.549047Z","shell.execute_reply.started":"2022-05-04T19:14:55.173360Z","shell.execute_reply":"2022-05-04T19:14:56.548406Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"meta_copy.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:56.550596Z","iopub.execute_input":"2022-05-04T19:14:56.551134Z","iopub.status.idle":"2022-05-04T19:14:56.579603Z","shell.execute_reply.started":"2022-05-04T19:14:56.551089Z","shell.execute_reply":"2022-05-04T19:14:56.578652Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"The function does its job well and I'm very happy with the results.\n\nNext, we will extract digits from the **imdb_id** column and rename it","metadata":{}},{"cell_type":"code","source":"# Agenda 3: Extract the digits from the imdb_id column and rename the column to imdbId\n\nre_pattern = '\\d+' # regex to extract 1 or more digits\n\n# applying regex, search through every row text, find every digits\n# in the text then return the group of texts if the row is not 'null'\nmeta_copy['imdb_id'] = meta_copy['imdb_id']\\\n                        .apply(lambda row: re.search(re_pattern, row)\\\n                              .group() if type(row) == str else np.nan)\\\n                                .astype('float32') # convert the column to dtype 'int32'\n\n# rename the 'imdb_id' to 'imdbId' \nmeta_copy = meta_copy.rename(columns={'imdb_id': 'imdbId'})","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:56.580941Z","iopub.execute_input":"2022-05-04T19:14:56.581164Z","iopub.status.idle":"2022-05-04T19:14:56.704854Z","shell.execute_reply.started":"2022-05-04T19:14:56.581136Z","shell.execute_reply":"2022-05-04T19:14:56.703980Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# Agenda 4: Convert popularity to float32\n\nmeta_copy['popularity'] = pd.to_numeric(meta_copy['popularity'], downcast='float', errors='coerce')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:56.705977Z","iopub.execute_input":"2022-05-04T19:14:56.706211Z","iopub.status.idle":"2022-05-04T19:14:56.733493Z","shell.execute_reply.started":"2022-05-04T19:14:56.706181Z","shell.execute_reply":"2022-05-04T19:14:56.732520Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Agenda 5: Extract production_companies and production_countries\n\n# extract indices of rows with irregular values\nboolean = np.where((meta_copy['production_companies'] == True) | \\\n                   (meta_copy['production_companies'] == 'True') | \\\n                  (meta_copy['production_companies'] == False) | \\\n                  (meta_copy['production_companies'] == 'False') | \\\n                  (meta_copy['production_companies'] == 'nan') | \\\n                  (meta_copy['production_companies'] == np.nan)) \n\ndata = meta_copy.iloc[boolean]\ndata.loc[:,'overview':'release_date']","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:56.735052Z","iopub.execute_input":"2022-05-04T19:14:56.735799Z","iopub.status.idle":"2022-05-04T19:14:56.793700Z","shell.execute_reply.started":"2022-05-04T19:14:56.735763Z","shell.execute_reply":"2022-05-04T19:14:56.793094Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"There are boolean values in the **production_companies** column which can cause problems, so we will replace these values with 'np.NaN'. Also **production_countries** contains some floating point values which are not supposed to be there. we will replace these with np.nan","metadata":{}},{"cell_type":"code","source":"# extract company name fro production companies\nmeta_copy['production_companies'] = meta_copy['production_companies']\\\n                                    .apply(lambda x: np.nan if x in ('', ' ', 'True', True, 'False', False) else x)\\\n                                    .apply(decompose)\n# extract countries from production countries\nmeta_copy['production_countries'] = meta_copy['production_countries']\\\n                                    .apply(lambda x: np.nan if x in ('', ' ', 'True', True, 'False', False) else x)\\\n                                    .apply(decompose)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:56.794748Z","iopub.execute_input":"2022-05-04T19:14:56.795216Z","iopub.status.idle":"2022-05-04T19:14:58.901512Z","shell.execute_reply.started":"2022-05-04T19:14:56.795172Z","shell.execute_reply":"2022-05-04T19:14:58.900899Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"meta_copy.loc[:, 'overview':'release_data'].iloc[boolean]","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:58.902477Z","iopub.execute_input":"2022-05-04T19:14:58.903104Z","iopub.status.idle":"2022-05-04T19:14:58.924824Z","shell.execute_reply.started":"2022-05-04T19:14:58.903069Z","shell.execute_reply":"2022-05-04T19:14:58.923872Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Agenda 6: Extract year from release_date\n\nmeta_copy['year'] = meta_copy['release_date']\\\n                        .apply(lambda row: row[0:4] \\\n                              if type(row) == str else np.nan).astype('float32')\n\nmeta_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:58.926585Z","iopub.execute_input":"2022-05-04T19:14:58.927512Z","iopub.status.idle":"2022-05-04T19:14:58.993892Z","shell.execute_reply.started":"2022-05-04T19:14:58.927462Z","shell.execute_reply":"2022-05-04T19:14:58.993006Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# Agenda 7: Extract language from spoken_languages\n\n# here, we change the key of the decompose function\n# because we want the encoding of the language\n# not the language name itself\nmeta_copy['spoken_languages'] = meta_copy['spoken_languages']\\\n                                    .apply(decompose, args=('iso_639_1',))\n\nmeta_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:58.995427Z","iopub.execute_input":"2022-05-04T19:14:58.995762Z","iopub.status.idle":"2022-05-04T19:14:59.970612Z","shell.execute_reply.started":"2022-05-04T19:14:58.995717Z","shell.execute_reply":"2022-05-04T19:14:59.969647Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# Agenda 8: Replace Zeros in budget with np.nan and make column a numerical column\n\nmeta_copy['budget'] = meta_copy['budget'].apply(lambda row: np.nan if row == '0' else row)\nmeta_copy['budget'] = pd.to_numeric(meta_copy['budget'], downcast='float', errors='coerce')\n\nmeta_copy['budget'].tail()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:14:59.972152Z","iopub.execute_input":"2022-05-04T19:14:59.972799Z","iopub.status.idle":"2022-05-04T19:15:00.016031Z","shell.execute_reply.started":"2022-05-04T19:14:59.972748Z","shell.execute_reply":"2022-05-04T19:15:00.015067Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Agenda 9: Convert title to lowercase\n\n# Convert title column to lowercase\nmeta_copy['title'] = meta_copy['title'].str.lower()\n\nmeta_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:15:00.017078Z","iopub.execute_input":"2022-05-04T19:15:00.017663Z","iopub.status.idle":"2022-05-04T19:15:00.070590Z","shell.execute_reply.started":"2022-05-04T19:15:00.017623Z","shell.execute_reply":"2022-05-04T19:15:00.069976Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"We have successfully cleaned up some of our datasets a bit which may aid our process of `Exploratory Data Analsys` or **EDA** for short.","metadata":{}},{"cell_type":"markdown","source":"However, we do have a decision to make concerning which of **Movies and IMDB** datasets and **Meta Data** dataset.\n\nLet's carry out a preliminary exploratory data analysis on these datasets","metadata":{}},{"cell_type":"markdown","source":"##### PRELIM EDA.","metadata":{}},{"cell_type":"code","source":"print(f'Movies has: {movies_copy.shape[0]} rows and {movies_copy.shape[1]} columns\\n\\\nImdb data has: {imdb_copy.shape[0]} rows and {imdb_copy.shape[1]} columns\\n\\\nMeta_data has: {meta_copy.shape[0]} rows and {meta_copy.shape[1]} columns')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:15:00.071706Z","iopub.execute_input":"2022-05-04T19:15:00.072124Z","iopub.status.idle":"2022-05-04T19:15:00.077214Z","shell.execute_reply.started":"2022-05-04T19:15:00.072087Z","shell.execute_reply":"2022-05-04T19:15:00.076353Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"Movies dataset has more rows than imdb and meta_data, meaning there are more observations in the Movies dataset. Meanwhile, Meta_data has more columns than movies and imdb datasets combined, meaning the meta_data dataset holds more information that may useful to our course than the other two.\n\n\nNext, lets see the features(columns) they all have.","metadata":{}},{"cell_type":"code","source":"print(f\"Movies_features \\t>>>> \\t{' | '.join(movies.columns)}\\n\\\nImdb_features \\t>>>> \\t{' | '.join(imdb.columns)}\\n\\\nMeta_features \\t>>>> \\t{' | '.join(meta_copy.columns)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:15:00.078343Z","iopub.execute_input":"2022-05-04T19:15:00.078652Z","iopub.status.idle":"2022-05-04T19:15:00.090211Z","shell.execute_reply.started":"2022-05-04T19:15:00.078539Z","shell.execute_reply":"2022-05-04T19:15:00.089311Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"There are some intesting features in all the datasets that immense use to our algorithms. Meanwhile, it is worthy to note that all of the features in the movies dataset appear in the meta_data dataset, bar 'movieId'. Not forgetting that the movies dataset has more observations than the meta_data, we have to look for a way to make the best use of both datasets.\n\nThe IMDB dataset more data about the people in the movie - the casts and the directors of each movies. These are particularly important features when we try to make movie recommendations to users based on the actors in the movies or the director of the movie as many users have biases/favorites in these categories.\n\nWe have to think of ways to incorporate these datasets and their unique attribute into the algorithms we will build later on.","metadata":{}},{"cell_type":"markdown","source":"In the meta_data dataset, there are come movies without budget amounts while in the Imdb dataset, these movies seem to have budgets. Let's do a bit of Sherlock Holmes here, shall we?","metadata":{}},{"cell_type":"code","source":"# first, lets merge the movies and Imdb data on movieId\n# to get the title column\n\ntemp_df = imdb_copy.merge(movies_copy[['title', 'movieId']], on='movieId')\ntemp_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:15:00.091599Z","iopub.execute_input":"2022-05-04T19:15:00.092401Z","iopub.status.idle":"2022-05-04T19:15:00.144284Z","shell.execute_reply.started":"2022-05-04T19:15:00.092360Z","shell.execute_reply":"2022-05-04T19:15:00.143393Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# Get movies without budget from meta_data\nmeta_without_budget = meta_copy['title'].str.lower()[meta_copy['budget'].isnull()]\n\n# Get movies without budget from temp_df\nimdb_without_budget = temp_df['title'][temp_df['budget_amount'].isnull()]\n\nprint(f'meta: {meta_without_budget.shape} making up \\\n{round(meta_without_budget.shape[0]/meta_copy.shape[0]*100)}% missing\\n\\\nImdb: {imdb_without_budget.shape} making up \\\n{round(imdb_without_budget.shape[0]/imdb_copy.shape[0]*100)}% missing')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:15:00.145855Z","iopub.execute_input":"2022-05-04T19:15:00.146112Z","iopub.status.idle":"2022-05-04T19:15:00.180657Z","shell.execute_reply.started":"2022-05-04T19:15:00.146082Z","shell.execute_reply":"2022-05-04T19:15:00.180073Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"These are significant amounts of missing data.\n\nLets see the similar movies","metadata":{}},{"cell_type":"code","source":"sim_movies = meta_without_budget[meta_without_budget.isin(imdb_without_budget)]\n\nprint(sim_movies)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:15:00.182199Z","iopub.execute_input":"2022-05-04T19:15:00.182543Z","iopub.status.idle":"2022-05-04T19:15:00.200426Z","shell.execute_reply.started":"2022-05-04T19:15:00.182497Z","shell.execute_reply":"2022-05-04T19:15:00.199784Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"The above output are movies that do not have a budget amount in both Imdb dataset and meta_data dataset. There __10,122__ movies without a budget in both datasets","metadata":{}},{"cell_type":"code","source":"# Extracting movies that are in the Imdb dataset \n# from the ones without a budget amount in meta_data\nsim = np.where(temp_df['title'].isin(sim_movies))\n\nnew_df = temp_df.iloc[sim]\n\nnew_df = new_df[new_df['budget_amount'].notnull()]\n\nnew_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:15:00.201491Z","iopub.execute_input":"2022-05-04T19:15:00.201748Z","iopub.status.idle":"2022-05-04T19:15:00.226716Z","shell.execute_reply.started":"2022-05-04T19:15:00.201717Z","shell.execute_reply":"2022-05-04T19:15:00.225690Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"There are __289__ movies in the Imdb data that have budget amounts which are missing in the meta_data dataset. This doesn't solve a significant amount of our problem, we can move on without it.","metadata":{}},{"cell_type":"code","source":"# free up some space\ndel temp_df\ndel sim_movies\ndel sim\ndel new_df\ndel meta_without_budget\ndel imdb_without_budget","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:15:00.228437Z","iopub.execute_input":"2022-05-04T19:15:00.229026Z","iopub.status.idle":"2022-05-04T19:15:00.237851Z","shell.execute_reply.started":"2022-05-04T19:15:00.228979Z","shell.execute_reply":"2022-05-04T19:15:00.236889Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"All other datasets seem to be prim and proper","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='eda'>\n    <h2 style='text-transform: uppercase;'>Exploratory Data Analysis</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Exploratory Data Analysis,also known as EDA, refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.","metadata":{}},{"cell_type":"markdown","source":"As before, we will be performing the EDA process dataset by dataset, starting with the IMDB Data. Let's see what we have to work with first","metadata":{}},{"cell_type":"markdown","source":"#### 1. IMDB DATA","metadata":{}},{"cell_type":"code","source":"imdb_copy.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the data, there are a few questions we can find answers to;\n\n1. What are the minimum, average and maximum runtimes?\n2. What are the minimum, average and maximim budgets for a movie?\n3. The currency symbols denote countries, which countries are movies produced in?\n4. Who is/are the most featured casts?\n5. Director with the most movies?\n6. What plot keywords are most frequent?\n7. Does runtime influence the budget?\n8. How many movies are represented by this dataset?","metadata":{}},{"cell_type":"markdown","source":"To answer Questions 1 through 3, Let us get some descriptive statistics about the data","metadata":{}},{"cell_type":"code","source":"# Question 1: What are the minimum, average and maximum runtimes?\n# Question 2: What are the minimum, average and maximim budgets for a movie?\n\nimdb_copy.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This gives a brief statistical description of the data. \n\n**ANSWER 1 & 2**\n\nFrom the output, we can deduce that movies have an average runtime of __100__ mins, a minimum of __1__ minute and a maximum of __877__ mins (roughly __14__ hours! interesting.)\n\nThe average budget for a movie is roughly __40 Million__, a minimum of __0__ and a maximum of __30 Billion__. Notice we are not using any currency to quantify the budget? That's because not all of the budget amounts are denoted in a single currency; they vary, leading to a lot of skewness in our data. We will sort that out later in `Feature Engineering`. ","metadata":{}},{"cell_type":"code","source":"# Question 3: The currency symbols denote countries, which countries are movies produced in?\n\nimdb_copy.describe(include='object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 3**\n\nHere, we can deduce that there are __9,874__ movie directors, __USD10 Million__ budget for movies is more frequent and we can assume that a lot of the movies in the dataset were produced in the United States with a '$' frequency of __6,426__","metadata":{}},{"cell_type":"markdown","source":"For the remainder of questions, we will employ the help of some custom functions","metadata":{}},{"cell_type":"code","source":"def aggregate(series):\n    \"\"\"\n    This function `collects` all the values in a \n    pd.Series and returns a frequency table of the \n    number of occurence for each value\n    \"\"\"\n    # call the collect function the get a list\n    # of values in a column and store result in\n    # a variable called items\n    items = collect(series)\n    \n    # initialise an empty dictionary to store \n    # values and their counts\n    frequency = {}\n    \n    # iterate over items\n    for item in items:\n        # check if individual item is in initialised \n        # dictionary\n        if item in frequency:\n            # add 1 to its value count if it exists\n            # in the initialised dictionary\n            frequency[item] += 1\n        else:\n            # set its value count to 1 if it doesn't\n            # exist in the initialised dictionary\n            frequency[item] = 1\n    \n    # sort the frequecy table from highest to lowest\n    sorted_freq = {k: v for k, v in sorted(frequency.items(), reverse=True, key=lambda item: item[1])}\n    \n    # return sorted frequency table\n    return sorted_freq\n\ndef collect(series):\n    \"\"\"\n    This function takes in a pd.Series object that\n    contains lists as rows and then breaks it down\n    into a single list.\n    \"\"\"\n    # initialise and empty list to contain\n    # the broken down column of lists\n    collection = []\n    \n    # iterate over the column\n    for i in series:\n        # iterate over the column iter\n        for j in i:\n            # append value to collection\n            collection.append(j)\n    # return collection\n    return collection\n\ndef word_cloud(data, category):\n    \"\"\"\n    This function generates a word cloud visualisation\n    \"\"\"\n    \n    stopwords = set(STOPWORDS)\n    \n    # Instantiate wordcloud object\n    word_cloud = WordCloud(collocations =False,\n                           background_color = 'Black',\n                           stopwords=stopwords,\n                           width=1600,\n                           height=800,\n                           contour_width=2,\n                           contour_color='steelblue',\n                          random_state= 1)\n    # generate wordcloud images\n    word_cloud.generate_from_frequencies(data)\n    # Create Plot\n    \n    plt.figure(figsize =(10,10))\n    plt.imshow(word_cloud, interpolation ='bilinear' )\n    plt.axis('off')\n    plt.title('Most Frequent {}'.format(category.capitalize()), size = 25, pad =15)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Question 4: Who is/are the most featured actor(s)\n\ndef get_top_n(series, n=10, col=None):\n    \n    \"\"\"\n    Function to get the top n observations from a text\n    column of interest.\n    \"\"\"\n    # call the aggregate function\n    features = aggregate(series.apply(lambda row: row.split()))\n    # create a dataframe\n    most_featured = pd.DataFrame.from_dict(features, orient='index').reset_index().head(n)\n    # rename columns\n    most_featured.columns = [col, '# of features']\n\n    return (most_featured, features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get top n for actors\nfeatured_actors = get_top_n(imdb_copy['title_cast'], col='Actors')[1]\nmost_featured_actors_10 = get_top_n(imdb_copy['title_cast'], col='actors')[0]\n\nprint(f\"There are {len(set(featured_actors.keys()))} actors in total\")\nmost_featured_actors_10","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create barplot with seaborn\nax = sns.barplot(most_featured_actors_10['actors'], most_featured_actors_10['# of features'], color='blue')\n# annotate each bar with percentage of occurrence\n# of the plotted features\nfor p in ax.patches:\n    percentage = '{:.3f}%'.format(100 * p.get_height()/ sum(featured_actors.values()))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n\nplt.xticks(rotation=20) # rotate by 20degrees\nplt.xlabel('Actors')\nplt.ylabel('# of features')\nplt.title(\"Most Featured Actors\", fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 4**\n\n**Samuel L Jackson** has been featured the most; great guy! \n\nThe list features top-rated stars, hence we can say that top-rated stars get featured the most.","metadata":{}},{"cell_type":"code","source":"# generate word cloud\nword_cloud(featured_actors, 'actors')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot shows clearly the frequency of appearance of each actor.","metadata":{}},{"cell_type":"markdown","source":"Next, we will look at the top 10 directors with the most movies directed.","metadata":{}},{"cell_type":"code","source":"# Question 5: Director with the most movies?\n\n# ge top n for directors\nfeatured_directors = get_top_n(imdb_copy['director'], col='director')[1]\nmost_featured_directors_10 = get_top_n(imdb_copy['director'], col='director')[0]\n\nprint(f\"There are {len(set(featured_directors.keys()))} directors in total\")\nmost_featured_directors_10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 5**\n\n**See_Full_Summary**, either that's a weird name from Europe or it means there's no data for director provided for these rows. We will run with the latter and replace this value with something we can work with. But, **Woody Allen**, **Luc Besson** and **Stephen King** are up there too.","metadata":{}},{"cell_type":"code","source":"# create barplot with seaborn\nax = sns.barplot(most_featured_directors_10['director'], most_featured_directors_10['# of features'], color='blue')\n# annotate each bar with percentage of occurrence\n# of the plotted features\nfor p in ax.patches:\n    percentage = '{:.3f}%'.format(100 * p.get_height()/ sum(featured_directors.values()))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n\nplt.xticks(rotation=20) # rotate by 20degrees\nplt.xlabel('Directors')\nplt.ylabel('# of features')\nplt.title(\"Most Featured Directors\", fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate word cloud\nword_cloud(featured_directors, 'directors')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Question 6: What plot keywords are most frequent?\n\n# get top n for plot keywords\nkeywords = get_top_n(imdb_copy['plot_keywords'], col='keywords')\nkeywords_10 = keywords[0]\nfeatured_keywords = keywords[1]\n\nprint(f\"There are {len(set(featured_keywords.keys()))} keywords in total\")\nkeywords_10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 6**\n\n**Female**, **Nudity** & **Title** make up the top three most frequent keywords","metadata":{}},{"cell_type":"code","source":"# create barplot with seaborn\nax = sns.barplot(keywords_10['keywords'], keywords_10['# of features'], color='blue')\n# annotate each bar with percentage of occurrence\n# of the plotted features\nfor p in ax.patches:\n    percentage = '{:.3f}%'.format(100 * p.get_height()/ sum(featured_keywords.values()))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n\nplt.xticks(rotation=20)\nplt.xlabel('Keywords')\nplt.ylabel('# of features')\nplt.title(\"Most Frequent Keywords\", fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate word cloud\nword_cloud(featured_keywords, 'keywords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The word cloud paints a clear picture of what words occur most in movies plot_keywords","metadata":{}},{"cell_type":"code","source":"# Question 8: Does runtime influence budget?\n\n# for this task, we will create a scatter plot of runtime and budget\n\nsns.scatterplot(x=imdb_copy['runtime'].fillna(0), y=imdb_copy['budget_amount'].fillna(0), size=imdb_copy['runtime'].fillna(0))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, we can't confidently say there's any relationship between runtime and budget. There's also an outlier wayyy up there.\n\nLet's confirm this by looking at the correlation figures","metadata":{}},{"cell_type":"code","source":"imdb_copy[['runtime', 'budget_amount']].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 7**\n\nThe correlation between runtime and budget_amount is __0.055__ which practically means theres no correlation between runtime and budget_amount.\n\nA possible explanation for this phenomenon: Recall that there are alot of missing values in the budget column which we filled with Zeros, skewing the data.","metadata":{}},{"cell_type":"code","source":"# Question 8: How many movies are represented by this dataset?\n\nprint(f'There are {imdb_copy[\"movieId\"].nunique()} movies in the dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are done with exploring the Imdb_data. We have seen who the most featured actors are and which director has directed the highest number of movies, what the most common plot keywords are and if there's a relationship between the budget and the movie runtime.\n\nWhat about the most common genres? and the most prolific year of production? Let's unearth these from the movies dataset.","metadata":{}},{"cell_type":"markdown","source":"#### 2. MOVIES DATA","metadata":{}},{"cell_type":"code","source":"# let's see what we have to work with\nmovies_copy.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's set outline questions that require answers from this dataset.\n\nAgenda:\n\n1. What are the most common genres?\n    - How many genres are there?\n2. What year are the most movies released?\n3. what is the total number of unique movies represented in the dataset","metadata":{}},{"cell_type":"code","source":"# Question 1: What are the most common genres?\n\n# get top n for genres\ngenres = get_top_n(movies_copy['genres'], col='genres')\ngenres_10 = genres[0]\nfeatured_genres = genres[1]\n\nprint(f'There are {len(featured_genres.keys())} genres in total.')\ngenres_10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 1**\n\nThere are a lot of movies that **drama**, followed by **comedy** and **thriller** and there are __22__ genres in total","metadata":{}},{"cell_type":"code","source":"# create barplot\nax = sns.barplot(genres_10['genres'], genres_10['# of features'], color='blue')\n# annotate each bar with percentage of occurrence\n# of the plotted features\nfor p in ax.patches:\n    percentage = '{:.3f}%'.format(100 * p.get_height()/ sum(featured_genres.values()))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n\nplt.xticks(rotation=20)\nplt.xlabel('Genres')\nplt.ylabel('# of features')\nplt.title(\"Most Frequent Genres\", fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see clearly by how much **drama** __20.915%__ and **comedy** __13.997%__ lead the pack. Not minding the tail end '(no and genres' as they are one but got caught, unfortunately, in a split() operation. Nonetheless, we will need to replace these values appropriately.","metadata":{}},{"cell_type":"code","source":"# generate word cloud\nword_cloud(featured_genres, 'genres')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Question 2: What year are the most movies released?\n\n# count the number of times each year occur\nyear_count = movies_copy.groupby('year')['year'].count()\n\nfig, ax = plt.subplots()\nax.set_xticks(np.arange(min(year_count.values),max(year_count.values),10))\nax.bar(year_count.index, year_count.values)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"year_count.sort_values(ascending=False).head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 2**\n\nWe can see that the number of the movies released increased over time and peeked in the year __2015__ with __2,513__ movies, then there's a gradual descent from __2016__ and it becomes abrupt in __2019__ - we can credit this to the Coronavirus Pandemic.","metadata":{}},{"cell_type":"code","source":"# Question 3: How many movies are there in the dataset?\n\nprint(f\"There are {movies_copy['movieId'].nunique()} movies in the dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section, we have been able to drilldown a bit into the movies dataset and see what story it tells. Next we will be exploring the meta_data dataset which holds features contained in the movies dataset with a bit more information.","metadata":{}},{"cell_type":"markdown","source":"#### 3. META DATA","metadata":{}},{"cell_type":"code","source":"# As usual, let's see what we are working with\n\nmeta_copy.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a lot to unearth from this data;\n\n1. We will look at how many movies belong to a collection\n2. The spoken languages distribution\n3. How original_language differ from spoken languages\n4. How original_title differs from the title\n5. Correlation between budget, revenue and runtime\n6. Correlation between runtime, vote average\n\nBut first, we will look at the descriptive statistics of both the numerical and the categorical variables","metadata":{}},{"cell_type":"code","source":"meta_copy.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This table presents us some interesting statistics...\n\nThe meta_data budget column has 8889 values compare to 7409 of the budget column in Imdb data. There's an average of __21 Million__ spent on movies, a minimum of __1__ and a maximum of __380 Million__, which is fair compared to Imdb's __30 Billion__.\n\nThe average popularity index of a movie is __2.92__, a minimum of __0__ and a maximum of __547__, indicative of an outlier. We will need to transform this column either by log(1+y) transformation or by taking the square root of each observation.\n\nThere does not seem to be much going on with revenue, so we will skip it for now.\n\nAverage runtime for movies in this dataset is __94__ minutes, a minimum of __0__, or perhaps 'no entry' and a maximum of __1256__ mins(**approx 21hours!** yikes!)\n\nAverage vote_average is __5.6__, with a minimum of __1__ and a maximum of __10__ indicating a range of values from **0 - 10**.\n\nVote_count ranges from __0__ minimum to __14,075__ maximum with a mean of __110__. Year has a minimum of 1, a year we are very certain that motion picture had not be thought of, even remotely and maximum of __2020__. The mean year is __1992__, 30 years ago.\n\nWe have noted a few anomalies that needs taking care of, and we will do that in `Feature Engineering`","metadata":{}},{"cell_type":"code","source":"meta_copy.describe(include='object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_copy['adult'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that there are more non-adult films than there are adult ones. Drama takes the lead in the genres category, Most movies are 'english' movies which can be explained by the fact that a large chunk of these movies are produced in the United States of America. Cinderella has been produced more times than any other movie with 11 remakes. There are __42, 277__ unique movies in this dataset\n\n\nMany of the columns in this dataset will be of no use to our algorithm, and as such, will be discarded in `feature engineering`\n\nLet's go on with out set agenda for the data\n","metadata":{}},{"cell_type":"code","source":"# Question 1: how many movies belong to a collection?\n\ncollection = meta_copy['belongs_to_collection'].value_counts()\ncollection","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create barplot\nax = sns.barplot(collection.index, collection.values)\n# annotate each bar with percentage of occurrence\n# of the plotted features\nfor p in ax.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()/len(meta_copy['belongs_to_collection'].tolist()))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 1**\n\nFalse has a big chunk of this with __40,959__ values making up __90.1%__ of the entire column. This means that many movies do not belong to a collection i.e have sequels of part of a franchise.","metadata":{}},{"cell_type":"code","source":"# Question 2: What is the spoken languages distribution?\n# fill na with blanks\nmeta_copy['spoken_languages'] = meta_copy['spoken_languages'].fillna('')\n\n# get top n langugaes\nlanguages_set = get_top_n(meta_copy['spoken_languages'], col='languages')\nlanguages = languages_set[1]\nlanguages_10 = languages_set[0]\n\nprint(f'There are {len(languages.keys())} languages in total.')\n\nlanguages_10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The languages are encoded and difficult to read. To solve this, we will employ a dataset that contains the names that match correctly to these language codes.","metadata":{}},{"cell_type":"code","source":"# load csv with languages and their codes\nlanguage_map = pd.read_csv('/kaggle/input//languages-and-codes/language-codes.csv')\nlanguage_map.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# map the language and the language code\n# convert dictionary of languages to pd dataframe\nlanguages_df = pd.DataFrame.from_dict(languages, orient='index').reset_index()\\\n                .rename(columns={'index':'language_code', 0: '# of features'})\n# merge language df and language map\nlanguages_df = languages_df.merge(language_map, how='left', left_on='language_code', right_on='alpha2')\nlanguages_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop missing languages\nlanguages_df = languages_df.dropna()\n# convert languages to dict\nlanguages = dict(zip(languages_df['English'].tolist(), languages_df['# of features'].tolist()))\n# extract top 10 and convert to a dataframe\nlanguages_10 = pd.DataFrame.from_dict(dict(list(languages.items())[:10]), orient='index').reset_index()\\\n                .rename(columns={'index':'languages', 0:'# of features'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a barplot\nax = sns.barplot(languages_10['languages'], languages_10['# of features'], color='blue')\n# annotate each bar with percentage of occurrence\n# of the plotted features\nfor p in ax.patches:\n    percentage = '{:.2f}%'.format(100 * p.get_height()/ sum(languages.values()))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n\nplt.xticks(rotation=0)\nplt.xlabel('Languages')\nplt.ylabel('# of features')\nplt.title(\"Most Spoken Languages\", fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate word cloud\nword_cloud(languages, 'Languages')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 2**\n\nThe English language is the most spoken language in movies, being spoken in roughly __55%__ of movies, followed by French in __8%__ of movies and German in __5%__ of movies. This is not surprising as English Language is the language of the world and it will only make sense to have more movies in that language.","metadata":{}},{"cell_type":"code","source":"# Question 3: How original_language differ from spoken languages\n\n# using the `~` sign to negate .isin() \ndiff = meta_copy[['original_language', 'spoken_languages']][~meta_copy['original_language'].isin(meta_copy['spoken_languages'])]\n\ndiff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 3**\n\n`Spoken_languages` looks to be a more useable column than `original_language` because some data points contain more than 1 language. This will be very useful for a recommender system","metadata":{}},{"cell_type":"code","source":"# Question 4: How original_title differs from the title\n\ndiff = meta_copy[['original_title', 'title']][~meta_copy['original_title'].str.lower().isin(meta_copy['title'])]\n\ndiff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 4**\n\nNotice that the **original_title** has some titles written in the original dialect of the country they were produced in, **title** on the other hand has the English translated version of such movie titles, so we're keeping that.","metadata":{}},{"cell_type":"code","source":"# Question 5: Correlation between budget, revenue and runtime\nmeta_copy[['budget', 'revenue', 'runtime']].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate pairplot\ng = sns.pairplot(meta_copy[['budget', 'revenue', 'runtime']], corner=True)\ng.fig.set_figwidth(15) # set figure with to 15\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 5**\n\nThere appears to be a strong relationship between __budget__ and __revenue__. From the plot, we can say that the higher the budget, the higher the revenue. \n\nThere's a very weak relationship between runtime and the other variables","metadata":{}},{"cell_type":"code","source":"# Question 6: Correlation between runtime, vote average, vote count\n\nmeta_copy[['runtime', 'vote_average', 'vote_count']].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate pairplot\ng = sns.pairplot(meta_copy[['runtime', 'vote_average', 'vote_count']], corner=True)\n\ng.fig.set_figwidth(15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 6**\n\nThere are no meaningful relationships to observe here as all correlation values between variables are closer to __0__.","metadata":{}},{"cell_type":"markdown","source":"An interesting analysis will be to find out if there's a relationship between the number of top rated stars in a movie and the budget for the movie. Let us find out!\n\nFirst we will define a custom class to help us breakdown the title_cast column of Imdb data and create an arbitrary star rating based solely on number of appearances in movies.","metadata":{}},{"cell_type":"markdown","source":"##### BRINGING BACK IMDB DATA FOR A BIT","metadata":{}},{"cell_type":"code","source":"# engineer a_list_actors, b_list_actors, c_list_actors\n\ndef classify(df, col, columns=[], threshold=[]):\n    # get the featured items into a dictionary\n    most_featured = aggregate(df[col])\n    # convert the dictionary into a pd.DataFrame\n    most_featured_df = pd.DataFrame.from_dict(most_featured.items())\n    # change the column names\n    most_featured_df.columns = columns\n\n    print('feature extraction complete')\n    \n    # classify based on threshold supplied\n    a_list_actors = most_featured_df[columns[0]][most_featured_df[columns[1]] >= threshold[0]].tolist()\n    b_list_actors = most_featured_df[columns[0]][(most_featured_df[columns[1]] >= threshold[1]) & (most_featured_df[columns[1]] < threshold[0])].tolist()\n    c_list_actors = most_featured_df[columns[0]][most_featured_df[columns[1]] < threshold[1]].tolist()\n    \n    # initialise empty lists to save counts of items \n    # per row\n    a_list_count = []\n    b_list_count = []\n    c_list_count = []\n    \n    # iterate over each row in the column\n    for row in df[col]:\n        a_counter = 0\n        b_counter = 0\n        c_counter = 0\n        \n        # get every item in a row\n        for item in row:\n            # do conditional checking\n            # to know which class each\n            # item is in and append a count\n            # to the appropriate quarters\n            if item in a_list_actors:\n                a_counter += 1\n            if item in b_list_actors:\n                b_counter += 1\n            if item in c_list_actors:\n                c_counter += 1\n        \n        # append a count to the empty list initialised\n        a_list_count.append(a_counter)\n        b_list_count.append(b_counter)\n        c_list_count.append(c_counter)\n\n    return (a_list_count, b_list_count, c_list_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df = imdb_copy.copy(deep=True)\ntemp_df['title_cast'] = temp_df['title_cast'].apply(lambda row: row.split() if type(row) ==str else '[]')\na, b, c = classify(temp_df, 'title_cast', columns=['actor', '# of features'], threshold=[30, 15])\ntemp_df['a_stars']  = a\n\ntemp_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df[['budget_amount', 'a_stars']].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.pairplot(temp_df[['budget_amount', 'a_stars']], corner=True)\n# g.fig.set_figheight(10)\ng.fig.set_figwidth(15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Shocking as there seems to be absolutely no relationship between budget for a movie and the number of superstars in the movie. This can be misleading as there may be budget amounts in other currencies than the US Dollar that will appear to be higher/or lower than the actual amount in US Dollar. \n\nLet's fillter the dataset for only budget denoted in dollars and perform the correlation again.","metadata":{}},{"cell_type":"code","source":"temp_df2 = temp_df[temp_df['symbol'] == '$']\n\ntemp_df2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, now we have roughly __6,400__ rows of data","metadata":{}},{"cell_type":"code","source":"temp_df2[['budget_amount', 'a_stars']].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.pairplot(temp_df2[['budget_amount', 'a_stars']], corner=True)\n# g.fig.set_figheight(10)\ng.fig.set_figwidth(15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While there's an improvement in the correlation values when considering only budgets denoted in US Dollars, it is still a weak relationship and nothing much can be made of it. Perhaps, during `Feature engineering`, after converting all currencies to the dollar, we can get a different result (if eventually we choose to work with this dataset or the column)","metadata":{}},{"cell_type":"markdown","source":"Which companies produce the most movies and in which country are most movies produced?","metadata":{}},{"cell_type":"code","source":"# Companies that produce the most movies\n\ntop_n = get_top_n(meta_copy['production_companies'].dropna(), n=20, col='company')\n\nall_companies = top_n[1]\ntop_20_companies = top_n[0]\n\ntop_20_companies","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Warner Bros. has produced the most movies in this dataset with a total of __1,250__ movies produced, followed closely by Metro Goldwyn Meyer with __1,075__, then Paramount Pictures with __1,003__.","metadata":{}},{"cell_type":"code","source":"# create a seaborn barplot\nax = sns.barplot(top_20_companies['company'], top_20_companies['# of features'], color='blue')\n# annotate each bar with the percentage\n# of each bar represented\nfor p in ax.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()/ sum(all_companies.values()))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n# rotate xticks by 90degrees\nplt.xticks(rotation=90)\n# rename x and y label and the title of the plot\nplt.xlabel('Companies')\nplt.ylabel('# of features')\nplt.title(\"Production Companies With Most Movies\", fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_cloud(all_companies, 'Production Companies')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Countries with the most movies produced in\n\ntop_n = get_top_n(meta_copy['production_countries'].dropna(), n=20, col='countries')\n\nall_countries = top_n[1]\ntop_20_countries = top_n[0]\n\ntop_20_countries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the movies were produced in the United States of America with a whooping __21,150__ movies, which is not a surprise as the US is a powerhouse in the movie industry, followed by the United Kingdom with a meager __4,091__ movies and France with __3,936__ movies","metadata":{}},{"cell_type":"code","source":"# create a barplot in seaborn\nax = sns.barplot(top_20_countries['countries'], top_20_countries['# of features'], color='blue')\n# annotate each bar with the percentage\n# of each bar represented\nfor p in ax.patches:\n    percentage = '{:.0f}%'.format(100 * p.get_height()/ sum(all_countries.values()))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n# rotate xticks by 20degrees\nplt.xticks(rotation=90)\n# rename x and y label and the title of the plot\nplt.xlabel('Countries')\nplt.ylabel('# of features')\nplt.title(\"Production Countries\", fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_cloud(all_countries, \"Production Countries\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is telling of the magnitude of difference between the united states and other countries","metadata":{}},{"cell_type":"code","source":"del temp_df, temp_df2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now done with exploring the meta_data dataset. In this section, we looked at the distribution of some certain features, how some features differ from similar features and determined the relationship between two or more numeric features. We found that budget and revenue have a moderately positive relationship, so it is either we drop one column or engineer a new feature from both columns (Profit, for example).\n\nNext we explore genome_scores and genome_tags","metadata":{}},{"cell_type":"markdown","source":"#### 4. GENOME SCORES AND GENOME TAGS","metadata":{}},{"cell_type":"code","source":"# show the head of the genome_scores\ngenome_scores.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Questions:**\n\n1. How many unique movies are in the dataset\n2. How many unique tags are in the dataset\n3. What are the Average, Minimum and Maximum relevance scores","metadata":{}},{"cell_type":"code","source":"# Question 1: How many unique movies are in the dataset\nprint(f\"There are {genome_scores['movieId'].nunique()} unique movies in the dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 1:**\n\nThere are **13,816** unique movies in the dataset","metadata":{}},{"cell_type":"code","source":"# Question 2: How many unique tags are in the dataset\nprint(f\"There are {genome_scores['tagId'].nunique()} unique tags in the dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 2:**\n\nThere are **1,128** unique tags in the dataset","metadata":{}},{"cell_type":"code","source":"# descriptive statistics\ngenome_scores['relevance'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 3:**\n\nThe average relevance score is __0.116__, a minimum of __0.00025__ and a maximum of __100__ indicating that the scale of relevance is from __0% - 100%__. There, apparently, are a lot of low rated tags, with a relevance score of __0.14%__ being higher than __75%__ of the data. Only the most relevant tags (at a certain threshold) should be considered. This will be addressed in `Feature Engineering`","metadata":{}},{"cell_type":"code","source":"# view genome tags\ngenome_tags.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# descriptive statistics\ngenome_tags.describe(include='category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are __1,128__ rows, with __1,128__ unique values. Nothing much to gain from this. We will. nevertherless, look deeper when we merge the two dataframes in `Feature Engineering`.\n\n\nWe will take a look at the train dataset which contains the movie ratings and the users that provided the ratings next.","metadata":{}},{"cell_type":"markdown","source":"#### 5. TRAIN (RATINGS) DATASET","metadata":{}},{"cell_type":"code","source":"train.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Descriptive statistics\ntrain.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can deduce that there are __10 Million__ rows of data for each column, meaning no missing data as we've established when we took a <a href=\"#sneak-peek\">sneak peek</a> into the data earlier.","metadata":{}},{"cell_type":"markdown","source":"**QUESTIONS**\n\n1. How many unique users are represented in the dataset\n2. How many movies were rated in the dataset\n3. What are the Mean, Minimum and Maximum ratings","metadata":{}},{"cell_type":"code","source":"# Question 1: How many unique users are represented in the dataset\n\nprint(f\"There are {train['userId'].nunique()} users in the dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 1:**\n\nThere are __162,541__ unique users in the dataset","metadata":{}},{"cell_type":"code","source":"# Question 2: How many movies were rated in the dataset\n\nprint(f\"There are {train['movieId'].nunique()} movies in the dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANSWER 2:**\n\nThere are __48,213__ movies rated in the dataset","metadata":{}},{"cell_type":"markdown","source":"**ANSWER 3:**\n\nThe average rating is __3.5__, a minimum of __0.5__ and a maximum of __5.0__. Indicating that rating is on a scale of __0 - 5__ points","metadata":{}},{"cell_type":"code","source":"# lets see how ratings distributed \nratings = train['rating'].value_counts().reset_index()\nratings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create barplot with seaborn\nax = sns.barplot(ratings['index'], ratings['rating'], color='darkblue',\n                order=ratings.sort_values('rating',ascending = False)['index'])\n# annotate each bar with the percentages\n# of each bar represented\nfor p in ax.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()/len(train['rating'].tolist()))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Majority of users gave movies a rating of __4.0__ at __26.5%__, followed by __3.0__ at __19.6%__.","metadata":{}},{"cell_type":"code","source":"# What movies received the most ratings by volume of users who rated.\n# get the frequency distribution of the \n# most rated movies by volumn of the number\n# of users who left a rating\nmovies_most_rated = train.groupby(['movieId'])\\\n                            .agg({'rating':'mean', 'userId':'count'}).reset_index()\\\n                            .rename(columns={'rating':'ave_rating', 'userId':'rating_count'})\\\n                            .sort_values(['rating_count', 'ave_rating'], ascending=False)\n\nmost_rated_20 = movies_most_rated.head(20).reset_index().drop('index',axis=1)\n\nmost_rated_20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies_most_rated.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use a plot to tell us a better story","metadata":{}},{"cell_type":"code","source":"# create a barplot with seaborn\nax = sns.barplot(most_rated_20['movieId'], most_rated_20['rating_count'], color='darkblue',\n                order=most_rated_20.sort_values('rating_count',ascending = False).movieId)\n# annotate each bar with the average rating\n# of each bar represented\nfor i, p in enumerate(ax.patches):\n    ratings = '{:.2f}'.format(round(most_rated_20['ave_rating'].tolist()[i],2))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(ratings, (x, y),ha='right', fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interpretation:\n\nThe movie with the highest number of ratings is the movie with Id __318__, which has over __32,000__ user ratings and an average rating of __4.42__.\n\nThe rating of a movie is most trustworthy when it has been rated by a lot of users. Movie 318 must be a great movie. What is movie __318__? Let's ask _Movies_","metadata":{}},{"cell_type":"code","source":"movie_318 = movies['title'][movies['movieId'] == 318]\n\nprint(f\"Movie 318: {movie_318.values}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### MOVIE 318 IS **THE SHAWSHANK REDEMPTION**.\n\nlittle wonder it has been rated the most.","metadata":{}},{"cell_type":"markdown","source":"**QUESTION?**\n\nWhich user has rated the most movies?","metadata":{}},{"cell_type":"code","source":"# create a frequency table for the most\n# frequent user\nfrequent_user = train.groupby(['userId'])\\\n                            .agg({'rating':'mean', 'movieId':'count'}).reset_index()\\\n                            .rename(columns={'rating':'ave_rating', 'movieId':'movies_count'})\\\n                            .sort_values(['movies_count', 'ave_rating'], ascending=False)\n\nmost_ratings_20 = frequent_user.head(20).reset_index().drop('index',axis=1)\n\nmost_ratings_20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a barplot with seaborn\nax = sns.barplot(most_ratings_20['userId'], most_ratings_20['movies_count'], color='darkblue',\n                 order=most_ratings_20.sort_values('movies_count',ascending = False).userId)\n# annotate wach bar in the plot with the rating value\n# represented by the bar\nfor i, p in enumerate(ax.patches):\n    ratings = '{:.2f}'.format(round(most_ratings_20['ave_rating'].tolist()[i],2))\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(ratings, (x, y),ha='right', fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"User __72315__ has rated __12, 952__ movies, with an average rating of __3.09__. <h2>Stellar!<h2>","metadata":{}},{"cell_type":"code","source":"train[train.userId == 72315]['rating'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This user has rated more movies a __3.0__ and __3.5__ than other scales combined.","metadata":{}},{"cell_type":"markdown","source":"In this session, we have dived into the world of ratings and have answered a couple of interesting questions.\n\n\nAnd for the final show of the Exploratory Data Analysis, we will look into the tags dataframe.","metadata":{}},{"cell_type":"markdown","source":"#### 6. TAGS DATA","metadata":{}},{"cell_type":"code","source":"tags.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Descriptive Statistics\ntags.describe(include='category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are __73,050__ unique tags in this dataframe, with Sci-fi taking the lead with __8,330__ occurences.","metadata":{}},{"cell_type":"code","source":"# number of unique users\nprint(f\"There are {tags.userId.nunique()} unique users in the dataframe\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of unique movies\nprint(f\"There are {tags.movieId.nunique()} unique movies in the dataframe\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get how freequently tags appears\ntags_freq = {}\n# iterate over tag column of tags\n# dataframe\nfor tag in tags['tag']:\n    #if tag in tag freq, add 1 to the value\n    #of the tag key\n    if tag in tags_freq:\n        tags_freq[tag] += 1\n    #if tag not in tag freq\n    #set tag key to value 1\n    else:\n        tags_freq[tag] = 1\n\nword_cloud(tags_freq, 'tags')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see how frequently each tag occurs; the bigger the tag, the more frequent it is.","metadata":{}},{"cell_type":"markdown","source":"That concludes the `Exploratory Data Analysis` Section and it has been a long ride. Here, we have been able to drilldown into our datasets and unearth some details about the features they contain and how that might affect the **Movie Recommendation Systems** we want to develop.\n\nAs all these datasets cannot be used individually to make recommendations, they have to be merged into one dataframe, taking only the key features that are of importance to our algorithms and discarding the rest. We may also be required to engineer new features. With that said, we will go into `Feature Engineering` next.","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='FE'>\n    <h2 style='text-transform: uppercase;'>Feature Engineering</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Feature engineering or feature extraction is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. The motivation is to use these extra features to improve the quality of results from a machine learning process, compared with supplying only the raw data to the machine learning process. - wikipedia\n\nFeature engineering is a machine learning technique that leverages data to create new variables that aren't in the training set. It can produce new features for both supervised and unsupervised learning, with the goal of simplifying and speeding up data transformations while also enhancing model accuracy. - TowardsDataScience","metadata":{}},{"cell_type":"markdown","source":"Our first point of call will be to determine which of Imdb data and meta_data should we use, or what features from both datasets we can combine.\n\nWe have seen that meta_data has more observations than imdb data. Meta_data has 42,277 unique movies compared to 27,778 movies in the Imdb data. Both datasets have important features that will ben of immense help.\n\nSo, we will merge both dataframes, only picking our features that interest us.","metadata":{}},{"cell_type":"code","source":"# print imdb_data columns\nprint(list(imdb_copy.columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:07.678753Z","iopub.execute_input":"2022-05-04T19:21:07.679169Z","iopub.status.idle":"2022-05-04T19:21:07.686629Z","shell.execute_reply.started":"2022-05-04T19:21:07.679130Z","shell.execute_reply":"2022-05-04T19:21:07.685765Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# print meta_data columns\nprint(list(meta_copy.columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:07.807041Z","iopub.execute_input":"2022-05-04T19:21:07.808474Z","iopub.status.idle":"2022-05-04T19:21:07.814365Z","shell.execute_reply.started":"2022-05-04T19:21:07.808404Z","shell.execute_reply":"2022-05-04T19:21:07.813013Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# select only columns that are of interest to us\nimdb_columns_of_interest = ['movieId', 'title_cast', 'director', 'runtime', 'plot_keywords']\nmeta_columns_of_interest = ['imdbId', 'title', 'spoken_languages', 'overview', 'popularity',\n                            'production_companies', 'production_countries',\n                            'tagline', 'vote_average', 'vote_count']","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:08.365350Z","iopub.execute_input":"2022-05-04T19:21:08.366513Z","iopub.status.idle":"2022-05-04T19:21:08.372973Z","shell.execute_reply.started":"2022-05-04T19:21:08.366427Z","shell.execute_reply":"2022-05-04T19:21:08.371896Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"There are seemingly no similar columns between the 2 datasets and as such, merging will prove very difficult to do. Luckily for us, we have a data of links to assist us with this.","metadata":{}},{"cell_type":"code","source":"links.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:08.618545Z","iopub.execute_input":"2022-05-04T19:21:08.618829Z","iopub.status.idle":"2022-05-04T19:21:08.629245Z","shell.execute_reply.started":"2022-05-04T19:21:08.618799Z","shell.execute_reply":"2022-05-04T19:21:08.628591Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"The links dataset has 3 columns, all of which are 'Primary Keys' to other tables. We are only interested in __movieId__ and __imdbId__.\n\nFirst, we will merge meta_data with links, then merge the output with the imdb data","metadata":{}},{"cell_type":"code","source":"# merge links and meta_copy\nmeta_link = links.merge(meta_copy[meta_columns_of_interest], on='imdbId')\nmeta_link.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:08.726010Z","iopub.execute_input":"2022-05-04T19:21:08.726338Z","iopub.status.idle":"2022-05-04T19:21:08.840989Z","shell.execute_reply.started":"2022-05-04T19:21:08.726298Z","shell.execute_reply":"2022-05-04T19:21:08.840239Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# make a soup of 'overview' column and 'tagline' column\nmeta_link['movie_description'] = meta_link['overview'].str.lower() + \" \" + meta_link['tagline'].str.lower()\n# convert column to lowercase\nmeta_link['title'] = meta_link['title'].str.lower()\n# drop unwanted columns\ncolumns_to_drop = ['tmdbId', 'overview', 'tagline']\nmeta_link.drop(columns_to_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:08.914500Z","iopub.execute_input":"2022-05-04T19:21:08.915480Z","iopub.status.idle":"2022-05-04T19:21:09.099313Z","shell.execute_reply.started":"2022-05-04T19:21:08.915435Z","shell.execute_reply":"2022-05-04T19:21:09.098073Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"meta_link.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:09.101355Z","iopub.execute_input":"2022-05-04T19:21:09.101687Z","iopub.status.idle":"2022-05-04T19:21:09.118238Z","shell.execute_reply.started":"2022-05-04T19:21:09.101644Z","shell.execute_reply":"2022-05-04T19:21:09.117386Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# Merge meta_link and imdb dataframes\nimdb_meta = meta_link.merge(imdb_copy[imdb_columns_of_interest], how='left', on='movieId')\n\nimdb_meta.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:09.119681Z","iopub.execute_input":"2022-05-04T19:21:09.119940Z","iopub.status.idle":"2022-05-04T19:21:09.211084Z","shell.execute_reply.started":"2022-05-04T19:21:09.119888Z","shell.execute_reply":"2022-05-04T19:21:09.210096Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# Merge imdb_meta and movies dataframes\nall_movies = movies_copy.merge(imdb_meta.drop('title', axis=1), on='movieId')\n\nall_movies.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:09.213394Z","iopub.execute_input":"2022-05-04T19:21:09.214108Z","iopub.status.idle":"2022-05-04T19:21:09.336581Z","shell.execute_reply.started":"2022-05-04T19:21:09.214056Z","shell.execute_reply":"2022-05-04T19:21:09.335339Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# drop imdbId\nall_movies.drop('imdbId', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:09.338089Z","iopub.execute_input":"2022-05-04T19:21:09.338425Z","iopub.status.idle":"2022-05-04T19:21:09.376643Z","shell.execute_reply.started":"2022-05-04T19:21:09.338384Z","shell.execute_reply":"2022-05-04T19:21:09.375767Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"Let's bring in tags. \n\nWe will work with Genome Scores, to get the relevant tags at a certain threshold, Genome tags to the tags and possibly Tags df.","metadata":{}},{"cell_type":"code","source":"# group tag df on movie id, squeezing tag into a list\ntags_grouped = tags.drop('userId',axis=1).groupby(['movieId'])['tag'].apply(list).reset_index()\n\ntags_grouped.tail(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:09.378552Z","iopub.execute_input":"2022-05-04T19:21:09.378808Z","iopub.status.idle":"2022-05-04T19:21:12.505348Z","shell.execute_reply.started":"2022-05-04T19:21:09.378776Z","shell.execute_reply":"2022-05-04T19:21:12.504278Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# make tag column a string of tags separated by a space (' ')\ndef tag_to_str(tag_list):\n    \"\"\"\n    this function converts a list \n    to a string.\n    \"\"\"\n    try:\n        tag_str = ' '.join(tag_list)\n    except (TypeError, ValueError):\n        tag_str = str(tag_list)\n        \n    return tag_str\n\n# apply tag_to_str function\ntags_grouped['tag'] = tags_grouped['tag'].apply(tag_to_str)\ntags_grouped.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:12.506875Z","iopub.execute_input":"2022-05-04T19:21:12.507204Z","iopub.status.idle":"2022-05-04T19:21:12.619108Z","shell.execute_reply.started":"2022-05-04T19:21:12.507170Z","shell.execute_reply":"2022-05-04T19:21:12.618263Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"# Set relevance threshold for genome tags\nthreshold = 0.80 # 80% relevance at least\n\n# filter genome scores based on the threshold\nrelevant_genomes = genome_scores[genome_scores.relevance >= threshold]\n\n# merge genome tags and relevant genomes \nscores_tags = genome_tags.merge(relevant_genomes, on='tagId')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:12.621274Z","iopub.execute_input":"2022-05-04T19:21:12.621532Z","iopub.status.idle":"2022-05-04T19:21:13.081169Z","shell.execute_reply.started":"2022-05-04T19:21:12.621499Z","shell.execute_reply":"2022-05-04T19:21:13.080226Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"movie_count = scores_tags.groupby('tag')['movieId'].count()\\\n                .reset_index().rename(columns={'movieId':'movieId_counts'})","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:13.082437Z","iopub.execute_input":"2022-05-04T19:21:13.084144Z","iopub.status.idle":"2022-05-04T19:21:13.094360Z","shell.execute_reply.started":"2022-05-04T19:21:13.084094Z","shell.execute_reply":"2022-05-04T19:21:13.093193Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"This whole process takes a long time to complete, so we have a saved copy of the output that has been loaded already. let's just bring that here. \n\nTo run the code, nonetheless, just uncomment the block of code.","metadata":{}},{"cell_type":"code","source":"# from fuzzywuzzy import fuzz\n# from fuzzywuzzy import process\n\n# \"\"\"\n# Fuzzywuzzy is a library used for string matching. Fuzzy string matching is the process of finding strings that match a given pattern.\n# Basically, it uses Levenshtein distance to calculate the differences between sequences.\n# \"\"\"\n\n# match_list = []\n# ratio_list = []\n\n# bad_tags = scores_tags.tag.values\n# good_tags = movie_count.tag[movie_count.movieId_counts > 100].values\n\n# threshold = 80\n\n# for b_tag in bad_tags:\n#     process_extract = process.extractOne(b_tag, good_tags, scorer=fuzz.token_sort_ratio)\n#     match_list.append(process_extract[0])\n#     ratio_list.append(process_extract[1])\n\n# scores_tags['matches'] = match_list\n# scores_tags['match_ratio'] = ratio_list","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:13.096083Z","iopub.execute_input":"2022-05-04T19:21:13.096380Z","iopub.status.idle":"2022-05-04T19:21:13.110545Z","shell.execute_reply.started":"2022-05-04T19:21:13.096336Z","shell.execute_reply":"2022-05-04T19:21:13.109539Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"# make a copy of scores_n_tags df\nscores_tags = scores_n_tags.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:13.112408Z","iopub.execute_input":"2022-05-04T19:21:13.112948Z","iopub.status.idle":"2022-05-04T19:21:13.127477Z","shell.execute_reply.started":"2022-05-04T19:21:13.112874Z","shell.execute_reply":"2022-05-04T19:21:13.126709Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"print(scores_tags.shape)\nscores_tags.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:13.128867Z","iopub.execute_input":"2022-05-04T19:21:13.129169Z","iopub.status.idle":"2022-05-04T19:21:13.153156Z","shell.execute_reply.started":"2022-05-04T19:21:13.129136Z","shell.execute_reply":"2022-05-04T19:21:13.152378Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"filtered_tags_threshold = scores_tags[scores_tags.match_ratio >= threshold]\nadjusted_tags = pd.merge(scores_tags[['movieId', 'tag']], \\\n                         filtered_tags_threshold[['tag', 'matches']], on='tag')\ncleaned_tags = adjusted_tags.groupby(['movieId', 'matches'])['tag'].count()\\\n                            .reset_index().rename(columns={'tag':'tag_count', 'matches':'tag'})\n\ncleaned_tags = cleaned_tags.groupby('movieId')['tag'].agg(list).reset_index()\n\ncleaned_tags.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:13.156326Z","iopub.execute_input":"2022-05-04T19:21:13.156993Z","iopub.status.idle":"2022-05-04T19:21:36.830553Z","shell.execute_reply.started":"2022-05-04T19:21:13.156945Z","shell.execute_reply":"2022-05-04T19:21:36.829742Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"cleaned_tags['tag'] = cleaned_tags['tag'].apply(tag_to_str)\ncleaned_tags.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:36.831837Z","iopub.execute_input":"2022-05-04T19:21:36.832672Z","iopub.status.idle":"2022-05-04T19:21:36.858401Z","shell.execute_reply.started":"2022-05-04T19:21:36.832626Z","shell.execute_reply":"2022-05-04T19:21:36.857407Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# Merge cleaned tags and tag_grouped\ntags_merged = tags_grouped.merge(cleaned_tags, how='left', on='movieId')\n# fill na with blank\ntags_merged = tags_merged.fillna('')\n# concatenate both columns\ntags_merged['tag'] = tags_merged['tag_x'] + \" \" + tags_merged['tag_y']","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:36.859990Z","iopub.execute_input":"2022-05-04T19:21:36.860304Z","iopub.status.idle":"2022-05-04T19:21:36.928665Z","shell.execute_reply.started":"2022-05-04T19:21:36.860259Z","shell.execute_reply":"2022-05-04T19:21:36.927687Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# drop unwanted columns\ntags_merged.drop(['tag_x', 'tag_y'], axis=1, inplace=True)\ntags_merged.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:36.929820Z","iopub.execute_input":"2022-05-04T19:21:36.930074Z","iopub.status.idle":"2022-05-04T19:21:36.952299Z","shell.execute_reply.started":"2022-05-04T19:21:36.930044Z","shell.execute_reply":"2022-05-04T19:21:36.951380Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"We will merge the train dataset and all_movies. First, we are going to group the train dataset based on movies and get the average rating of each movie and the number of ratings the movie received. We have done this in the <a href=#eda>EDA</a> section, but we will redo it here.\n\nFinally merge with tage_merged","metadata":{}},{"cell_type":"code","source":"# get average rating per movie\nave_ratings = train.groupby(['movieId'])\\\n                            .agg({'rating':'mean', 'userId':'count'}).reset_index()\\\n                            .rename(columns={'rating':'ave_rating', 'userId':'rating_count'})\nprint(ave_ratings.shape)\nave_ratings.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:36.953774Z","iopub.execute_input":"2022-05-04T19:21:36.954269Z","iopub.status.idle":"2022-05-04T19:21:37.460943Z","shell.execute_reply.started":"2022-05-04T19:21:36.954223Z","shell.execute_reply":"2022-05-04T19:21:37.460062Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# merge ave_ratings and all_movies\n\nfull_movie = ave_ratings.merge(all_movies, how='right', on='movieId')\n\n# rearrange dataset\ncolumns = ['movieId', 'title', 'genres', 'title_cast', 'director', 'production_companies',\n           'production_countries', 'movie_description', 'plot_keywords', 'spoken_languages', \n           'year', 'runtime', 'ave_rating', 'rating_count', 'vote_average', \n           'vote_count', 'popularity']\nfull_movie = full_movie[columns]\n\nfull_movie.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:37.462628Z","iopub.execute_input":"2022-05-04T19:21:37.463288Z","iopub.status.idle":"2022-05-04T19:21:37.615355Z","shell.execute_reply.started":"2022-05-04T19:21:37.463232Z","shell.execute_reply":"2022-05-04T19:21:37.614690Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# merge full movies and tags on movieId\n# how='left' is specified for a left join\nfull_movies = full_movie.merge(tags_merged, how='left', on='movieId')\nfull_movies['tag'] = full_movies['tag'].fillna('').str.lower()\nfull_movies.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:37.616513Z","iopub.execute_input":"2022-05-04T19:21:37.616862Z","iopub.status.idle":"2022-05-04T19:21:37.727381Z","shell.execute_reply.started":"2022-05-04T19:21:37.616833Z","shell.execute_reply":"2022-05-04T19:21:37.726532Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"Next, we will lemmatize and stem the **movie-descriptions**, **plot_keywords** and **tags**. First, What are Lemmatization and Stemming?\n\n\nLemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word.\n\nStemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words chocolates, chocolatey, choco to the root word, chocolate and retrieval, retrieved, retrieves reduce to the stem retrieve. This is very important because we want similar words grouped, to give our algorithm a better accuracy\n\nBoth Lemming and Stemming both take as inputs, tokenized words.\n\nTokenization is a process of converting a string of words into a list of separated individual words.","metadata":{}},{"cell_type":"code","source":"# lets define our tokenizer\n\ndef tokenize(text):\n    return text.split(' ')\n\n# Lemming and stemming function\n\ndef transform(text):\n    # tokenize words\n    words = tokenize(text)\n    \n    # define both Lemmatizer and stemmer\n    lemmer = WordNetLemmatizer()\n    stemmer = SnowballStemmer(language='english')\n    \n    # lemmatize and stem words\n    lemmatized = [lemmer.lemmatize(x) for x in words]\n    stemmed = [stemmer.stem(x) for x in lemmatized]\n    \n    return ' '.join(stemmed)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:37.728674Z","iopub.execute_input":"2022-05-04T19:21:37.729483Z","iopub.status.idle":"2022-05-04T19:21:37.736000Z","shell.execute_reply.started":"2022-05-04T19:21:37.729439Z","shell.execute_reply":"2022-05-04T19:21:37.735318Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# fill null values with blanks\nfull_movies[['movie_description', 'plot_keywords', 'tag']] = full_movies[['movie_description', 'plot_keywords', 'tag']].fillna('')\n# apply transform function to text columns\nfull_movies['movie_description'] = full_movies['movie_description'].apply(transform)\nfull_movies['plot_keywords'] = full_movies['plot_keywords'].apply(transform)\nfull_movies['tag'] = full_movies['tag'].apply(transform)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:21:37.737219Z","iopub.execute_input":"2022-05-04T19:21:37.737448Z","iopub.status.idle":"2022-05-04T19:22:52.620176Z","shell.execute_reply.started":"2022-05-04T19:21:37.737420Z","shell.execute_reply":"2022-05-04T19:22:52.618972Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# Check number and percentage of missing data\n\n# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\nfull_movies_missing = pd.concat([full_movies.isnull().sum(), round(full_movies.isnull().sum()/full_movies.shape[0] * 100)], axis=1)\nfull_movies_missing.columns = ['missing_count', 'missing_percentage'] # rename columns\nfull_movies_missing","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:52.622353Z","iopub.execute_input":"2022-05-04T19:22:52.622738Z","iopub.status.idle":"2022-05-04T19:22:52.752054Z","shell.execute_reply.started":"2022-05-04T19:22:52.622679Z","shell.execute_reply":"2022-05-04T19:22:52.750715Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"def fill_na(df):\n    \n    # iterate over columns in the dataframe\n    for column in df.columns:\n        # impute based on the type of column\n        if df[column].dtype == 'object':\n            # fill categorical columns with blanks\n            df[column] = df[column].fillna('')\n        elif df[column].dtype == 'float32' or df[column].dtype == 'float64':\n            # fill numerical columns with the mean value of the column\n            df[column] = df[column].fillna(df[column].mean())\n    #return dataframe\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:52.753602Z","iopub.execute_input":"2022-05-04T19:22:52.753839Z","iopub.status.idle":"2022-05-04T19:22:52.762248Z","shell.execute_reply.started":"2022-05-04T19:22:52.753802Z","shell.execute_reply":"2022-05-04T19:22:52.760967Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# apply fill_na function\nfull_movies = fill_na(full_movies)\n\nfull_movies.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:52.764131Z","iopub.execute_input":"2022-05-04T19:22:52.764650Z","iopub.status.idle":"2022-05-04T19:22:52.923391Z","shell.execute_reply.started":"2022-05-04T19:22:52.764604Z","shell.execute_reply":"2022-05-04T19:22:52.922331Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"# check if fill_na worked.\nfull_movies_missing = pd.concat([full_movies.isnull().sum(), round(full_movies.isnull().sum()/full_movies.shape[0] * 100)], axis=1)\nfull_movies_missing.columns = ['missing_count', 'missing_percentage'] # rename columns\nfull_movies_missing","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:52.924899Z","iopub.execute_input":"2022-05-04T19:22:52.925160Z","iopub.status.idle":"2022-05-04T19:22:53.057974Z","shell.execute_reply.started":"2022-05-04T19:22:52.925130Z","shell.execute_reply":"2022-05-04T19:22:53.056995Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"Fill_na worked a charm. The categorical columns are filled with blanks(''), while the numerical columns are filled with the mean value of the column.","metadata":{}},{"cell_type":"markdown","source":"Next, we will remove stopwords from 'movie_description","metadata":{}},{"cell_type":"code","source":"full_movies['movie_description'] = full_movies['movie_description'].apply(remove_stopwords)\nfull_movies['tag'] = full_movies['tag'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:53.059331Z","iopub.execute_input":"2022-05-04T19:22:53.059580Z","iopub.status.idle":"2022-05-04T19:22:53.872169Z","shell.execute_reply.started":"2022-05-04T19:22:53.059549Z","shell.execute_reply":"2022-05-04T19:22:53.870526Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's downcast the columns of the dataset, to reduce the size of the dataset, using 'convert_columns' custom function","metadata":{}},{"cell_type":"code","source":"full_movies = convert_columns(full_movies)\nfull_movies.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:53.874251Z","iopub.execute_input":"2022-05-04T19:22:53.874570Z","iopub.status.idle":"2022-05-04T19:22:54.621652Z","shell.execute_reply.started":"2022-05-04T19:22:53.874528Z","shell.execute_reply":"2022-05-04T19:22:54.620836Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"We have come to the end of feature engineering. Here we drop features, engineered a new one, merged dataframes and finally downcasted columns to reduce the size of the final dataframe. Now we can go ahead an create our recommender systems.","metadata":{}},{"cell_type":"code","source":"del cleaned_tags, tags_merged, all_movies, full_movie","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:54.626708Z","iopub.execute_input":"2022-05-04T19:22:54.627000Z","iopub.status.idle":"2022-05-04T19:22:54.635857Z","shell.execute_reply.started":"2022-05-04T19:22:54.626960Z","shell.execute_reply":"2022-05-04T19:22:54.634923Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='recommender'>\n    <h2>RECOMMENDER SYSTEMS</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"A brief introduction into recommender systems and the types there are was given in <a href='#introduction'>Introduction</a>","metadata":{}},{"cell_type":"markdown","source":"#### IMDB TOP 250","metadata":{}},{"cell_type":"markdown","source":"Before we dive in, let's get the IMDB top 250 movies, based on a calculation developed by IMDB.\n\n$$ \nWeighted Rating(WR) = \\frac{v}{v + m} * R + \\frac{m}{v + m} * C\n$$\n\nWhere;\n - v = Number of votes for the movie\n - m = minimum votes required to be listed in the Top 250\n - R = average rating for the movie (mean rating)\n - C = mean vote across the whole report","metadata":{}},{"cell_type":"code","source":"# calculate the value of C\nC = full_movies['vote_average'].mean()\n\n# get the minimum votes require to be listed\n# we are taking values in the top 10%\nm = full_movies['vote_count'].quantile(.90)\n\nC, m","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:16:19.584535Z","iopub.execute_input":"2022-05-04T20:16:19.584819Z","iopub.status.idle":"2022-05-04T20:16:19.594698Z","shell.execute_reply.started":"2022-05-04T20:16:19.584789Z","shell.execute_reply":"2022-05-04T20:16:19.593744Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"# Calculating IMDB's weighted rating\n\nfull_movies['imdb_wr'] = ((full_movies['vote_count']/(full_movies['vote_count']+m)) * full_movies['ave_rating'] +\\\n                        (m/(full_movies['vote_count']+m)) * round(C, 2))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:17:12.563694Z","iopub.execute_input":"2022-05-04T20:17:12.564040Z","iopub.status.idle":"2022-05-04T20:17:12.577523Z","shell.execute_reply.started":"2022-05-04T20:17:12.564007Z","shell.execute_reply":"2022-05-04T20:17:12.576817Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"top_250_movies = full_movies.sort_values(by='imdb_wr').head(250)\ntop_250_movies = top_250_movies.title.tolist()\nprint(top_250_movies)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T20:18:07.109967Z","iopub.execute_input":"2022-05-04T20:18:07.110275Z","iopub.status.idle":"2022-05-04T20:18:07.125780Z","shell.execute_reply.started":"2022-05-04T20:18:07.110243Z","shell.execute_reply":"2022-05-04T20:18:07.125180Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"markdown","source":"#### HIGHEST RATED 100","metadata":{}},{"cell_type":"code","source":"top_100 = full_movies.sort_values(by='rating_count', ascending=False).head(100)\ntop_rated_100 = top_100.sort_values(by='ave_rating', ascending=False)\ntop_rated_100 = top_rated_100['title'].tolist()\nprint(top_rated_100)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:54.678246Z","iopub.execute_input":"2022-05-04T19:22:54.678815Z","iopub.status.idle":"2022-05-04T19:22:54.696110Z","shell.execute_reply.started":"2022-05-04T19:22:54.678780Z","shell.execute_reply":"2022-05-04T19:22:54.695145Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"#### MOST POPULAR 100","metadata":{}},{"cell_type":"code","source":"most_popular_100 = full_movies.sort_values(by='popularity', ascending=False).head(100)\nmost_popular_100 = most_popular_100['title'].tolist()\nprint(most_popular_100)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:54.697255Z","iopub.execute_input":"2022-05-04T19:22:54.697840Z","iopub.status.idle":"2022-05-04T19:22:54.717241Z","shell.execute_reply.started":"2022-05-04T19:22:54.697807Z","shell.execute_reply":"2022-05-04T19:22:54.716502Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"### 1. CONTENT BASED RECOMMENDER SYSTEM","metadata":{}},{"cell_type":"code","source":"# a handy function to create cosine similarities\n\ndef get_similarity(series):\n    tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 1))\n    tfidf_matrix = tf.fit_transform(series)\n\n    tfidf_matrix.shape\n    tfidf_matrix = tfidf_matrix.astype('float32')\n    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n    \n    return cosine_sim","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:54.719149Z","iopub.execute_input":"2022-05-04T19:22:54.719528Z","iopub.status.idle":"2022-05-04T19:22:54.726139Z","shell.execute_reply.started":"2022-05-04T19:22:54.719460Z","shell.execute_reply":"2022-05-04T19:22:54.725191Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"def content_model(category):\n    cosine_sim = get_similarity(category)\n    return cosine_sim\n\ndef get_recommendations(title, category, n=10):\n    \n    cosine_sim = content_model(category)\n    movies = full_movies[['movieId', 'title']]\n    movies  = movies.reset_index()\n    titles = movies['title']\n    indices = pd.Series(movies.index, index=movies['title'])\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:31]\n    movie_indices = [i[0] for i in sim_scores]\n    return titles.iloc[movie_indices].astype('object').tolist()[:n]","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:54.727900Z","iopub.execute_input":"2022-05-04T19:22:54.728371Z","iopub.status.idle":"2022-05-04T19:22:54.742505Z","shell.execute_reply.started":"2022-05-04T19:22:54.728336Z","shell.execute_reply":"2022-05-04T19:22:54.741724Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"title = 'dark knight, the'\n\nmovies = full_movies[['movieId', 'title']]\nmovies  = movies.reset_index()\ntitles = movies['title']\nindices = pd.Series(movies.index, index=movies['title'])\nidx = indices[title]\nidx","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:54.743941Z","iopub.execute_input":"2022-05-04T19:22:54.744748Z","iopub.status.idle":"2022-05-04T19:22:54.770073Z","shell.execute_reply.started":"2022-05-04T19:22:54.744686Z","shell.execute_reply":"2022-05-04T19:22:54.769230Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"\n#### Based on the cast and crew of the movie","metadata":{}},{"cell_type":"code","source":"cast_n_crew = full_movies['title_cast'].astype('O') + \" \" + full_movies['director'].astype('O').apply(lambda x: ' '.join([x, x, x]))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:54.771713Z","iopub.execute_input":"2022-05-04T19:22:54.771985Z","iopub.status.idle":"2022-05-04T19:22:54.833430Z","shell.execute_reply.started":"2022-05-04T19:22:54.771953Z","shell.execute_reply":"2022-05-04T19:22:54.832427Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"get_recommendations('dark knight, the', cast_n_crew)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:54.835764Z","iopub.execute_input":"2022-05-04T19:22:54.836376Z","iopub.status.idle":"2022-05-04T19:22:58.388050Z","shell.execute_reply.started":"2022-05-04T19:22:54.836318Z","shell.execute_reply":"2022-05-04T19:22:58.386620Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"#### Based on plot keywords and movie description","metadata":{}},{"cell_type":"code","source":"keywords = full_movies['plot_keywords'].astype('object') + \" \" + full_movies['movie_description'].astype('object')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:58.389611Z","iopub.execute_input":"2022-05-04T19:22:58.389961Z","iopub.status.idle":"2022-05-04T19:22:58.413384Z","shell.execute_reply.started":"2022-05-04T19:22:58.389920Z","shell.execute_reply":"2022-05-04T19:22:58.412462Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"get_recommendations('dark knight, the', keywords)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:22:58.415042Z","iopub.execute_input":"2022-05-04T19:22:58.415601Z","iopub.status.idle":"2022-05-04T19:23:15.688323Z","shell.execute_reply.started":"2022-05-04T19:22:58.415566Z","shell.execute_reply":"2022-05-04T19:23:15.687336Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"#### Based on tags","metadata":{}},{"cell_type":"code","source":"tags = full_movies['tag']","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:23:15.689986Z","iopub.execute_input":"2022-05-04T19:23:15.690924Z","iopub.status.idle":"2022-05-04T19:23:15.695891Z","shell.execute_reply.started":"2022-05-04T19:23:15.690856Z","shell.execute_reply":"2022-05-04T19:23:15.695206Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"get_recommendations('dark knight, the', tags)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:23:15.696891Z","iopub.execute_input":"2022-05-04T19:23:15.697574Z","iopub.status.idle":"2022-05-04T19:23:35.481754Z","shell.execute_reply.started":"2022-05-04T19:23:15.697482Z","shell.execute_reply":"2022-05-04T19:23:35.480679Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"#### Based on production companies and countries","metadata":{}},{"cell_type":"code","source":"production = full_movies['production_companies'].astype('object') + \" \" + full_movies['production_countries'].astype('object')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:23:35.483543Z","iopub.execute_input":"2022-05-04T19:23:35.483894Z","iopub.status.idle":"2022-05-04T19:23:35.512822Z","shell.execute_reply.started":"2022-05-04T19:23:35.483849Z","shell.execute_reply":"2022-05-04T19:23:35.511845Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"get_recommendations('dark knight, the', production)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:23:35.515130Z","iopub.execute_input":"2022-05-04T19:23:35.515409Z","iopub.status.idle":"2022-05-04T19:23:51.994509Z","shell.execute_reply.started":"2022-05-04T19:23:35.515377Z","shell.execute_reply":"2022-05-04T19:23:51.993594Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"## RATINGS PREDICTION","metadata":{}},{"cell_type":"code","source":"def split_text(text, sep):\n    split_text = text.split(sep)\n    return split_text\n\ndef join_lists(list_):\n    return ' '.join(list_)\n\ndef stringify(num):\n    num = str(num)\n    return num","metadata":{"id":"iE-qn_PPDwCQ","execution":{"iopub.status.busy":"2022-05-04T19:23:51.996769Z","iopub.execute_input":"2022-05-04T19:23:51.997092Z","iopub.status.idle":"2022-05-04T19:23:52.004385Z","shell.execute_reply.started":"2022-05-04T19:23:51.997050Z","shell.execute_reply":"2022-05-04T19:23:52.003362Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"reader = Reader()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:23:52.006305Z","iopub.execute_input":"2022-05-04T19:23:52.007093Z","iopub.status.idle":"2022-05-04T19:23:52.018761Z","shell.execute_reply.started":"2022-05-04T19:23:52.007036Z","shell.execute_reply":"2022-05-04T19:23:52.017519Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"train['userId'] = train['userId'].apply(stringify)\ntrain['movieId'] = train['movieId'].apply(stringify)\n# train['Id'] = train['userId'] + \"_\" + train['movieId']","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:23:52.035343Z","iopub.execute_input":"2022-05-04T19:23:52.035924Z","iopub.status.idle":"2022-05-04T19:24:04.734450Z","shell.execute_reply.started":"2022-05-04T19:23:52.035865Z","shell.execute_reply":"2022-05-04T19:24:04.733464Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"data = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], reader)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:24:04.736010Z","iopub.execute_input":"2022-05-04T19:24:04.736418Z","iopub.status.idle":"2022-05-04T19:24:19.815788Z","shell.execute_reply.started":"2022-05-04T19:24:04.736371Z","shell.execute_reply":"2022-05-04T19:24:19.815098Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"svd = SVD()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:24:19.816953Z","iopub.execute_input":"2022-05-04T19:24:19.817510Z","iopub.status.idle":"2022-05-04T19:24:19.822663Z","shell.execute_reply.started":"2022-05-04T19:24:19.817461Z","shell.execute_reply":"2022-05-04T19:24:19.821653Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"trainset, testset = train_test_split(data, test_size=.25)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:24:19.825147Z","iopub.execute_input":"2022-05-04T19:24:19.825580Z","iopub.status.idle":"2022-05-04T19:25:09.093617Z","shell.execute_reply.started":"2022-05-04T19:24:19.825531Z","shell.execute_reply":"2022-05-04T19:25:09.092559Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"svd.fit(trainset)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:25:09.095155Z","iopub.execute_input":"2022-05-04T19:25:09.096076Z","iopub.status.idle":"2022-05-04T19:40:31.770078Z","shell.execute_reply.started":"2022-05-04T19:25:09.095863Z","shell.execute_reply":"2022-05-04T19:40:31.769084Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"predictions = svd.test(testset)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:40:31.772142Z","iopub.execute_input":"2022-05-04T19:40:31.773039Z","iopub.status.idle":"2022-05-04T19:41:25.053998Z","shell.execute_reply.started":"2022-05-04T19:40:31.772993Z","shell.execute_reply":"2022-05-04T19:41:25.053088Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"from surprise import accuracy\n\naccuracy.rmse(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:41:25.055346Z","iopub.execute_input":"2022-05-04T19:41:25.055641Z","iopub.status.idle":"2022-05-04T19:41:29.009531Z","shell.execute_reply.started":"2022-05-04T19:41:25.055601Z","shell.execute_reply":"2022-05-04T19:41:29.008603Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"test['userId'] = test['userId'].apply(stringify)\ntest['movieId'] = test['movieId'].apply(stringify)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:41:29.011280Z","iopub.execute_input":"2022-05-04T19:41:29.012193Z","iopub.status.idle":"2022-05-04T19:41:34.611187Z","shell.execute_reply.started":"2022-05-04T19:41:29.012151Z","shell.execute_reply":"2022-05-04T19:41:34.610422Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nmodel_filename = \"/kaggle/working/models.pkl\"\n\nprint(\">>> starting pickling...\")\npickle.dump(svd, open(model_filename, 'wb'))\nprint('>>> pickling done')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:41:34.612364Z","iopub.execute_input":"2022-05-04T19:41:34.612597Z","iopub.status.idle":"2022-05-04T19:41:51.748343Z","shell.execute_reply.started":"2022-05-04T19:41:34.612569Z","shell.execute_reply":"2022-05-04T19:41:51.747487Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"model_filename = \"/kaggle/working/models.pkl\"\nmodel = pickle.load(open(model_filename, 'rb'))\n\ndef item_rating(user, item):\n    uid = str(user)\n    iid = str(item)\n    prediction = model.predict(user, item, verbose=False)\n    rating = round(prediction.est, 1)\n    \n    return rating","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:41:51.749978Z","iopub.execute_input":"2022-05-04T19:41:51.750193Z","iopub.status.idle":"2022-05-04T19:41:58.225499Z","shell.execute_reply.started":"2022-05-04T19:41:51.750165Z","shell.execute_reply":"2022-05-04T19:41:58.224642Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"test['ratings'] = test.apply(lambda row: item_rating(row['userId'], row['movieId']), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:41:58.226744Z","iopub.execute_input":"2022-05-04T19:41:58.226965Z","iopub.status.idle":"2022-05-04T19:45:22.425966Z","shell.execute_reply.started":"2022-05-04T19:41:58.226938Z","shell.execute_reply":"2022-05-04T19:45:22.425285Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T19:45:22.427248Z","iopub.execute_input":"2022-05-04T19:45:22.428005Z","iopub.status.idle":"2022-05-04T19:45:22.438114Z","shell.execute_reply.started":"2022-05-04T19:45:22.427957Z","shell.execute_reply":"2022-05-04T19:45:22.437315Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}