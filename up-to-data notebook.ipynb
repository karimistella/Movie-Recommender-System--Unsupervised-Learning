{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:45px; color: teal; letter-spacing: .1em;\">\n",
    "    MOVIES RECOMMENDATION SYSTEMS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n",
    "</div>\n",
    "<div id='introduction'>\n",
    "    <h2> INTRODUCTION </h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intelligent algorithms can help viewers sift through tens of thousands of titles to find the best ones. Recommender systems are both socially and economically important in ensuring that people can make informed decisions about the content they consume on a daily basis. This is particularly true in the case of movie recommendations.\n",
    "\n",
    "Providing an accurate and robust solution to this challenge has enormous economic potential, with users of the system receiving personalized recommendations, thereby enhancing platform affinity for the streaming services that best facilitate their audience's viewing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECOMMENDER SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recommendation system is an information filtering system whose main goal is to predict the rating or preference a user might give to an item. This helps create personalized content and better product search experience. One popular use is recommending to users which movie to watch. This is because significant dependencies exist between users and item centric activity. For example a user who is interested in s historical documentary is more likely to be interested in another historical documentary or an educational program, rather than in an action movie.\n",
    "\n",
    "A recommendation system can use either of these two techniques:\n",
    "\n",
    "- Content based filtering\n",
    "- Collaborative filtering\n",
    "\n",
    "In content based filtering, the algorithm seeks to make recommendations based on how similar the properties or features of an item are to other items.\n",
    "\n",
    "In collaborative filtering, we use similarities between users and items simultaneously to provide recommendations. This allows for serendipitous recommendations; that is, collaborative filtering models can recommend an item to user A based on the interests of a similar user B.\n",
    "\n",
    "Here we are going to explore both methods and assess which recommendation system gives us the best results. Increasing sales is the primary goal of a recommender system. By recommending carefully selected items to users, recommender systems bring relevant items to the attention of users. This increases the sales volumes and profits to the merchants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='tbl-contents'>\n",
    "    <h2>TABLE OF CONTENTS</h2>\n",
    "    <ol>\n",
    "        <li>\n",
    "            <h4>\n",
    "                <a href='#introduction'>Introduction</a>\n",
    "            </h4>\n",
    "        </li>\n",
    "        <li>\n",
    "            <h4>\n",
    "                <a href='#load-dependencies'>Load Dependencies</a>\n",
    "            </h4>\n",
    "        </li>\n",
    "        <li>\n",
    "            <h4>\n",
    "                <a href='#load-data'>Load Data</a>\n",
    "            </h4>\n",
    "        </li> \n",
    "        <li>\n",
    "            <h4>\n",
    "                <a href='#sneak-peek'>Sneak Peek into Loaded Data</a>\n",
    "            </h4>\n",
    "        </li> \n",
    "        <li>\n",
    "            <h4>\n",
    "                <a href='#data-cleaning'>Data Cleaning</a>\n",
    "            </h4>\n",
    "        </li> \n",
    "        <li>\n",
    "            <h4>\n",
    "                <a href='#eda'>Exploratory Data Analysis</a>\n",
    "            </h4>\n",
    "        </li>  \n",
    "        <li>\n",
    "            <h4>\n",
    "                <a href='#FE'>Feature Engineering</a>\n",
    "            </h4>\n",
    "        </li>  \n",
    "        <li>\n",
    "            <h4>\n",
    "                <a href='#recommender'>Recommender Systems</a>\n",
    "            </h4>\n",
    "        </li>  \n",
    "    </ol>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n",
    "</div>\n",
    "<div id='load-dependencies'>\n",
    "    <h2>LOAD DEPENDENCIES</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T19:05:42.901231Z",
     "iopub.status.busy": "2022-05-01T19:05:42.899979Z",
     "iopub.status.idle": "2022-05-01T19:05:42.942154Z",
     "shell.execute_reply": "2022-05-01T19:05:42.941119Z",
     "shell.execute_reply.started": "2022-05-01T19:05:42.901177Z"
    },
    "id": "jJoKzyWPDpVd"
   },
   "outputs": [],
   "source": [
    "import os # for os operations on kaggle\n",
    "\n",
    "# for pattern searching and extraction \n",
    "import re\n",
    "\n",
    "# libraries for data analysis and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# libraries for numerical efficiencies\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "# libraries for string matching\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# libraries for data visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# library to evaluate strings containing python literals\n",
    "from ast import literal_eval\n",
    "\n",
    "# libraries for natural language processing\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# libraries for building and analyzing recommender systems that deal with explicit rating data.\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise import KNNBasic, BaselineOnly, NMF\n",
    "\n",
    "# libraries for entity featurization and similarity computation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "\n",
    "# to ignore whatever warnings that may arise\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 10) # defaulting all plots to a fixed size\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"Blues_r\")\n",
    "# sns.set_palette(sns.dark_palette(\"#69d\"))\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n",
    "</div>\n",
    "<div id='load-data'>\n",
    "    <h2>LOAD DATASETS</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:21.572957Z",
     "iopub.status.busy": "2022-05-01T13:07:21.571902Z",
     "iopub.status.idle": "2022-05-01T13:07:21.583417Z",
     "shell.execute_reply": "2022-05-01T13:07:21.582048Z",
     "shell.execute_reply.started": "2022-05-01T13:07:21.5729Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_columns(data):\n",
    "    \"\"\"\n",
    "    This function takes in a dataset and converts the \n",
    "    dtype of each column to a lesser version to reduce\n",
    "    the size of the dataset for further operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    for col in data.columns: # iterate over the columns in the dataset\n",
    "        \n",
    "        if data[col].dtype == 'object':\n",
    "            data[col] = data[col].astype('category') # convert objects to categories\n",
    "        \n",
    "        if data[col].dtype == 'int64':\n",
    "            data[col] = data[col].astype('int32') # convert int64 to int32\n",
    "        \n",
    "        if data[col].dtype == 'float64':\n",
    "            data[col] = data[col].astype('float32') # convert float64 to float32\n",
    "        \n",
    "    return data # return converted data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T17:33:41.290308Z",
     "iopub.status.busy": "2022-05-01T17:33:41.290016Z",
     "iopub.status.idle": "2022-05-01T17:34:09.189574Z",
     "shell.execute_reply": "2022-05-01T17:34:09.188516Z",
     "shell.execute_reply.started": "2022-05-01T17:33:41.290275Z"
    },
    "id": "AX_ZbyJ6DyQG"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'movies_metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19964/2650209913.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imdb_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmovies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'movies.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmeta_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'movies_metadata.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mgenome_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'genome_scores.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgenome_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenome_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movies_metadata.csv'"
     ]
    }
   ],
   "source": [
    "# Loading datasets. Change paths when working on a different platform or personal computer\n",
    "\n",
    "imdb = pd.read_csv('imdb_data.csv')\n",
    "movies = pd.read_csv('movies.csv')\n",
    "meta_data = pd.read_csv('movies_metadata.csv')\n",
    "genome_scores = pd.read_csv('genome_scores.csv')\n",
    "genome_scores = convert_columns(genome_scores)\n",
    "genome_tags = pd.read_csv('genome_tags.csv')\n",
    "genome_tags= convert_columns(genome_tags)\n",
    "train = pd.read_csv('train.csv')\n",
    "train = convert_columns(train)\n",
    "test = pd.read_csv('test.csv')\n",
    "test = convert_columns(test)\n",
    "links = pd.read_csv('links.csv')\n",
    "links = convert_columns(links)\n",
    "tags = pd.read_csv('tags.csv')\n",
    "tags = convert_columns(tags)\n",
    "scores_n_tags = pd.read_csv('scores_tags.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n",
    "</div>\n",
    "<div id='sneak-peek'>\n",
    "    <h2 style='text-transform: uppercase;'>Sneak Peak into Loaded Data</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. IMDB DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.642672Z",
     "iopub.status.busy": "2022-05-01T13:07:52.642399Z",
     "iopub.status.idle": "2022-05-01T13:07:52.664317Z",
     "shell.execute_reply": "2022-05-01T13:07:52.663461Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.642624Z"
    }
   },
   "outputs": [],
   "source": [
    "# imdb data\n",
    "imdb.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we see that the IMDB Data is made up of __6__ columns - __movieId__, __title_cast__, __director__, __runtime__, __budget__, __plot_keywords__.\n",
    "\n",
    "We can observe that the __title_cast__ and __plot_keywords__ columns are separated by a pipe - '|'. This makes each row in these columns seem to be one(1) long complicated word, which will make further analysis difficult. We will treat this problem in the `Data Cleaning` section.\n",
    "\n",
    "Also, data in the __budget__ column, which is meant to be a numerical column, are prepended with currency symbols and separated by commas(,). This is bad format and needs to be taken care of in the `Data Cleaning` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.666459Z",
     "iopub.status.busy": "2022-05-01T13:07:52.66593Z",
     "iopub.status.idle": "2022-05-01T13:07:52.674235Z",
     "shell.execute_reply": "2022-05-01T13:07:52.67332Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.666411Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the dimensions of the data\n",
    "imdb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB data is made up of __27,278__ rows and __6__ columns.\n",
    "\n",
    "How about some information about the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.677484Z",
     "iopub.status.busy": "2022-05-01T13:07:52.676182Z",
     "iopub.status.idle": "2022-05-01T13:07:52.716299Z",
     "shell.execute_reply": "2022-05-01T13:07:52.715375Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.677431Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB data has __1__ column of dtype `float64` - __runtime__, __1__ column of dtype `int64` - __movieId__ and __4__ columns of dtype `object` - __title_cast, director, budget & plot_keywords__. \n",
    "\n",
    "The __budget__ column is meant to be numerical to aid aggregation. This will be taken care of in the `Data Cleaning` section.\n",
    "\n",
    "We also have a case of missing data in all columns bar __movieId__. Let's see by how much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.718244Z",
     "iopub.status.busy": "2022-05-01T13:07:52.718006Z",
     "iopub.status.idle": "2022-05-01T13:07:52.758689Z",
     "shell.execute_reply": "2022-05-01T13:07:52.757689Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.718212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the number of missing data and the percentage\n",
    "# of missing data and concatenate into one dataframe\n",
    "imdb_missing_data = pd.concat([imdb.isnull().sum(), round(imdb.isnull().sum()/imdb.shape[0] * 100)], axis=1)\n",
    "imdb_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "imdb_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Budget` has the highest missing data with __19,372__! rows of missing data, making up __71%__! of the entire column - that is a huge amount!.\n",
    "\n",
    "`Runtime` has the second highest missing values at __44%__, followed closely by `plot_keywords` at __41%__. `title_cast` and `director` also record missing values at __37%__ and __36%__ respectively.\n",
    "\n",
    "These look like a lot of missing data and have to come up with creative ways to deal with this problem in `Feature Engineering` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Movies data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. MOVIES DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.76076Z",
     "iopub.status.busy": "2022-05-01T13:07:52.760411Z",
     "iopub.status.idle": "2022-05-01T13:07:52.773519Z",
     "shell.execute_reply": "2022-05-01T13:07:52.772409Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.760719Z"
    }
   },
   "outputs": [],
   "source": [
    "# movies data\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the Movies data is made up of __3__ columns - __movieId__, __title__, __genres__.\n",
    "\n",
    "Similar to IMDB's __title_cast__ and __plot_keywords__, data in the __genres__ column are separated distinctly by a '|' symbol. As stated earlier, this will need to be taken care of in the `Data Cleaning` section.\n",
    "\n",
    "The __title__ column holds both the _title_ of the movie and the _year of release_, like co-joined twins they need to be separated in the theatre of `Feature Engineering`.\n",
    "\n",
    "\n",
    "Next, we will look at the dimensions of the data using `.shape` attribute of a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.776471Z",
     "iopub.status.busy": "2022-05-01T13:07:52.775476Z",
     "iopub.status.idle": "2022-05-01T13:07:52.788114Z",
     "shell.execute_reply": "2022-05-01T13:07:52.787178Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.776422Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the dimensions of the data\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movies dataset has __62,423__ rows of data and __3__ columns of features.\n",
    "\n",
    "Fine, let's drill down a bit on the data by columns to gain a slightly better understanding using `.info` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.793003Z",
     "iopub.status.busy": "2022-05-01T13:07:52.792438Z",
     "iopub.status.idle": "2022-05-01T13:07:52.827615Z",
     "shell.execute_reply": "2022-05-01T13:07:52.826696Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.792959Z"
    }
   },
   "outputs": [],
   "source": [
    "# get more information about the data\n",
    "movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __2__ objects columns - __title__ and __genres__ and __1__ numerical(_int64_) column.\n",
    "\n",
    "We can safely say there are no missing values in any of the columns, judging from the shape of the dataset and the number of _non-null count_ for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.829443Z",
     "iopub.status.busy": "2022-05-01T13:07:52.828711Z",
     "iopub.status.idle": "2022-05-01T13:07:52.87505Z",
     "shell.execute_reply": "2022-05-01T13:07:52.874267Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.829404Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the number of missing data and the percentage\n",
    "# of missing data and concatenate into one dataframe\n",
    "movies_missing_data = pd.concat([movies.isnull().sum(), round(movies.isnull().sum()/movies.shape[0] * 100)], axis=1)\n",
    "movies_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "movies_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our intuition was right afterall...\n",
    "\n",
    "Next on the list, Meta_data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. META_DATA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.877107Z",
     "iopub.status.busy": "2022-05-01T13:07:52.876131Z",
     "iopub.status.idle": "2022-05-01T13:07:52.911826Z",
     "shell.execute_reply": "2022-05-01T13:07:52.910909Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.877058Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! the meta_data dataset seem to be a more robust, upgraded and an agglomerated version of the IMDB dataset and Movies dataset with a lot more information about a movie. This will be very useful for our recommendation systems.\n",
    "\n",
    "A drawback of note is that the meta_data does not have a \"movieId\", while we may use the \"id\" column instead, it doesn't map correctly with \"movieId\" of other datasets. \n",
    "\n",
    "What to do? let's keep that pending while we continue exploring the dataset.\n",
    "\n",
    "Let's look at the dimensions of the data next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.913571Z",
     "iopub.status.busy": "2022-05-01T13:07:52.913302Z",
     "iopub.status.idle": "2022-05-01T13:07:52.925401Z",
     "shell.execute_reply": "2022-05-01T13:07:52.924686Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.913536Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the dimension of the data\n",
    "meta_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has __45,466__ rows and __24__ columns.\n",
    "\n",
    "More information please! `.info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:52.926912Z",
     "iopub.status.busy": "2022-05-01T13:07:52.926622Z",
     "iopub.status.idle": "2022-05-01T13:07:53.047302Z",
     "shell.execute_reply": "2022-05-01T13:07:53.046233Z",
     "shell.execute_reply.started": "2022-05-01T13:07:52.926879Z"
    }
   },
   "outputs": [],
   "source": [
    "# get more information about the dataset\n",
    "meta_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot more columns than we do for both `movies` and `IMDB` datasets combined. \n",
    "\n",
    "Question is, are they all useful for what we are trying to achieve? \n",
    "\n",
    "There are columns with missing values. It is difficult to know by how much, so let's break it down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.049465Z",
     "iopub.status.busy": "2022-05-01T13:07:53.049108Z",
     "iopub.status.idle": "2022-05-01T13:07:53.265247Z",
     "shell.execute_reply": "2022-05-01T13:07:53.264518Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.049417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the number of missing data and the percentage\n",
    "# of missing data and concatenate into one dataframe\n",
    "meta_data_missing_data = pd.concat([meta_data.isnull().sum(), round(meta_data.isnull().sum()/meta_data.shape[0] * 100)], axis=1)\n",
    "meta_data_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "meta_data_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An astonishing __90%__! of data are missing in the __belongs_to_collection__ column, but that can be very misleading because not all the movies are part of a franchise or collection, meaning they don't have sequels. Also, __budget__ column appears to not have any missing value but from the initial sneak peek, we can see that there are movies with **Zero (0)** budget. This is practically not possible and need to be dealt with.\n",
    "\n",
    "__homepage__ on the other hand, which has __83%__ of its data missing is not useful to us in the particular context of a recommender system. Therefore, it will be removed during `Feature engineering`\n",
    "\n",
    "__tagline__, while have approximately half of its data missing may be of value us and cannot be discarded so easily.\n",
    "\n",
    "Honorable mentions in the missing data category include; __overview__ - __2%__, __poster_path__ - __1%__ and __runtime__ - __1%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.266843Z",
     "iopub.status.busy": "2022-05-01T13:07:53.266462Z",
     "iopub.status.idle": "2022-05-01T13:07:53.50268Z",
     "shell.execute_reply": "2022-05-01T13:07:53.501739Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.26681Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for duplicated values\n",
    "\n",
    "meta_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta data contains __13__ duplicated rows. We will handle this in `Data Cleaning`\n",
    "\n",
    "Up Next, Genome_scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. GENOME SCORES DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.504611Z",
     "iopub.status.busy": "2022-05-01T13:07:53.5043Z",
     "iopub.status.idle": "2022-05-01T13:07:53.517249Z",
     "shell.execute_reply": "2022-05-01T13:07:53.516312Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.504567Z"
    }
   },
   "outputs": [],
   "source": [
    "genome_scores.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is made up of __3__ columns, namely; __movieId__, __tagId__ and __relevance__.\n",
    "\n",
    "Right now, we can only assume that __relevance__ indicates by how much a tag is of importance to a movie.\n",
    "\n",
    "Let's look at the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.518898Z",
     "iopub.status.busy": "2022-05-01T13:07:53.518582Z",
     "iopub.status.idle": "2022-05-01T13:07:53.534207Z",
     "shell.execute_reply": "2022-05-01T13:07:53.533263Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.518863Z"
    }
   },
   "outputs": [],
   "source": [
    "genome_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over __15 Million__ rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.535891Z",
     "iopub.status.busy": "2022-05-01T13:07:53.535574Z",
     "iopub.status.idle": "2022-05-01T13:07:53.555912Z",
     "shell.execute_reply": "2022-05-01T13:07:53.555192Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.535856Z"
    }
   },
   "outputs": [],
   "source": [
    "genome_scores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no information on the number of non-null rows.\n",
    "\n",
    "Ther are __2__ *int32* columns and __1__ *float32* column, indicating it's an all-numeric dataset\n",
    "\n",
    "Let's see if there are any missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.558214Z",
     "iopub.status.busy": "2022-05-01T13:07:53.557336Z",
     "iopub.status.idle": "2022-05-01T13:07:53.718851Z",
     "shell.execute_reply": "2022-05-01T13:07:53.717746Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.558168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the number of missing data and the percentage\n",
    "# of missing data and concatenate into one dataframe\n",
    "gs_missing_data = pd.concat([genome_scores.isnull().sum(), round(genome_scores.isnull().sum()/genome_scores.shape[0] * 100)], axis=1)\n",
    "gs_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "gs_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing data.\n",
    "\n",
    "Next, we look at Genome Tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. GENOME TAGS DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.720799Z",
     "iopub.status.busy": "2022-05-01T13:07:53.720454Z",
     "iopub.status.idle": "2022-05-01T13:07:53.733979Z",
     "shell.execute_reply": "2022-05-01T13:07:53.73295Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.720751Z"
    }
   },
   "outputs": [],
   "source": [
    "genome_tags.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has __2__ columns; __tagId__ and __tag__. \n",
    "\n",
    "Movie tags are another way to relate movies to each other.\n",
    "\n",
    "Next, The dimensions of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.736163Z",
     "iopub.status.busy": "2022-05-01T13:07:53.735531Z",
     "iopub.status.idle": "2022-05-01T13:07:53.749159Z",
     "shell.execute_reply": "2022-05-01T13:07:53.74816Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.73611Z"
    }
   },
   "outputs": [],
   "source": [
    "genome_tags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __1,128__ rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.752138Z",
     "iopub.status.busy": "2022-05-01T13:07:53.750967Z",
     "iopub.status.idle": "2022-05-01T13:07:53.774408Z",
     "shell.execute_reply": "2022-05-01T13:07:53.773399Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.752076Z"
    }
   },
   "outputs": [],
   "source": [
    "genome_tags.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of __1__ categorical or text data column, much like 'object' and __1__ numerical column ('int32')\n",
    "\n",
    "And there are no missing values, but let's double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.775896Z",
     "iopub.status.busy": "2022-05-01T13:07:53.775633Z",
     "iopub.status.idle": "2022-05-01T13:07:53.792716Z",
     "shell.execute_reply": "2022-05-01T13:07:53.791652Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.775867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the number of missing data and the percentage\n",
    "# of missing data and concatenate into one dataframe\n",
    "gt_missing_data = pd.concat([genome_tags.isnull().sum(), round(genome_tags.isnull().sum()/genome_tags.shape[0] * 100)], axis=1)\n",
    "gt_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "gt_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our suspicion was correct afterall. Let's trust our gut feelings next time. :-)\n",
    "\n",
    "Up next, we will be sneak peaking into the train dataset. stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.794699Z",
     "iopub.status.busy": "2022-05-01T13:07:53.794252Z",
     "iopub.status.idle": "2022-05-01T13:07:53.815966Z",
     "shell.execute_reply": "2022-05-01T13:07:53.815265Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.794629Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, we see that the train dataset is made up of __4__ columns; __userId__, __movieId__, __rating__ and __timestamp__.\n",
    "\n",
    "Here we have the rating each user gives a movie and also a timestamp of when such rating occured.\n",
    "\n",
    "We will look at the shape of the data next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.817563Z",
     "iopub.status.busy": "2022-05-01T13:07:53.817205Z",
     "iopub.status.idle": "2022-05-01T13:07:53.829555Z",
     "shell.execute_reply": "2022-05-01T13:07:53.828377Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.817533Z"
    }
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is made up of about __10 Million__ rows of data. Pretty large.\n",
    "\n",
    "Let's extract more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.831389Z",
     "iopub.status.busy": "2022-05-01T13:07:53.831147Z",
     "iopub.status.idle": "2022-05-01T13:07:53.850454Z",
     "shell.execute_reply": "2022-05-01T13:07:53.849437Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.831357Z"
    }
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __4__ columns, all of which are numerical, consisting of __3__ columns of dtype *int32* and __1__ column of dtype *float32*. I am tempted to say there are no missing values and trust my gut feelings, but just to double check again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.857139Z",
     "iopub.status.busy": "2022-05-01T13:07:53.856551Z",
     "iopub.status.idle": "2022-05-01T13:07:53.997025Z",
     "shell.execute_reply": "2022-05-01T13:07:53.996065Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.857101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the number of missing data and the percentage\n",
    "# of missing data and concatenate into one dataframe\n",
    "train_missing_data = pd.concat([train.isnull().sum(), round(train.isnull().sum()/train.shape[0] * 100)], axis=1)\n",
    "train_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "train_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorry gut feelings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. LINKS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:53.998547Z",
     "iopub.status.busy": "2022-05-01T13:07:53.998308Z",
     "iopub.status.idle": "2022-05-01T13:07:54.008946Z",
     "shell.execute_reply": "2022-05-01T13:07:54.008298Z",
     "shell.execute_reply.started": "2022-05-01T13:07:53.998517Z"
    }
   },
   "outputs": [],
   "source": [
    "links.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links, as the name suggests, contains only primary keys (columns with unique identity for each data point)  to other datasets. \n",
    "\n",
    "It is made up of **3** columns; __movieId__, __imdbId__ & __tmdbId__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.010828Z",
     "iopub.status.busy": "2022-05-01T13:07:54.010133Z",
     "iopub.status.idle": "2022-05-01T13:07:54.01962Z",
     "shell.execute_reply": "2022-05-01T13:07:54.018765Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.01079Z"
    }
   },
   "outputs": [],
   "source": [
    "links.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __62,423__ rows in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.021326Z",
     "iopub.status.busy": "2022-05-01T13:07:54.021084Z",
     "iopub.status.idle": "2022-05-01T13:07:54.042538Z",
     "shell.execute_reply": "2022-05-01T13:07:54.041476Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.021295Z"
    }
   },
   "outputs": [],
   "source": [
    "links.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns are numeric columns; __2__ _int32_ and __1__ _float32_ column(s) respectively. \n",
    "\n",
    "__tmdbId__ seems to have missing values, let's check by how much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.044696Z",
     "iopub.status.busy": "2022-05-01T13:07:54.043868Z",
     "iopub.status.idle": "2022-05-01T13:07:54.061857Z",
     "shell.execute_reply": "2022-05-01T13:07:54.061049Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.044634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the number of missing data and the percentage\n",
    "# of missing data and concatenate into one dataframe\n",
    "links_missing_data = pd.concat([links.isnull().sum(), round(links.isnull().sum()/links.shape[0] * 100, 3)], axis=1)\n",
    "links_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "links_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of missing data here is very negligible at __0.171%__, which I believe won't create significant problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final show in this section, we will take a sneak peek into the tags dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. TAGS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.06385Z",
     "iopub.status.busy": "2022-05-01T13:07:54.063237Z",
     "iopub.status.idle": "2022-05-01T13:07:54.075391Z",
     "shell.execute_reply": "2022-05-01T13:07:54.074441Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.063809Z"
    }
   },
   "outputs": [],
   "source": [
    "tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset also has __4__ columns; __userId__, __movieId__, __tag__ and __timestamp__.\n",
    "\n",
    "__tag__ also features here as it did in `genome_tags` dataset. Is there a difference? or are they the same? This we will explore in the `Exploratory Data Analysis` section.\n",
    "\n",
    "Next, we will take a look at the dimensions of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.077721Z",
     "iopub.status.busy": "2022-05-01T13:07:54.076862Z",
     "iopub.status.idle": "2022-05-01T13:07:54.09054Z",
     "shell.execute_reply": "2022-05-01T13:07:54.089549Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.07767Z"
    }
   },
   "outputs": [],
   "source": [
    "tags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over __1 Million__ rows of data in this dataset.\n",
    "\n",
    "Let's get some more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.093001Z",
     "iopub.status.busy": "2022-05-01T13:07:54.092096Z",
     "iopub.status.idle": "2022-05-01T13:07:54.186502Z",
     "shell.execute_reply": "2022-05-01T13:07:54.185781Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.092933Z"
    }
   },
   "outputs": [],
   "source": [
    "tags.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __3__ numerical columns and __1__ categorical column which is the __tag__ column. \n",
    "\n",
    "The tag column also seem to be missing some values. Let's confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.188246Z",
     "iopub.status.busy": "2022-05-01T13:07:54.18787Z",
     "iopub.status.idle": "2022-05-01T13:07:54.217252Z",
     "shell.execute_reply": "2022-05-01T13:07:54.216554Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.188198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the number of missing data and the percentage\n",
    "# of missing data and concatenate into one dataframe\n",
    "tags_missing_data = pd.concat([tags.isnull().sum(), round(tags.isnull().sum()/tags.shape[0] * 100, 3)], axis=1)\n",
    "tags_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "tags_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a very very negligible number of missing data at __0.001%__. That won't do us much harm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, we've come to the end of the `Sneak Peek into Loaded Data Section`. \n",
    "\n",
    "Here, we had a brief overview of the datasets we intend to work with and what needs to be done to get our data ready for further analysis and modeling.\n",
    "\n",
    "\n",
    "During the sneak peeking, we noticed that there are some columns that aren't just right. In the next section, we will be making use of a bunch of techniques to prepare the data into the right and useable formats in a process known as `Data Cleaning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n",
    "</div>\n",
    "<div id='data-cleaning'>\n",
    "    <h2 style='text-transform: uppercase;'>data cleaning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WHAT IS DATA CLEANING?\n",
    "\n",
    "The process of repairing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data from a dataset is known as `Data Cleaning`. There are numerous opportunities for data to be duplicated or mislabeled when combining multiple data sources. If the data is incorrect, the results and algorithms are untrustworthy, even if they appear to be correct.\n",
    "\n",
    "This will be done for every datasets we intend to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. IMDB DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make a copy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.218895Z",
     "iopub.status.busy": "2022-05-01T13:07:54.218503Z",
     "iopub.status.idle": "2022-05-01T13:07:54.224365Z",
     "shell.execute_reply": "2022-05-01T13:07:54.22345Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.218852Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a copy of the dataset\n",
    "imdb_copy = imdb.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...then let's remind ourselves what messy data we have on our hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.226011Z",
     "iopub.status.busy": "2022-05-01T13:07:54.225596Z",
     "iopub.status.idle": "2022-05-01T13:07:54.247002Z",
     "shell.execute_reply": "2022-05-01T13:07:54.24624Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.225979Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_copy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the *title_cast* column, we will split each row on '|' and we want to keep the firstname and lastname of the actors together so we join the first names and last names with an underscore('_'), same for the directors and make them lowercase\n",
    "\n",
    "For *plot_keyword*, we will replace '|' with a space ' ', remove stopwords and we lemmatize or stem the word or both.\n",
    "\n",
    "For the *budget* column, we will remove the commas(',') and extract the digits into a separate column and the currency symbols into another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.249423Z",
     "iopub.status.busy": "2022-05-01T13:07:54.24849Z",
     "iopub.status.idle": "2022-05-01T13:07:54.290503Z",
     "shell.execute_reply": "2022-05-01T13:07:54.289741Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.249383Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for duplicated data\n",
    "\n",
    "imdb_copy.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicated data in the IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenda\n",
    "1. Fill null values with ''(blank) for both *title_cast*, *director* and *plot_keywords*\n",
    "2. Split *title_cast* on '|', join the first and last names of each actor with '_' and convert to lowercase letters\n",
    "3. Replace '|' with ' '(a space) in *plot_keywords*\n",
    "4. Remove stopwords in plot_keywords\n",
    "5. Replace ',' in _budget_ column with ''(nothing)\n",
    "6. Extract currency symbol into another column called 'symbol' and amount into 'budget_amount'\n",
    "7. Convert the dtype of budget to 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.292329Z",
     "iopub.status.busy": "2022-05-01T13:07:54.291969Z",
     "iopub.status.idle": "2022-05-01T13:07:54.312136Z",
     "shell.execute_reply": "2022-05-01T13:07:54.311352Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.292297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 1: Fill null values with ''(blank)\n",
    "\n",
    "imdb_copy['title_cast'] = imdb_copy['title_cast'].fillna('')\n",
    "imdb_copy['director'] = imdb_copy['director'].fillna('')\n",
    "imdb_copy['plot_keywords'] = imdb_copy['plot_keywords'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.313834Z",
     "iopub.status.busy": "2022-05-01T13:07:54.313455Z",
     "iopub.status.idle": "2022-05-01T13:07:54.491692Z",
     "shell.execute_reply": "2022-05-01T13:07:54.490917Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.313796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 2: Split title_cast on '|', join the first and last names of each actor with '_'\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # split text on '|'\n",
    "    text_split = text.split('|')\n",
    "    \n",
    "    # replace the space between the actors first name and\n",
    "    # lastname with an underscore, convert to lowercase\n",
    "    # and then join into a string.\n",
    "    text_replace = ' '.join([x.replace(' ', '_') if len(x) > 0 else '' for x in text_split]).lower()\n",
    "    \n",
    "    # return transformed text\n",
    "    return text_replace\n",
    "\n",
    "# apply clean_text function to each row\n",
    "imdb_copy['title_cast'] = imdb_copy['title_cast'].apply(clean_text) \n",
    "\n",
    "# replace the space between the directors' first name and \n",
    "# last names with an underscore, and convert to lowercase\n",
    "imdb_copy['director'] = imdb_copy['director'].apply(lambda row: row.replace(' ', '_').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.493396Z",
     "iopub.status.busy": "2022-05-01T13:07:54.493014Z",
     "iopub.status.idle": "2022-05-01T13:07:54.512181Z",
     "shell.execute_reply": "2022-05-01T13:07:54.510878Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.493364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 3: Replace '|' with ' '(a space) in plot_keywords\n",
    "\n",
    "# pick column and use .apply() with the lambda function to replace \"|\" character\n",
    "# with a space.\n",
    "imdb_copy['plot_keywords'] = imdb_copy['plot_keywords']\\\n",
    "                            .apply(lambda row: row.replace('|', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.51356Z",
     "iopub.status.busy": "2022-05-01T13:07:54.513327Z",
     "iopub.status.idle": "2022-05-01T13:07:54.577535Z",
     "shell.execute_reply": "2022-05-01T13:07:54.576722Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.513531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 4: Remove stopwords from *plot_keywords*\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    split_text = text.split()\n",
    "    text = ' '.join([x for x in split_text if x not in stopwords])  \n",
    "    return text    \n",
    "\n",
    "imdb_copy['plot_keywords'] = imdb_copy['plot_keywords'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.580439Z",
     "iopub.status.busy": "2022-05-01T13:07:54.580084Z",
     "iopub.status.idle": "2022-05-01T13:07:54.60291Z",
     "shell.execute_reply": "2022-05-01T13:07:54.601798Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.580393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 5: Replace ',' in budget column with ''(nothing)\n",
    "\n",
    "# replace commas in budget amount with blanks\n",
    "# excluding rows with values np.Nan\n",
    "imdb_copy['budget'] = imdb_copy['budget']\\\n",
    "                                .apply(lambda row: row.replace(',', '')\\\n",
    "                                      if type(row) == str else row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the commas before using regex to extract amounts and currency symbols is important because trying to extract the digits first before removing commas will result in extracting only the first few digits found before a comma, leaving us with only the million figure without the zeros (i.e 35 instead of 35000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.604794Z",
     "iopub.status.busy": "2022-05-01T13:07:54.604456Z",
     "iopub.status.idle": "2022-05-01T13:07:54.668601Z",
     "shell.execute_reply": "2022-05-01T13:07:54.667752Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.604749Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 6: Extract currency symbol into another column called 'symbol' and amount into 'budget_amount'\n",
    "\n",
    "pattern = '[0-9]+' # pattern to search for digits\n",
    "symbol = '[$A-Za-z]+' # patter to search for alpha characters\n",
    "\n",
    "# extract the budget amount from the budget column \n",
    "# and put it in `budget_amount column \n",
    "# excluding rows with values np.Nan\n",
    "imdb_copy['budget_amount'] = imdb_copy['budget']\\\n",
    "                            .apply(lambda row: re.search(pattern, row)\\\n",
    "                                   .group() if type(row) == str else row)\n",
    "\n",
    "# extract the currency symbol from the budget column\n",
    "# and put it in `symbol` column\n",
    "# excluding rows with values np.Nan\n",
    "imdb_copy['symbol'] = imdb_copy['budget']\\\n",
    "                            .apply(lambda row: re.search(symbol, row)\\\n",
    "                                   .group() if type(row) == str else row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.670376Z",
     "iopub.status.busy": "2022-05-01T13:07:54.669964Z",
     "iopub.status.idle": "2022-05-01T13:07:54.679177Z",
     "shell.execute_reply": "2022-05-01T13:07:54.678173Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.67034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 7: Convert the dtype of budget to 'float32'\n",
    "\n",
    "imdb_copy['budget_amount'] = imdb_copy['budget_amount'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.681742Z",
     "iopub.status.busy": "2022-05-01T13:07:54.681117Z",
     "iopub.status.idle": "2022-05-01T13:07:54.703878Z",
     "shell.execute_reply": "2022-05-01T13:07:54.7032Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.681688Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_copy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been able to clean the imdb data to a certain degree and I am currently happy with the result. \n",
    "\n",
    "The redundant columns will be handled during `Feature Engineering`\n",
    "\n",
    "Next we will be cleaning the Movies Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. MOVIES DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.705614Z",
     "iopub.status.busy": "2022-05-01T13:07:54.705161Z",
     "iopub.status.idle": "2022-05-01T13:07:54.710986Z",
     "shell.execute_reply": "2022-05-01T13:07:54.710366Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.705579Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a copy\n",
    "\n",
    "movies_copy = movies.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.712573Z",
     "iopub.status.busy": "2022-05-01T13:07:54.711978Z",
     "iopub.status.idle": "2022-05-01T13:07:54.729359Z",
     "shell.execute_reply": "2022-05-01T13:07:54.72861Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.712538Z"
    }
   },
   "outputs": [],
   "source": [
    "movies_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.731488Z",
     "iopub.status.busy": "2022-05-01T13:07:54.730763Z",
     "iopub.status.idle": "2022-05-01T13:07:54.769443Z",
     "shell.execute_reply": "2022-05-01T13:07:54.768585Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.731446Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for duplicated data\n",
    "\n",
    "movies_copy.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agenda for this dataset will be;\n",
    "\n",
    "1. Split the title into 2 columns, title and year_released and convert the title column to lowercase\n",
    "2. Replace '|' with ' '(space) in the genres column and convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.771157Z",
     "iopub.status.busy": "2022-05-01T13:07:54.770837Z",
     "iopub.status.idle": "2022-05-01T13:07:54.9287Z",
     "shell.execute_reply": "2022-05-01T13:07:54.927843Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.771121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 1: Split the title into 2 columns, title and year_released\n",
    "\n",
    "movies_copy['year'] = movies_copy['title'].apply(lambda x: x[-7:].replace('(', '').replace(')', ''))\n",
    "\n",
    "#convert year to a numeric column\n",
    "movies_copy['year'] = pd.to_numeric(movies_copy['year'], errors='coerce', downcast='float')\n",
    "\n",
    "movies_copy['title'] = movies_copy['title'].apply(lambda x: x[:-7].strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.930819Z",
     "iopub.status.busy": "2022-05-01T13:07:54.930518Z",
     "iopub.status.idle": "2022-05-01T13:07:54.972202Z",
     "shell.execute_reply": "2022-05-01T13:07:54.971098Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.930783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 2: Replace '|' with ' '(space) in the genres column\n",
    "\n",
    "movies_copy['genres'] = movies_copy['genres'].apply(lambda row: row.replace('|', ' ').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.974066Z",
     "iopub.status.busy": "2022-05-01T13:07:54.97377Z",
     "iopub.status.idle": "2022-05-01T13:07:54.985946Z",
     "shell.execute_reply": "2022-05-01T13:07:54.985065Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.974031Z"
    }
   },
   "outputs": [],
   "source": [
    "movies_copy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to have worked out fine, and I am happy with the results\n",
    "\n",
    "\n",
    "Next, Meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. META DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that meta_data is a robust agglomerated version of the IMDB and Movies Datasets. The meta_data meanwhile, does not contain values for some of the data points that are available in Movies and IMDB datasets, for instance, some movie budgets data available in the IMDB dataset are not available in the Meta dataset.\n",
    "\n",
    "After cleaning, we will take a look at how to maximally utilise all datasets involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:54.9877Z",
     "iopub.status.busy": "2022-05-01T13:07:54.987271Z",
     "iopub.status.idle": "2022-05-01T13:07:55.228088Z",
     "shell.execute_reply": "2022-05-01T13:07:55.227268Z",
     "shell.execute_reply.started": "2022-05-01T13:07:54.98763Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a copy\n",
    "meta_copy = meta_data.copy(deep=True)\n",
    "\n",
    "# drop possible duplicates\n",
    "meta_copy.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:55.229959Z",
     "iopub.status.busy": "2022-05-01T13:07:55.229671Z",
     "iopub.status.idle": "2022-05-01T13:07:55.259692Z",
     "shell.execute_reply": "2022-05-01T13:07:55.258809Z",
     "shell.execute_reply.started": "2022-05-01T13:07:55.229923Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:55.261198Z",
     "iopub.status.busy": "2022-05-01T13:07:55.260966Z",
     "iopub.status.idle": "2022-05-01T13:07:55.291012Z",
     "shell.execute_reply": "2022-05-01T13:07:55.290034Z",
     "shell.execute_reply.started": "2022-05-01T13:07:55.261169Z"
    }
   },
   "outputs": [],
   "source": [
    "# displaying the truncated columns\n",
    "meta_copy.loc[:, 'overview': 'release_date'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very messy data. Let's get to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our cleaning agenda for this dataset;\n",
    "\n",
    "1. Column *Belongs_to_collection* should be converted to a boolean field (True or False)\n",
    "2. Extract the genres from column _genres_ as they are store in dictionaries in a list\n",
    "3. Extract the digits from the *imdb_id* column and rename the column to _imdbId_\n",
    "4. Convert *popularity* to 'float32'\n",
    "5. Extract *production_companies* and *production_countries*\n",
    "6. Extract _year_ from *release_date*\n",
    "7. Extract _language_ from *spoken_languages*\n",
    "8. Replace Zeros in budget with np.nan and make column a numerical column\n",
    "9. convert _title_ column to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:55.292372Z",
     "iopub.status.busy": "2022-05-01T13:07:55.292134Z",
     "iopub.status.idle": "2022-05-01T13:07:55.327917Z",
     "shell.execute_reply": "2022-05-01T13:07:55.326988Z",
     "shell.execute_reply.started": "2022-05-01T13:07:55.292333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 1: Column Belongs_to_collection should be converted to a boolean field (True or False)\n",
    "\n",
    "meta_copy['belongs_to_collection'] = meta_copy['belongs_to_collection']\\\n",
    "                                        .apply(lambda row: True if type(row) != float else False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the second agenda. The __Genres__ column is a list of objects or dictionaries made of 'id' - referring to the genre's id and the 'name' - referring to the genre itself. This is really poor formatting if we are going to make use of this dataset. **Production_companies**, **production_countries** and **spoken_languages** are formatted In a similar fashion.\n",
    "\n",
    "To deal with this problem, we will be applying a custom function we will call `decompose` to help us extract what we need from these columns. But first, since each row in these columns is stored as a 'string', we will apply 'literal_eval' which evaluates a string containing a python literal, such as a list, dictionary etc...\n",
    "\n",
    "Let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:55.330456Z",
     "iopub.status.busy": "2022-05-01T13:07:55.329868Z",
     "iopub.status.idle": "2022-05-01T13:07:55.336788Z",
     "shell.execute_reply": "2022-05-01T13:07:55.33586Z",
     "shell.execute_reply.started": "2022-05-01T13:07:55.330409Z"
    }
   },
   "outputs": [],
   "source": [
    "x = \"[{'id': 12, 'name': 'Adventure'},{'id': 14, 'name': 'Fantasy'},{'id': 10751, 'name': 'Family'}]\"\n",
    "\n",
    "print(f\"Data type of x before literal_eval: {type(x)}\")\n",
    "\n",
    "x = literal_eval(x)\n",
    "\n",
    "print(f\"Data type of x after literal_eval: {type(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literal_eval takes the text and returns its true form. Let's apply this in cleaning our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:55.338609Z",
     "iopub.status.busy": "2022-05-01T13:07:55.338305Z",
     "iopub.status.idle": "2022-05-01T13:07:55.349168Z",
     "shell.execute_reply": "2022-05-01T13:07:55.348196Z",
     "shell.execute_reply.started": "2022-05-01T13:07:55.338578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 2: Extract the genres from column genres as they are store in dictionaries in a list\n",
    "\n",
    "def decompose(text, key='name'):\n",
    "    \n",
    "    try:\n",
    "        # check if text is np.nan \n",
    "        if type(text) == float:\n",
    "            decomposed_text = text\n",
    "            \n",
    "        #apply literal_exal to each row of data\n",
    "        eval_text = literal_eval(text)\n",
    "\n",
    "        # get the name key of each dictionary in the list\n",
    "        # store extracted name in a list\n",
    "        # join each item in the list into a string\n",
    "        decomposed_text = ' '.join([dictionary[key].replace(' ', '_') for dictionary in eval_text]).lower()\n",
    "    \n",
    "    except (ValueError, TypeError):\n",
    "        decomposed_text = np.nan\n",
    "        \n",
    "    return decomposed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:55.351748Z",
     "iopub.status.busy": "2022-05-01T13:07:55.350612Z",
     "iopub.status.idle": "2022-05-01T13:07:56.71857Z",
     "shell.execute_reply": "2022-05-01T13:07:56.717617Z",
     "shell.execute_reply.started": "2022-05-01T13:07:55.351699Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply the decompose function on the genres column.\n",
    "meta_copy['genres'] = meta_copy['genres'].apply(decompose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:56.720613Z",
     "iopub.status.busy": "2022-05-01T13:07:56.720284Z",
     "iopub.status.idle": "2022-05-01T13:07:56.751922Z",
     "shell.execute_reply": "2022-05-01T13:07:56.750937Z",
     "shell.execute_reply.started": "2022-05-01T13:07:56.720569Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_copy.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function does its job well and I'm very happy with the results.\n",
    "\n",
    "Next, we will extract digits from the **imdb_id** column and rename it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:56.753797Z",
     "iopub.status.busy": "2022-05-01T13:07:56.753459Z",
     "iopub.status.idle": "2022-05-01T13:07:56.891901Z",
     "shell.execute_reply": "2022-05-01T13:07:56.890779Z",
     "shell.execute_reply.started": "2022-05-01T13:07:56.753753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 3: Extract the digits from the imdb_id column and rename the column to imdbId\n",
    "\n",
    "re_pattern = '\\d+' # regex to extract 1 or more digits\n",
    "\n",
    "# applying regex, search through every row text, find every digits\n",
    "# in the text then return the group of texts if the row is not 'null'\n",
    "meta_copy['imdb_id'] = meta_copy['imdb_id']\\\n",
    "                        .apply(lambda row: re.search(re_pattern, row)\\\n",
    "                              .group() if type(row) == str else np.nan)\\\n",
    "                                .astype('float32') # convert the column to dtype 'int32'\n",
    "\n",
    "# rename the 'imdb_id' to 'imdbId' \n",
    "meta_copy = meta_copy.rename(columns={'imdb_id': 'imdbId'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:56.89357Z",
     "iopub.status.busy": "2022-05-01T13:07:56.8933Z",
     "iopub.status.idle": "2022-05-01T13:07:56.925801Z",
     "shell.execute_reply": "2022-05-01T13:07:56.924999Z",
     "shell.execute_reply.started": "2022-05-01T13:07:56.893535Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 4: Convert popularity to float32\n",
    "\n",
    "meta_copy['popularity'] = pd.to_numeric(meta_copy['popularity'], downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:56.928076Z",
     "iopub.status.busy": "2022-05-01T13:07:56.927151Z",
     "iopub.status.idle": "2022-05-01T13:07:56.989819Z",
     "shell.execute_reply": "2022-05-01T13:07:56.988906Z",
     "shell.execute_reply.started": "2022-05-01T13:07:56.928037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 5: Extract production_companies and production_countries\n",
    "\n",
    "boolean = np.where((meta_copy['production_companies'] == True) | \\\n",
    "                   (meta_copy['production_companies'] == 'True') | \\\n",
    "                  (meta_copy['production_companies'] == False) | \\\n",
    "                  (meta_copy['production_companies'] == 'False') | \\\n",
    "                  (meta_copy['production_companies'] == 'nan') | \\\n",
    "                  (meta_copy['production_companies'] == np.nan)) \n",
    "\n",
    "data = meta_copy.iloc[boolean]\n",
    "data.loc[:,'overview':'release_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are boolean values in the **production_companies** column which can cause problems, so we will replace these values with 'np.NaN'. Also **production_countries** contains some floating point values which are not supposed to be there. we will replace these with np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:56.991858Z",
     "iopub.status.busy": "2022-05-01T13:07:56.991582Z",
     "iopub.status.idle": "2022-05-01T13:07:59.126873Z",
     "shell.execute_reply": "2022-05-01T13:07:59.126066Z",
     "shell.execute_reply.started": "2022-05-01T13:07:56.991825Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_copy['production_companies'] = meta_copy['production_companies']\\\n",
    "                                    .apply(lambda x: np.nan if x in ('', ' ', 'True', True, 'False', False) else x)\\\n",
    "                                    .apply(decompose)\n",
    "\n",
    "meta_copy['production_countries'] = meta_copy['production_countries']\\\n",
    "                                    .apply(lambda x: np.nan if x in ('', ' ', 'True', True, 'False', False) else x)\\\n",
    "                                    .apply(decompose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:59.129106Z",
     "iopub.status.busy": "2022-05-01T13:07:59.128359Z",
     "iopub.status.idle": "2022-05-01T13:07:59.154947Z",
     "shell.execute_reply": "2022-05-01T13:07:59.153944Z",
     "shell.execute_reply.started": "2022-05-01T13:07:59.129063Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_copy.loc[:, 'overview':'release_data'].iloc[boolean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:59.156843Z",
     "iopub.status.busy": "2022-05-01T13:07:59.156506Z",
     "iopub.status.idle": "2022-05-01T13:07:59.22719Z",
     "shell.execute_reply": "2022-05-01T13:07:59.226139Z",
     "shell.execute_reply.started": "2022-05-01T13:07:59.156798Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 6: Extract year from release_date\n",
    "\n",
    "meta_copy['year'] = meta_copy['release_date']\\\n",
    "                        .apply(lambda row: row[0:4] \\\n",
    "                              if type(row) == str else np.nan).astype('float32')\n",
    "\n",
    "meta_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:07:59.229267Z",
     "iopub.status.busy": "2022-05-01T13:07:59.22894Z",
     "iopub.status.idle": "2022-05-01T13:08:00.219194Z",
     "shell.execute_reply": "2022-05-01T13:08:00.21674Z",
     "shell.execute_reply.started": "2022-05-01T13:07:59.229223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 7: Extract language from spoken_languages\n",
    "\n",
    "# here, we change the key of the decompose function\n",
    "# because we want the encoding of the language\n",
    "# not the language name itself\n",
    "meta_copy['spoken_languages'] = meta_copy['spoken_languages']\\\n",
    "                                    .apply(decompose, args=('iso_639_1',))\n",
    "\n",
    "meta_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.222417Z",
     "iopub.status.busy": "2022-05-01T13:08:00.222045Z",
     "iopub.status.idle": "2022-05-01T13:08:00.276589Z",
     "shell.execute_reply": "2022-05-01T13:08:00.27557Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.222367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 8: Replace Zeros in budget with np.nan and make column a numerical column\n",
    "\n",
    "meta_copy['budget'] = meta_copy['budget'].apply(lambda row: np.nan if row == '0' else row)\n",
    "meta_copy['budget'] = pd.to_numeric(meta_copy['budget'], downcast='float', errors='coerce')\n",
    "\n",
    "meta_copy['budget'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.278837Z",
     "iopub.status.busy": "2022-05-01T13:08:00.278477Z",
     "iopub.status.idle": "2022-05-01T13:08:00.336541Z",
     "shell.execute_reply": "2022-05-01T13:08:00.335585Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.278789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agenda 9: Convert title to lowercase\n",
    "\n",
    "# Convert title column to lowercase\n",
    "meta_copy['title'] = meta_copy['title'].str.lower()\n",
    "\n",
    "meta_copy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully cleaned up some of our datasets a bit which may aid our process of `Exploratory Data Analsys` or **EDA** for short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we do have a decision to make concerning which of **Movies and IMDB** datasets and **Meta Data** dataset.\n",
    "\n",
    "Let's carry out a preliminary exploratory data analysis on these datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PRELIM EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.338916Z",
     "iopub.status.busy": "2022-05-01T13:08:00.338099Z",
     "iopub.status.idle": "2022-05-01T13:08:00.345362Z",
     "shell.execute_reply": "2022-05-01T13:08:00.344442Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.338868Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Movies has: {movies_copy.shape[0]} rows and {movies_copy.shape[1]} columns\\n\\\n",
    "Imdb data has: {imdb_copy.shape[0]} rows and {imdb_copy.shape[1]} columns\\n\\\n",
    "Meta_data has: {meta_copy.shape[0]} rows and {meta_copy.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movies dataset has more rows than imdb and meta_data, meaning there are more observations in the Movies dataset. Meanwhile, Meta_data has more columns than movies and imdb datasets combined, meaning the meta_data dataset holds more information that may useful to our course than the other two.\n",
    "\n",
    "\n",
    "Next, lets see the features(columns) they all have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.347349Z",
     "iopub.status.busy": "2022-05-01T13:08:00.346947Z",
     "iopub.status.idle": "2022-05-01T13:08:00.359206Z",
     "shell.execute_reply": "2022-05-01T13:08:00.358296Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.347305Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Movies_features \\t>>>> \\t{' | '.join(movies.columns)}\\n\\\n",
    "Imdb_features \\t>>>> \\t{' | '.join(imdb.columns)}\\n\\\n",
    "Meta_features \\t>>>> \\t{' | '.join(meta_copy.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some intesting features in all the datasets that immense use to our algorithms. Meanwhile, it is worthy to note that all of the features in the movies dataset appear in the meta_data dataset, bar 'movieId'. Not forgetting that the movies dataset has more observations than the meta_data, we have to look for a way to make the best use of both datasets.\n",
    "\n",
    "The IMDB dataset more data about the people in the movie - the casts and the directors of each movies. These are particularly important features when we try to make movie recommendations to users based on the actors in the movies or the director of the movie as many users have biases/favorites in these categories.\n",
    "\n",
    "We have to think of ways to incorporate these datasets and their unique attribute into the algorithms we will build later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the meta_data dataset, there are come movies without budget amounts while in the Imdb dataset, these movies seem to have budgets. Let's do a bit of Sherlock Holmes here, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.360859Z",
     "iopub.status.busy": "2022-05-01T13:08:00.36057Z",
     "iopub.status.idle": "2022-05-01T13:08:00.412548Z",
     "shell.execute_reply": "2022-05-01T13:08:00.411887Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.360828Z"
    }
   },
   "outputs": [],
   "source": [
    "# first, lets merge the movies and Imdb data on movieId\n",
    "# to get the title column\n",
    "\n",
    "temp_df = imdb_copy.merge(movies_copy[['title', 'movieId']], on='movieId')\n",
    "temp_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.414521Z",
     "iopub.status.busy": "2022-05-01T13:08:00.41404Z",
     "iopub.status.idle": "2022-05-01T13:08:00.450426Z",
     "shell.execute_reply": "2022-05-01T13:08:00.449723Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.414484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get movies without budget from meta_data\n",
    "meta_without_budget = meta_copy['title'].str.lower()[meta_copy['budget'].isnull()]\n",
    "\n",
    "# Get movies without budget from temp_df\n",
    "imdb_without_budget = temp_df['title'][temp_df['budget_amount'].isnull()]\n",
    "\n",
    "print(f'meta: {meta_without_budget.shape} making up \\\n",
    "{round(meta_without_budget.shape[0]/meta_copy.shape[0]*100)}% missing\\n\\\n",
    "Imdb: {imdb_without_budget.shape} making up \\\n",
    "{round(imdb_without_budget.shape[0]/imdb_copy.shape[0]*100)}% missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are significant amounts of missing data.\n",
    "\n",
    "Lets see the similar movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.451903Z",
     "iopub.status.busy": "2022-05-01T13:08:00.451637Z",
     "iopub.status.idle": "2022-05-01T13:08:00.471258Z",
     "shell.execute_reply": "2022-05-01T13:08:00.470283Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.451871Z"
    }
   },
   "outputs": [],
   "source": [
    "sim_movies = meta_without_budget[meta_without_budget.isin(imdb_without_budget)]\n",
    "\n",
    "print(sim_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output are movies that do not have a budget amount in both Imdb dataset and meta_data dataset. There __10,122__ movies without a budget in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.473235Z",
     "iopub.status.busy": "2022-05-01T13:08:00.47275Z",
     "iopub.status.idle": "2022-05-01T13:08:00.498101Z",
     "shell.execute_reply": "2022-05-01T13:08:00.497209Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.4732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting movies that are in the Imdb dataset \n",
    "# from the ones without a budget amount in meta_data\n",
    "sim = np.where(temp_df['title'].isin(sim_movies))\n",
    "\n",
    "new_df = temp_df.iloc[sim]\n",
    "\n",
    "new_df = new_df[new_df['budget_amount'].notnull()]\n",
    "\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __289__ movies in the Imdb data that have budget amounts which are missing in the meta_data dataset. This doesn't solve a significant amount of our problem, we can move on without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.500305Z",
     "iopub.status.busy": "2022-05-01T13:08:00.499948Z",
     "iopub.status.idle": "2022-05-01T13:08:00.509476Z",
     "shell.execute_reply": "2022-05-01T13:08:00.508819Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.500259Z"
    }
   },
   "outputs": [],
   "source": [
    "# free up some space\n",
    "del temp_df\n",
    "del sim_movies\n",
    "del sim\n",
    "del new_df\n",
    "del meta_without_budget\n",
    "del imdb_without_budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other datasets seem to be prim and proper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n",
    "</div>\n",
    "<div id='eda'>\n",
    "    <h2 style='text-transform: uppercase;'>Exploratory Data Analysis</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis,also known as EDA, refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will be performing the EDA process dataset by dataset, starting with the IMDB Data. Let's see what we have to work with first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. IMDB DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.510972Z",
     "iopub.status.busy": "2022-05-01T13:08:00.510605Z",
     "iopub.status.idle": "2022-05-01T13:08:00.534425Z",
     "shell.execute_reply": "2022-05-01T13:08:00.533472Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.510942Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_copy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, there are a few questions we can find answers to;\n",
    "\n",
    "1. What are the minimum, average and maximum runtimes?\n",
    "2. What are the minimum, average and maximim budgets for a movie?\n",
    "3. The currency symbols denote countries, which countries are movies produced in?\n",
    "4. Who is/are the most featured casts?\n",
    "5. Director with the most movies?\n",
    "6. What plot keywords are most frequent?\n",
    "7. Does runtime influence the budget?\n",
    "8. How many movies are represented by this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer Questions 1 through 3, Let us get some descriptive statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.536361Z",
     "iopub.status.busy": "2022-05-01T13:08:00.536003Z",
     "iopub.status.idle": "2022-05-01T13:08:00.582511Z",
     "shell.execute_reply": "2022-05-01T13:08:00.581739Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.536328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 1: What are the minimum, average and maximum runtimes?\n",
    "# Question 2: What are the minimum, average and maximim budgets for a movie?\n",
    "\n",
    "imdb_copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a brief statistical description of the data. \n",
    "\n",
    "**ANSWER 1 & 2**\n",
    "\n",
    "From the output, we can deduce that movies have an average runtime of __100__ mins, a minimum of __1__ minute and a maximum of __877__ mins (roughly __14__ hours! interesting.)\n",
    "\n",
    "The average budget for a movie is roughly __40 Million__, a minimum of __0__ and a maximum of __30 Billion__. Notice we are not using any currency to quantify the budget? That's because not all of the budget amounts are denoted in a single currency; they vary, leading to a lot of skewness in our data. We will sort that out later in `Feature Engineering`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.584548Z",
     "iopub.status.busy": "2022-05-01T13:08:00.584303Z",
     "iopub.status.idle": "2022-05-01T13:08:00.664228Z",
     "shell.execute_reply": "2022-05-01T13:08:00.663258Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.584516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 3: The currency symbols denote countries, which countries are movies produced in?\n",
    "\n",
    "imdb_copy.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 3**\n",
    "\n",
    "Here, we can deduce that there are __9,874__ movie directors, __USD10 Million__ budget for movies is more frequent and we can assume that a lot of the movies in the dataset were produced in the United States with a '$' frequency of __6,426__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of questions, we will employ the help of some custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.666463Z",
     "iopub.status.busy": "2022-05-01T13:08:00.665776Z",
     "iopub.status.idle": "2022-05-01T13:08:00.678697Z",
     "shell.execute_reply": "2022-05-01T13:08:00.678026Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.666417Z"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate(series):\n",
    "    \"\"\"\n",
    "    This function `collects` all the values in a \n",
    "    pd.Series and returns a frequency table of the \n",
    "    number of occurence for each value\n",
    "    \"\"\"\n",
    "    # call the collect function the get a list\n",
    "    # of values in a column and store result in\n",
    "    # a variable called items\n",
    "    items = collect(series)\n",
    "    \n",
    "    # initialise an empty dictionary to store \n",
    "    # values and their counts\n",
    "    frequency = {}\n",
    "    \n",
    "    # iterate over items\n",
    "    for item in items:\n",
    "        # check if individual item is in initialised \n",
    "        # dictionary\n",
    "        if item in frequency:\n",
    "            # add 1 to its value count if it exists\n",
    "            # in the initialised dictionary\n",
    "            frequency[item] += 1\n",
    "        else:\n",
    "            # set its value count to 1 if it doesn't\n",
    "            # exist in the initialised dictionary\n",
    "            frequency[item] = 1\n",
    "    \n",
    "    # sort the frequecy table from highest to lowest\n",
    "    sorted_freq = {k: v for k, v in sorted(frequency.items(), reverse=True, key=lambda item: item[1])}\n",
    "    \n",
    "    # return sorted frequency table\n",
    "    return sorted_freq\n",
    "\n",
    "def collect(series):\n",
    "    \"\"\"\n",
    "    This function takes in a pd.Series object that\n",
    "    contains lists as rows and then breaks it down\n",
    "    into a single list.\n",
    "    \"\"\"\n",
    "    # initialise and empty list to contain\n",
    "    # the broken down column of lists\n",
    "    collection = []\n",
    "    \n",
    "    # iterate over the column\n",
    "    for i in series:\n",
    "        # iterate over the column iter\n",
    "        for j in i:\n",
    "            # append value to collection\n",
    "            collection.append(j)\n",
    "    # return collection\n",
    "    return collection\n",
    "\n",
    "def word_cloud(data, category):\n",
    "    \"\"\"\n",
    "    This function generates a word cloud visualisation\n",
    "    \"\"\"\n",
    "    \n",
    "    stopwords = set(STOPWORDS)\n",
    "    \n",
    "    # Instantiate wordcloud object\n",
    "    word_cloud = WordCloud(collocations =False,\n",
    "                           background_color = 'Black',\n",
    "                           stopwords=stopwords,\n",
    "                           width=1600,\n",
    "                           height=800,\n",
    "                           contour_width=2,\n",
    "                           contour_color='steelblue',\n",
    "                          random_state= 1)\n",
    "    # generate wordcloud images\n",
    "    word_cloud.generate_from_frequencies(data)\n",
    "    # Create Plot\n",
    "    \n",
    "    plt.figure(figsize =(10,10))\n",
    "    plt.imshow(word_cloud, interpolation ='bilinear' )\n",
    "    plt.axis('off')\n",
    "    plt.title('Most Frequent {}'.format(category.capitalize()), size = 25, pad =15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:00.690205Z",
     "iopub.status.busy": "2022-05-01T13:08:00.689481Z",
     "iopub.status.idle": "2022-05-01T13:08:01.65958Z",
     "shell.execute_reply": "2022-05-01T13:08:01.6587Z",
     "shell.execute_reply.started": "2022-05-01T13:08:00.690154Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 4: Who is/are the most featured actor(s)\n",
    "\n",
    "def get_top_n(series, n=10, col=None):\n",
    "\n",
    "    features = aggregate(series.apply(lambda row: row.split()))\n",
    "\n",
    "    most_featured = pd.DataFrame.from_dict(features, orient='index').reset_index().head(n)\n",
    "\n",
    "    most_featured.columns = [col, '# of features']\n",
    "\n",
    "    return (most_featured, features)\n",
    "\n",
    "featured_actors = get_top_n(imdb_copy['title_cast'], col='Actors')[1]\n",
    "most_featured_actors_10 = get_top_n(imdb_copy['title_cast'], col='actors')[0]\n",
    "\n",
    "print(f\"There are {len(set(featured_actors.keys()))} actors in total\")\n",
    "most_featured_actors_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:01.661149Z",
     "iopub.status.busy": "2022-05-01T13:08:01.660888Z",
     "iopub.status.idle": "2022-05-01T13:08:02.015229Z",
     "shell.execute_reply": "2022-05-01T13:08:02.01417Z",
     "shell.execute_reply.started": "2022-05-01T13:08:01.661116Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(most_featured_actors_10['actors'], most_featured_actors_10['# of features'], color='blue')\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.3f}%'.format(100 * p.get_height()/ sum(featured_actors.values()))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n",
    "\n",
    "plt.xticks(rotation=20)\n",
    "plt.xlabel('Actors')\n",
    "plt.ylabel('# of features')\n",
    "plt.title(\"Most Featured Actors\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 4**\n",
    "\n",
    "**Samuel L Jackson** has been featured the most; great guy! \n",
    "\n",
    "The list features top-rated stars, hence we can say that top-rated stars get featured the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:02.016841Z",
     "iopub.status.busy": "2022-05-01T13:08:02.01653Z",
     "iopub.status.idle": "2022-05-01T13:08:05.332319Z",
     "shell.execute_reply": "2022-05-01T13:08:05.331395Z",
     "shell.execute_reply.started": "2022-05-01T13:08:02.016802Z"
    }
   },
   "outputs": [],
   "source": [
    "word_cloud(featured_actors, 'actors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows clearly the frequency of appearance of each actor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the top 10 directors with the most movies directed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:05.334024Z",
     "iopub.status.busy": "2022-05-01T13:08:05.333719Z",
     "iopub.status.idle": "2022-05-01T13:08:05.649163Z",
     "shell.execute_reply": "2022-05-01T13:08:05.648495Z",
     "shell.execute_reply.started": "2022-05-01T13:08:05.333976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 5: Director with the most movies?\n",
    "\n",
    "featured_directors = get_top_n(imdb_copy['director'], col='director')[1]\n",
    "most_featured_directors_10 = get_top_n(imdb_copy['director'], col='director')[0]\n",
    "\n",
    "print(f\"There are {len(set(featured_directors.keys()))} directors in total\")\n",
    "most_featured_directors_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 5**\n",
    "\n",
    "**See_Full_Summary**, either that's a weird name from Europe or it means there's no data for director provided for these rows. We will run with the latter and replace this value with something we can work with. But, **Woody Allen**, **Luc Besson** and **Stephen King** are up there too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:05.650957Z",
     "iopub.status.busy": "2022-05-01T13:08:05.650276Z",
     "iopub.status.idle": "2022-05-01T13:08:05.960894Z",
     "shell.execute_reply": "2022-05-01T13:08:05.960198Z",
     "shell.execute_reply.started": "2022-05-01T13:08:05.650919Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(most_featured_directors_10['director'], most_featured_directors_10['# of features'], color='blue')\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.3f}%'.format(100 * p.get_height()/ sum(featured_directors.values()))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n",
    "\n",
    "plt.xticks(rotation=20)\n",
    "plt.xlabel('Directors')\n",
    "plt.ylabel('# of features')\n",
    "plt.title(\"Most Featured Directors\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:05.962707Z",
     "iopub.status.busy": "2022-05-01T13:08:05.962144Z",
     "iopub.status.idle": "2022-05-01T13:08:09.08909Z",
     "shell.execute_reply": "2022-05-01T13:08:09.087767Z",
     "shell.execute_reply.started": "2022-05-01T13:08:05.962673Z"
    }
   },
   "outputs": [],
   "source": [
    "word_cloud(featured_directors, 'directors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:09.090832Z",
     "iopub.status.busy": "2022-05-01T13:08:09.090556Z",
     "iopub.status.idle": "2022-05-01T13:08:09.223515Z",
     "shell.execute_reply": "2022-05-01T13:08:09.222676Z",
     "shell.execute_reply.started": "2022-05-01T13:08:09.090801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 6: What plot keywords are most frequent?\n",
    "\n",
    "keywords = get_top_n(imdb_copy['plot_keywords'], col='keywords')\n",
    "keywords_10 = keywords[0]\n",
    "featured_keywords = keywords[1]\n",
    "\n",
    "print(f\"There are {len(set(featured_keywords.keys()))} keywords in total\")\n",
    "keywords_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 6**\n",
    "\n",
    "**Female**, **Nudity** & **Title** make up the top three most frequent keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:09.226315Z",
     "iopub.status.busy": "2022-05-01T13:08:09.225263Z",
     "iopub.status.idle": "2022-05-01T13:08:09.544059Z",
     "shell.execute_reply": "2022-05-01T13:08:09.543171Z",
     "shell.execute_reply.started": "2022-05-01T13:08:09.226272Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(keywords_10['keywords'], keywords_10['# of features'], color='blue')\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.3f}%'.format(100 * p.get_height()/ sum(featured_keywords.values()))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n",
    "\n",
    "plt.xticks(rotation=20)\n",
    "plt.xlabel('Keywords')\n",
    "plt.ylabel('# of features')\n",
    "plt.title(\"Most Frequent Keywords\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:09.545719Z",
     "iopub.status.busy": "2022-05-01T13:08:09.545405Z",
     "iopub.status.idle": "2022-05-01T13:08:12.859502Z",
     "shell.execute_reply": "2022-05-01T13:08:12.858605Z",
     "shell.execute_reply.started": "2022-05-01T13:08:09.545664Z"
    }
   },
   "outputs": [],
   "source": [
    "word_cloud(featured_keywords, 'keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word cloud paints a clear picture of what words occur most in movies plot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:12.860927Z",
     "iopub.status.busy": "2022-05-01T13:08:12.860707Z",
     "iopub.status.idle": "2022-05-01T13:08:13.868372Z",
     "shell.execute_reply": "2022-05-01T13:08:13.867441Z",
     "shell.execute_reply.started": "2022-05-01T13:08:12.8609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 8: Does runtime influence budget?\n",
    "\n",
    "# for this task, we will create a scatter plot of runtime and budget\n",
    "\n",
    "sns.scatterplot(x=imdb_copy['runtime'].fillna(0), y=imdb_copy['budget_amount'].fillna(0), size=imdb_copy['runtime'].fillna(0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we can't confidently say there's any relationship between runtime and budget. There's also an outlier wayyy up there.\n",
    "\n",
    "Let's confirm this by looking at the correlation figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:13.870785Z",
     "iopub.status.busy": "2022-05-01T13:08:13.869855Z",
     "iopub.status.idle": "2022-05-01T13:08:13.887733Z",
     "shell.execute_reply": "2022-05-01T13:08:13.886812Z",
     "shell.execute_reply.started": "2022-05-01T13:08:13.870745Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_copy[['runtime', 'budget_amount']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 7**\n",
    "\n",
    "The correlation between runtime and budget_amount is __0.055__ which practically means theres no correlation between runtime and budget_amount.\n",
    "\n",
    "A possible explanation for this phenomenon: Recall that there are alot of missing values in the budget column which we filled with Zeros, skewing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:13.889985Z",
     "iopub.status.busy": "2022-05-01T13:08:13.88946Z",
     "iopub.status.idle": "2022-05-01T13:08:13.898861Z",
     "shell.execute_reply": "2022-05-01T13:08:13.897931Z",
     "shell.execute_reply.started": "2022-05-01T13:08:13.889943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 8: How many movies are represented by this dataset?\n",
    "\n",
    "print(f'There are {imdb_copy[\"movieId\"].nunique()} movies in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done with exploring the Imdb_data. We have seen who the most featured actors are and which director has directed the highest number of movies, what the most common plot keywords are and if there's a relationship between the budget and the movie runtime.\n",
    "\n",
    "What about the most common genres? and the most prolific year of production? Let's unearth these from the movies dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. MOVIES DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:13.900791Z",
     "iopub.status.busy": "2022-05-01T13:08:13.900385Z",
     "iopub.status.idle": "2022-05-01T13:08:13.917099Z",
     "shell.execute_reply": "2022-05-01T13:08:13.916203Z",
     "shell.execute_reply.started": "2022-05-01T13:08:13.900756Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's see what we have to work with\n",
    "movies_copy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set outline questions that require answers from this dataset.\n",
    "\n",
    "Agenda:\n",
    "\n",
    "1. What are the most common genres?\n",
    "    - How many genres are there?\n",
    "2. What year are the most movies released?\n",
    "3. what is the total number of unique movies represented in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:13.919294Z",
     "iopub.status.busy": "2022-05-01T13:08:13.918744Z",
     "iopub.status.idle": "2022-05-01T13:08:14.288629Z",
     "shell.execute_reply": "2022-05-01T13:08:14.287772Z",
     "shell.execute_reply.started": "2022-05-01T13:08:13.919257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 1: What are the most common genres?\n",
    "\n",
    "genres = get_top_n(movies_copy['genres'], col='genres')\n",
    "genres_10 = genres[0]\n",
    "featured_genres = genres[1]\n",
    "\n",
    "print(f'There are {len(featured_genres.keys())} genres in total.')\n",
    "genres_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 1**\n",
    "\n",
    "There are a lot of movies that **drama**, followed by **comedy** and **thriller** and there are __22__ genres in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:14.290272Z",
     "iopub.status.busy": "2022-05-01T13:08:14.290022Z",
     "iopub.status.idle": "2022-05-01T13:08:14.6509Z",
     "shell.execute_reply": "2022-05-01T13:08:14.649901Z",
     "shell.execute_reply.started": "2022-05-01T13:08:14.290241Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(genres_10['genres'], genres_10['# of features'], color='blue')\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.3f}%'.format(100 * p.get_height()/ sum(featured_genres.values()))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n",
    "\n",
    "plt.xticks(rotation=20)\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('# of features')\n",
    "plt.title(\"Most Frequent Genres\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see clearly by how much **drama** __20.915%__ and **comedy** __13.997%__ lead the pack. Not minding the tail end '(no and genres' as they are one but got caught, unfortunately, in a split() operation. Nonetheless, we will need to replace these values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:14.6525Z",
     "iopub.status.busy": "2022-05-01T13:08:14.65225Z",
     "iopub.status.idle": "2022-05-01T13:08:15.783741Z",
     "shell.execute_reply": "2022-05-01T13:08:15.782778Z",
     "shell.execute_reply.started": "2022-05-01T13:08:14.652468Z"
    }
   },
   "outputs": [],
   "source": [
    "word_cloud(featured_genres, 'genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:15.785842Z",
     "iopub.status.busy": "2022-05-01T13:08:15.78529Z",
     "iopub.status.idle": "2022-05-01T13:08:17.006875Z",
     "shell.execute_reply": "2022-05-01T13:08:17.005978Z",
     "shell.execute_reply.started": "2022-05-01T13:08:15.78579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 2: What year are the most movies released?\n",
    "\n",
    "year_count = movies_copy.groupby('year')['year'].count()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xticks(np.arange(min(year_count.values),max(year_count.values),10))\n",
    "ax.bar(year_count.index, year_count.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.008603Z",
     "iopub.status.busy": "2022-05-01T13:08:17.008373Z",
     "iopub.status.idle": "2022-05-01T13:08:17.017768Z",
     "shell.execute_reply": "2022-05-01T13:08:17.016798Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.008575Z"
    }
   },
   "outputs": [],
   "source": [
    "year_count.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 2**\n",
    "\n",
    "We can see that the number of the movies released increased over time and peeked in the year __2015__ with __2,513__ movies, then there's a gradual descent from __2016__ and it becomes abrupt in __2019__ - we can credit this to the Coronavirus Pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.020361Z",
     "iopub.status.busy": "2022-05-01T13:08:17.019624Z",
     "iopub.status.idle": "2022-05-01T13:08:17.037476Z",
     "shell.execute_reply": "2022-05-01T13:08:17.036722Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.020309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 3: How many movies are there in the dataset?\n",
    "\n",
    "print(f\"There are {movies_copy['movieId'].nunique()} movies in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we have been able to drilldown a bit into the movies dataset and see what story it tells. Next we will be exploring the meta_data dataset which holds features contained in the movies dataset with a bit more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. META DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.040423Z",
     "iopub.status.busy": "2022-05-01T13:08:17.039483Z",
     "iopub.status.idle": "2022-05-01T13:08:17.071439Z",
     "shell.execute_reply": "2022-05-01T13:08:17.070408Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.040387Z"
    }
   },
   "outputs": [],
   "source": [
    "# As usual, let's see what we are working with\n",
    "\n",
    "meta_copy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot to unearth from this data;\n",
    "\n",
    "1. We will look at how many movies belong to a collection\n",
    "2. The spoken languages distribution\n",
    "3. How original_language differ from spoken languages\n",
    "4. How original_title differs from the title\n",
    "5. Correlation between budget, revenue and runtime\n",
    "6. Correlation between runtime, vote average\n",
    "\n",
    "But first, we will look at the descriptive statistics of both the numerical and the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.073424Z",
     "iopub.status.busy": "2022-05-01T13:08:17.073078Z",
     "iopub.status.idle": "2022-05-01T13:08:17.132526Z",
     "shell.execute_reply": "2022-05-01T13:08:17.131792Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.073378Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table presents us some interesting statistics...\n",
    "\n",
    "The meta_data budget column has 8889 values compare to 7409 of the budget column in Imdb data. There's an average of __21 Million__ spent on movies, a minimum of __1__ and a maximum of __380 Million__, which is fair compared to Imdb's __30 Billion__.\n",
    "\n",
    "The average popularity index of a movie is __2.92__, a minimum of __0__ and a maximum of __547__, indicative of an outlier. We will need to transform this column either by log(1+y) transformation or by taking the square root of each observation.\n",
    "\n",
    "There does not seem to be much going on with revenue, so we will skip it for now.\n",
    "\n",
    "Average runtime for movies in this dataset is __94__ minutes, a minimum of __0__, or perhaps 'no entry' and a maximum of __1256__ mins(**approx 21hours!** yikes!)\n",
    "\n",
    "Average vote_average is __5.6__, with a minimum of __1__ and a maximum of __10__ indicating a range of values from **0 - 10**.\n",
    "\n",
    "Vote_count ranges from __0__ minimum to __14,075__ maximum with a mean of __110__. Year has a minimum of 1, a year we are very certain that motion picture had not be thought of, even remotely and maximum of __2020__. The mean year is __1992__, 30 years ago.\n",
    "\n",
    "We have noted a few anomalies that needs taking care of, and we will do that in `Feature Engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.134523Z",
     "iopub.status.busy": "2022-05-01T13:08:17.133878Z",
     "iopub.status.idle": "2022-05-01T13:08:17.534425Z",
     "shell.execute_reply": "2022-05-01T13:08:17.533481Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.134485Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_copy.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.535831Z",
     "iopub.status.busy": "2022-05-01T13:08:17.535582Z",
     "iopub.status.idle": "2022-05-01T13:08:17.54673Z",
     "shell.execute_reply": "2022-05-01T13:08:17.54599Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.535802Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_copy['adult'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that there are more non-adult films than there are adult ones. Drama takes the lead in the genres category, Most movies are 'english' movies which can be explained by the fact that a large chunk of these movies are produced in the United States of America. Cinderella has been produced more times than any other movie with 11 remakes. There are __42, 277__ unique movies in this dataset\n",
    "\n",
    "\n",
    "Many of the columns in this dataset will be of no use to our algorithm, and as such, will be discarded in `feature engineering`\n",
    "\n",
    "Let's go on with out set agenda for the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.548204Z",
     "iopub.status.busy": "2022-05-01T13:08:17.547978Z",
     "iopub.status.idle": "2022-05-01T13:08:17.561686Z",
     "shell.execute_reply": "2022-05-01T13:08:17.561037Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.548176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 1: how many movies belong to a collection?\n",
    "\n",
    "collection = meta_copy['belongs_to_collection'].value_counts()\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.563363Z",
     "iopub.status.busy": "2022-05-01T13:08:17.562768Z",
     "iopub.status.idle": "2022-05-01T13:08:17.738567Z",
     "shell.execute_reply": "2022-05-01T13:08:17.737627Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.56333Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(collection.index, collection.values)\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.1f}%'.format(100 * p.get_height()/len(meta_copy['belongs_to_collection'].tolist()))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 1**\n",
    "\n",
    "False has a big chunk of this with __40,959__ values making up __90.1%__ of the entire column. This means that many movies do not belong to a collection i.e have sequels of part of a franchise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.740278Z",
     "iopub.status.busy": "2022-05-01T13:08:17.739994Z",
     "iopub.status.idle": "2022-05-01T13:08:17.835364Z",
     "shell.execute_reply": "2022-05-01T13:08:17.834527Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.74024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 2: What is the spoken languages distribution?\n",
    "meta_copy['spoken_languages'] = meta_copy['spoken_languages'].fillna('')\n",
    "languages_set = get_top_n(meta_copy['spoken_languages'], col='languages')\n",
    "languages = languages_set[1]\n",
    "languages_10 = languages_set[0]\n",
    "\n",
    "print(f'There are {len(languages.keys())} languages in total.')\n",
    "\n",
    "languages_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The languages are encoded and difficult to read. To solve this, we will employ a dataset that contains the names that match correctly to these language codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.837516Z",
     "iopub.status.busy": "2022-05-01T13:08:17.836778Z",
     "iopub.status.idle": "2022-05-01T13:08:17.854451Z",
     "shell.execute_reply": "2022-05-01T13:08:17.853575Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.837477Z"
    }
   },
   "outputs": [],
   "source": [
    "language_map = pd.read_csv('/kaggle/input//languages-and-codes/language-codes.csv')\n",
    "language_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.856124Z",
     "iopub.status.busy": "2022-05-01T13:08:17.855872Z",
     "iopub.status.idle": "2022-05-01T13:08:17.879939Z",
     "shell.execute_reply": "2022-05-01T13:08:17.879008Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.856093Z"
    }
   },
   "outputs": [],
   "source": [
    "languages_df = pd.DataFrame.from_dict(languages, orient='index').reset_index()\\\n",
    "                .rename(columns={'index':'language_code', 0: '# of features'})\n",
    "languages_df = languages_df.merge(language_map, how='left', left_on='language_code', right_on='alpha2')\n",
    "languages_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.882498Z",
     "iopub.status.busy": "2022-05-01T13:08:17.881728Z",
     "iopub.status.idle": "2022-05-01T13:08:17.895865Z",
     "shell.execute_reply": "2022-05-01T13:08:17.894717Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.882445Z"
    }
   },
   "outputs": [],
   "source": [
    "languages_df = languages_df.dropna()\n",
    "languages = dict(zip(languages_df['English'].tolist(), languages_df['# of features'].tolist()))\n",
    "languages_10 = pd.DataFrame.from_dict(dict(list(languages.items())[:10]), orient='index').reset_index()\\\n",
    "                .rename(columns={'index':'languages', 0:'# of features'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:17.897789Z",
     "iopub.status.busy": "2022-05-01T13:08:17.897333Z",
     "iopub.status.idle": "2022-05-01T13:08:18.236374Z",
     "shell.execute_reply": "2022-05-01T13:08:18.235382Z",
     "shell.execute_reply.started": "2022-05-01T13:08:17.89775Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(languages_10['languages'], languages_10['# of features'], color='blue')\n",
    "\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.2f}%'.format(100 * p.get_height()/ sum(languages.values()))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel('Languages')\n",
    "plt.ylabel('# of features')\n",
    "plt.title(\"Most Spoken Languages\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:18.238801Z",
     "iopub.status.busy": "2022-05-01T13:08:18.238188Z",
     "iopub.status.idle": "2022-05-01T13:08:20.081375Z",
     "shell.execute_reply": "2022-05-01T13:08:20.080303Z",
     "shell.execute_reply.started": "2022-05-01T13:08:18.238749Z"
    }
   },
   "outputs": [],
   "source": [
    "word_cloud(languages, 'Languages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 2**\n",
    "\n",
    "The English language is the most spoken language in movies, being spoken in roughly __55%__ of movies, followed by French in __8%__ of movies and German in __5%__ of movies. This is not surprising as English Language is the language of the world and it will only make sense to have more movies in that language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:20.083056Z",
     "iopub.status.busy": "2022-05-01T13:08:20.082797Z",
     "iopub.status.idle": "2022-05-01T13:08:20.109631Z",
     "shell.execute_reply": "2022-05-01T13:08:20.108593Z",
     "shell.execute_reply.started": "2022-05-01T13:08:20.083026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 3: How original_language differ from spoken languages\n",
    "\n",
    "# using the `~` sign to negate .isin() \n",
    "diff = meta_copy[['original_language', 'spoken_languages']][~meta_copy['original_language'].isin(meta_copy['spoken_languages'])]\n",
    "\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 3**\n",
    "\n",
    "`Spoken_languages` looks to be a more useable column than `original_language` because some data points contain more than 1 language. This will be very useful for a recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:20.112295Z",
     "iopub.status.busy": "2022-05-01T13:08:20.111536Z",
     "iopub.status.idle": "2022-05-01T13:08:20.19077Z",
     "shell.execute_reply": "2022-05-01T13:08:20.189752Z",
     "shell.execute_reply.started": "2022-05-01T13:08:20.112241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 4: How original_title differs from the title\n",
    "\n",
    "diff = meta_copy[['original_title', 'title']][~meta_copy['original_title'].str.lower().isin(meta_copy['title'])]\n",
    "\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 4**\n",
    "\n",
    "Notice that the **original_title** has some titles written in the original dialect of the country they were produced in, **title** on the other hand has the English translated version of such movie titles, so we're keeping that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:20.192506Z",
     "iopub.status.busy": "2022-05-01T13:08:20.192158Z",
     "iopub.status.idle": "2022-05-01T13:08:20.211615Z",
     "shell.execute_reply": "2022-05-01T13:08:20.210976Z",
     "shell.execute_reply.started": "2022-05-01T13:08:20.192468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 5: Correlation between budget, revenue and runtime\n",
    "meta_copy[['budget', 'revenue', 'runtime']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:20.213565Z",
     "iopub.status.busy": "2022-05-01T13:08:20.21267Z",
     "iopub.status.idle": "2022-05-01T13:08:25.393887Z",
     "shell.execute_reply": "2022-05-01T13:08:25.392964Z",
     "shell.execute_reply.started": "2022-05-01T13:08:20.213529Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.pairplot(meta_copy[['budget', 'revenue', 'runtime']], corner=True)\n",
    "# g.fig.set_figheight(10)\n",
    "g.fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 5**\n",
    "\n",
    "There appears to be a strong relationship between __budget__ and __revenue__. From the plot, we can say that the higher the budget, the higher the revenue. \n",
    "\n",
    "There's a very weak relationship between runtime and the other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:25.395545Z",
     "iopub.status.busy": "2022-05-01T13:08:25.395262Z",
     "iopub.status.idle": "2022-05-01T13:08:25.417777Z",
     "shell.execute_reply": "2022-05-01T13:08:25.416811Z",
     "shell.execute_reply.started": "2022-05-01T13:08:25.39551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 6: Correlation between runtime, vote average, vote count\n",
    "\n",
    "meta_copy[['runtime', 'vote_average', 'vote_count']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:08:25.420208Z",
     "iopub.status.busy": "2022-05-01T13:08:25.419487Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.pairplot(meta_copy[['runtime', 'vote_average', 'vote_count']], corner=True)\n",
    "# g.fig.set_figheight(10)\n",
    "g.fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 6**\n",
    "\n",
    "There are no meaningful relationships to observe here as all correlation values between variables are closer to __0__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting analysis will be to find out if there's a relationship between the number of top rated stars in a movie and the budget for the movie. Let us find out!\n",
    "\n",
    "First we will define a custom class to help us breakdown the title_cast column of Imdb data and create an arbitrary star rating based solely on number of appearances in movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BRINGING BACK IMDB DATA FOR A BIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer a_list_actors, b_list_actors, c_list_actors\n",
    "\n",
    "def classify(df, col, columns=[], threshold=[]):\n",
    "    # get the featured items into a dictionary\n",
    "    most_featured = aggregate(df[col])\n",
    "    # convert the dictionary into a pd.DataFrame\n",
    "    most_featured_df = pd.DataFrame.from_dict(most_featured.items())\n",
    "    # change the column names\n",
    "    most_featured_df.columns = columns\n",
    "\n",
    "    print('feature extraction complete')\n",
    "    \n",
    "    # classify based on threshold supplied\n",
    "    a_list_actors = most_featured_df[columns[0]][most_featured_df[columns[1]] >= threshold[0]].tolist()\n",
    "    b_list_actors = most_featured_df[columns[0]][(most_featured_df[columns[1]] >= threshold[1]) & (most_featured_df[columns[1]] < threshold[0])].tolist()\n",
    "    c_list_actors = most_featured_df[columns[0]][most_featured_df[columns[1]] < threshold[1]].tolist()\n",
    "    \n",
    "    # initialise empty lists to save counts of items \n",
    "    # per row\n",
    "    a_list_count = []\n",
    "    b_list_count = []\n",
    "    c_list_count = []\n",
    "    \n",
    "    # iterate over each row in the column\n",
    "    for row in df[col]:\n",
    "        a_counter = 0\n",
    "        b_counter = 0\n",
    "        c_counter = 0\n",
    "        \n",
    "        # get every item in a row\n",
    "        for item in row:\n",
    "            # do conditional checking\n",
    "            # to know which class each\n",
    "            # item is in and append a count\n",
    "            # to the appropriate quarters\n",
    "            if item in a_list_actors:\n",
    "                a_counter += 1\n",
    "            if item in b_list_actors:\n",
    "                b_counter += 1\n",
    "            if item in c_list_actors:\n",
    "                c_counter += 1\n",
    "        \n",
    "        # append a count to the empty list initialised\n",
    "        a_list_count.append(a_counter)\n",
    "        b_list_count.append(b_counter)\n",
    "        c_list_count.append(c_counter)\n",
    "\n",
    "    return (a_list_count, b_list_count, c_list_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2022-05-01T13:14:27.40947Z",
     "shell.execute_reply": "2022-05-01T13:14:27.4084Z",
     "shell.execute_reply.started": "2022-05-01T13:08:46.943174Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_df = imdb_copy.copy(deep=True)\n",
    "temp_df['title_cast'] = temp_df['title_cast'].apply(lambda row: row.split() if type(row) ==str else '[]')\n",
    "a, b, c = classify(temp_df, 'title_cast', columns=['actor', '# of features'], threshold=[30, 15])\n",
    "temp_df['a_stars']  = a\n",
    "\n",
    "temp_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:27.412104Z",
     "iopub.status.busy": "2022-05-01T13:14:27.41125Z",
     "iopub.status.idle": "2022-05-01T13:14:27.428565Z",
     "shell.execute_reply": "2022-05-01T13:14:27.427804Z",
     "shell.execute_reply.started": "2022-05-01T13:14:27.41206Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_df[['budget_amount', 'a_stars']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:27.431345Z",
     "iopub.status.busy": "2022-05-01T13:14:27.430385Z",
     "iopub.status.idle": "2022-05-01T13:14:51.819213Z",
     "shell.execute_reply": "2022-05-01T13:14:51.818219Z",
     "shell.execute_reply.started": "2022-05-01T13:14:27.431306Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.pairplot(temp_df[['budget_amount', 'a_stars']], corner=True)\n",
    "# g.fig.set_figheight(10)\n",
    "g.fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shocking as there seems to be absolutely no relationship between budget for a movie and the number of superstars in the movie. This can be misleading as there may be budget amounts in other currencies than the US Dollar that will appear to be higher/or lower than the actual amount in US Dollar. \n",
    "\n",
    "Let's fillter the dataset for only budget denoted in dollars and perform the correlation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:51.820682Z",
     "iopub.status.busy": "2022-05-01T13:14:51.820437Z",
     "iopub.status.idle": "2022-05-01T13:14:51.832421Z",
     "shell.execute_reply": "2022-05-01T13:14:51.831577Z",
     "shell.execute_reply.started": "2022-05-01T13:14:51.820652Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_df2 = temp_df[temp_df['symbol'] == '$']\n",
    "\n",
    "temp_df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have roughly __6,400__ rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:51.833783Z",
     "iopub.status.busy": "2022-05-01T13:14:51.833546Z",
     "iopub.status.idle": "2022-05-01T13:14:51.849871Z",
     "shell.execute_reply": "2022-05-01T13:14:51.848844Z",
     "shell.execute_reply.started": "2022-05-01T13:14:51.833755Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_df2[['budget_amount', 'a_stars']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:51.85312Z",
     "iopub.status.busy": "2022-05-01T13:14:51.852374Z",
     "iopub.status.idle": "2022-05-01T13:14:53.020748Z",
     "shell.execute_reply": "2022-05-01T13:14:53.01974Z",
     "shell.execute_reply.started": "2022-05-01T13:14:51.853061Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.pairplot(temp_df2[['budget_amount', 'a_stars']], corner=True)\n",
    "# g.fig.set_figheight(10)\n",
    "g.fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there's an improvement in the correlation values when considering only budgets denoted in US Dollars, it is still a weak relationship and nothing much can be made of it. Perhaps, during `Feature engineering`, after converting all currencies to the dollar, we can get a different result (if eventually we choose to work with this dataset or the column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which companies produce the most movies and in which country are most movies produced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:53.022796Z",
     "iopub.status.busy": "2022-05-01T13:14:53.022414Z",
     "iopub.status.idle": "2022-05-01T13:14:53.159004Z",
     "shell.execute_reply": "2022-05-01T13:14:53.158066Z",
     "shell.execute_reply.started": "2022-05-01T13:14:53.022749Z"
    }
   },
   "outputs": [],
   "source": [
    "# Companies that produce the most movies\n",
    "\n",
    "top_n = get_top_n(meta_copy['production_companies'].dropna(), n=20, col='company')\n",
    "\n",
    "all_companies = top_n[1]\n",
    "top_20_companies = top_n[0]\n",
    "\n",
    "top_20_companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warner Bros. has produced the most movies in this dataset with a total of __1,250__ movies produced, followed closely by Metro Goldwyn Meyer with __1,075__, then Paramount Pictures with __1,003__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:53.160568Z",
     "iopub.status.busy": "2022-05-01T13:14:53.160327Z",
     "iopub.status.idle": "2022-05-01T13:14:53.719622Z",
     "shell.execute_reply": "2022-05-01T13:14:53.718883Z",
     "shell.execute_reply.started": "2022-05-01T13:14:53.160537Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(top_20_companies['company'], top_20_companies['# of features'], color='blue')\n",
    "\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.1f}%'.format(100 * p.get_height()/ sum(all_companies.values()))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Companies')\n",
    "plt.ylabel('# of features')\n",
    "plt.title(\"Production Companies With Most Movies\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:53.721267Z",
     "iopub.status.busy": "2022-05-01T13:14:53.720892Z",
     "iopub.status.idle": "2022-05-01T13:14:56.749588Z",
     "shell.execute_reply": "2022-05-01T13:14:56.74888Z",
     "shell.execute_reply.started": "2022-05-01T13:14:53.721234Z"
    }
   },
   "outputs": [],
   "source": [
    "word_cloud(all_companies, 'Production Companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:56.751238Z",
     "iopub.status.busy": "2022-05-01T13:14:56.750859Z",
     "iopub.status.idle": "2022-05-01T13:14:56.830393Z",
     "shell.execute_reply": "2022-05-01T13:14:56.829455Z",
     "shell.execute_reply.started": "2022-05-01T13:14:56.751206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Countries with the most movies produced in\n",
    "\n",
    "top_n = get_top_n(meta_copy['production_countries'].dropna(), n=20, col='countries')\n",
    "\n",
    "all_countries = top_n[1]\n",
    "top_20_countries = top_n[0]\n",
    "\n",
    "top_20_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the movies were produced in the United States of America with a whooping __21,150__ movies, which is not a surprise as the US is a powerhouse in the movie industry, followed by the United Kingdom with a meager __4,091__ movies and France with __3,936__ movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:56.8321Z",
     "iopub.status.busy": "2022-05-01T13:14:56.831838Z",
     "iopub.status.idle": "2022-05-01T13:14:57.385773Z",
     "shell.execute_reply": "2022-05-01T13:14:57.384801Z",
     "shell.execute_reply.started": "2022-05-01T13:14:56.832068Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(top_20_countries['countries'], top_20_countries['# of features'], color='blue')\n",
    "\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.0f}%'.format(100 * p.get_height()/ sum(all_countries.values()))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Countries')\n",
    "plt.ylabel('# of features')\n",
    "plt.title(\"Production Countries\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:57.387355Z",
     "iopub.status.busy": "2022-05-01T13:14:57.38711Z",
     "iopub.status.idle": "2022-05-01T13:14:59.227185Z",
     "shell.execute_reply": "2022-05-01T13:14:59.226417Z",
     "shell.execute_reply.started": "2022-05-01T13:14:57.387324Z"
    }
   },
   "outputs": [],
   "source": [
    "word_cloud(all_countries, \"Production Countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is telling of the magnitude of difference between the united states and other countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:59.228828Z",
     "iopub.status.busy": "2022-05-01T13:14:59.228445Z",
     "iopub.status.idle": "2022-05-01T13:14:59.234234Z",
     "shell.execute_reply": "2022-05-01T13:14:59.233071Z",
     "shell.execute_reply.started": "2022-05-01T13:14:59.228795Z"
    }
   },
   "outputs": [],
   "source": [
    "del temp_df, temp_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done with exploring the meta_data dataset. In this section, we looked at the distribution of some certain features, how some features differ from similar features and determined the relationship between two or more numeric features. We found that budget and revenue have a moderately positive relationship, so it is either we drop one column or engineer a new feature from both columns (Profit, for example).\n",
    "\n",
    "Next we explore genome_scores and genome_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. GENOME SCORES AND GENOME TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:59.236356Z",
     "iopub.status.busy": "2022-05-01T13:14:59.236053Z",
     "iopub.status.idle": "2022-05-01T13:14:59.256763Z",
     "shell.execute_reply": "2022-05-01T13:14:59.254953Z",
     "shell.execute_reply.started": "2022-05-01T13:14:59.236303Z"
    }
   },
   "outputs": [],
   "source": [
    "# show the head of the genome_scores\n",
    "genome_scores.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How many unique movies are in the dataset\n",
    "2. How many unique tags are in the dataset\n",
    "3. What are the Average, Minimum and Maximum relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:59.259128Z",
     "iopub.status.busy": "2022-05-01T13:14:59.258761Z",
     "iopub.status.idle": "2022-05-01T13:14:59.348496Z",
     "shell.execute_reply": "2022-05-01T13:14:59.347434Z",
     "shell.execute_reply.started": "2022-05-01T13:14:59.259069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 1: How many unique movies are in the dataset\n",
    "print(f\"There are {genome_scores['movieId'].nunique()} unique movies in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 1:**\n",
    "\n",
    "There are **13,816** unique movies in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:59.350065Z",
     "iopub.status.busy": "2022-05-01T13:14:59.349789Z",
     "iopub.status.idle": "2022-05-01T13:14:59.433934Z",
     "shell.execute_reply": "2022-05-01T13:14:59.432876Z",
     "shell.execute_reply.started": "2022-05-01T13:14:59.350033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 2: How many unique tags are in the dataset\n",
    "print(f\"There are {genome_scores['tagId'].nunique()} unique tags in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 2:**\n",
    "\n",
    "There are **1,128** unique tags in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:14:59.436368Z",
     "iopub.status.busy": "2022-05-01T13:14:59.435725Z",
     "iopub.status.idle": "2022-05-01T13:15:00.34838Z",
     "shell.execute_reply": "2022-05-01T13:15:00.347433Z",
     "shell.execute_reply.started": "2022-05-01T13:14:59.43633Z"
    }
   },
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "genome_scores['relevance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 3:**\n",
    "\n",
    "The average relevance score is __0.116__, a minimum of __0.00025__ and a maximum of __100__ indicating that the scale of relevance is from __0% - 100%__. There, apparently, are a lot of low rated tags, with a relevance score of __0.14%__ being higher than __75%__ of the data. Only the most relevant tags (at a certain threshold) should be considered. This will be addressed in `Feature Engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:00.350565Z",
     "iopub.status.busy": "2022-05-01T13:15:00.349866Z",
     "iopub.status.idle": "2022-05-01T13:15:00.360524Z",
     "shell.execute_reply": "2022-05-01T13:15:00.359689Z",
     "shell.execute_reply.started": "2022-05-01T13:15:00.350526Z"
    }
   },
   "outputs": [],
   "source": [
    "# view genome tags\n",
    "genome_tags.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:00.361921Z",
     "iopub.status.busy": "2022-05-01T13:15:00.361702Z",
     "iopub.status.idle": "2022-05-01T13:15:00.382236Z",
     "shell.execute_reply": "2022-05-01T13:15:00.381562Z",
     "shell.execute_reply.started": "2022-05-01T13:15:00.361893Z"
    }
   },
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "genome_tags.describe(include='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __1,128__ rows, with __1,128__ unique values. Nothing much to gain from this. We will. nevertherless, look deeper when we merge the two dataframes in `Feature Engineering`.\n",
    "\n",
    "\n",
    "We will take a look at the train dataset which contains the movie ratings and the users that provided the ratings next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. TRAIN (RATINGS) DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:00.384225Z",
     "iopub.status.busy": "2022-05-01T13:15:00.383635Z",
     "iopub.status.idle": "2022-05-01T13:15:00.396013Z",
     "shell.execute_reply": "2022-05-01T13:15:00.394988Z",
     "shell.execute_reply.started": "2022-05-01T13:15:00.384185Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:00.400803Z",
     "iopub.status.busy": "2022-05-01T13:15:00.39862Z",
     "iopub.status.idle": "2022-05-01T13:15:02.218977Z",
     "shell.execute_reply": "2022-05-01T13:15:02.218076Z",
     "shell.execute_reply.started": "2022-05-01T13:15:00.400721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can deduce that there are __10 Million__ rows of data for each column, meaning no missing data as we've established when we took a <a href=\"#sneak-peek\">sneak peek</a> into the data earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "1. How many unique users are represented in the dataset\n",
    "2. How many movies were rated in the dataset\n",
    "3. What are the Mean, Minimum and Maximum ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:02.221563Z",
     "iopub.status.busy": "2022-05-01T13:15:02.220911Z",
     "iopub.status.idle": "2022-05-01T13:15:02.286808Z",
     "shell.execute_reply": "2022-05-01T13:15:02.284631Z",
     "shell.execute_reply.started": "2022-05-01T13:15:02.221518Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 1: How many unique users are represented in the dataset\n",
    "\n",
    "print(f\"There are {train['userId'].nunique()} users in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 1:**\n",
    "\n",
    "There are __162,541__ unique users in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:02.289268Z",
     "iopub.status.busy": "2022-05-01T13:15:02.288659Z",
     "iopub.status.idle": "2022-05-01T13:15:02.352456Z",
     "shell.execute_reply": "2022-05-01T13:15:02.351542Z",
     "shell.execute_reply.started": "2022-05-01T13:15:02.289217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 2: How many movies were rated in the dataset\n",
    "\n",
    "print(f\"There are {train['movieId'].nunique()} movies in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 2:**\n",
    "\n",
    "There are __48,213__ movies rated in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 3:**\n",
    "\n",
    "The average rating is __3.5__, a minimum of __0.5__ and a maximum of __5.0__. Indicating that rating is on a scale of __0 - 5__ points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:02.354014Z",
     "iopub.status.busy": "2022-05-01T13:15:02.353769Z",
     "iopub.status.idle": "2022-05-01T13:15:02.461769Z",
     "shell.execute_reply": "2022-05-01T13:15:02.46081Z",
     "shell.execute_reply.started": "2022-05-01T13:15:02.353981Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets see how ratings distributed \n",
    "ratings = train['rating'].value_counts().reset_index()\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:02.464468Z",
     "iopub.status.busy": "2022-05-01T13:15:02.463528Z",
     "iopub.status.idle": "2022-05-01T13:15:09.394958Z",
     "shell.execute_reply": "2022-05-01T13:15:09.393666Z",
     "shell.execute_reply.started": "2022-05-01T13:15:02.464415Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(ratings['index'], ratings['rating'], color='darkblue',\n",
    "                order=ratings.sort_values('rating',ascending = False)['index'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.1f}%'.format(100 * p.get_height()/len(train['rating'].tolist()))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y),ha='right', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of users gave movies a rating of __4.0__ at __26.5%__, followed by __3.0__ at __19.6%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:09.397223Z",
     "iopub.status.busy": "2022-05-01T13:15:09.396888Z",
     "iopub.status.idle": "2022-05-01T13:15:09.903935Z",
     "shell.execute_reply": "2022-05-01T13:15:09.903111Z",
     "shell.execute_reply.started": "2022-05-01T13:15:09.397177Z"
    }
   },
   "outputs": [],
   "source": [
    "# What movies received the most ratings by volume of users who rated.\n",
    "\n",
    "movies_most_rated = train.groupby(['movieId'])\\\n",
    "                            .agg({'rating':'mean', 'userId':'count'}).reset_index()\\\n",
    "                            .rename(columns={'rating':'ave_rating', 'userId':'rating_count'})\\\n",
    "                            .sort_values(['rating_count', 'ave_rating'], ascending=False)\n",
    "\n",
    "most_rated_20 = movies_most_rated.head(20).reset_index().drop('index',axis=1)\n",
    "\n",
    "most_rated_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:09.90619Z",
     "iopub.status.busy": "2022-05-01T13:15:09.905866Z",
     "iopub.status.idle": "2022-05-01T13:15:09.936044Z",
     "shell.execute_reply": "2022-05-01T13:15:09.935128Z",
     "shell.execute_reply.started": "2022-05-01T13:15:09.906145Z"
    }
   },
   "outputs": [],
   "source": [
    "movies_most_rated.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a plot to tell us a better story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:09.937768Z",
     "iopub.status.busy": "2022-05-01T13:15:09.937496Z",
     "iopub.status.idle": "2022-05-01T13:15:10.389621Z",
     "shell.execute_reply": "2022-05-01T13:15:10.389006Z",
     "shell.execute_reply.started": "2022-05-01T13:15:09.937736Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(most_rated_20['movieId'], most_rated_20['rating_count'], color='darkblue',\n",
    "                order=most_rated_20.sort_values('rating_count',ascending = False).movieId)\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    ratings = '{:.2f}'.format(round(most_rated_20['ave_rating'].tolist()[i],2))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(ratings, (x, y),ha='right', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "The movie with the highest number of ratings is the movie with Id __318__, which has over __32,000__ user ratings and an average rating of __4.42__.\n",
    "\n",
    "The rating of a movie is most trustworthy when it has been rated by a lot of users. Movie 318 must be a great movie. What is movie __318__? Let's ask _Movies_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:10.391188Z",
     "iopub.status.busy": "2022-05-01T13:15:10.390845Z",
     "iopub.status.idle": "2022-05-01T13:15:10.39786Z",
     "shell.execute_reply": "2022-05-01T13:15:10.397029Z",
     "shell.execute_reply.started": "2022-05-01T13:15:10.391157Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_318 = movies['title'][movies['movieId'] == 318]\n",
    "\n",
    "print(f\"Movie 318: {movie_318.values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MOVIE 318 IS **THE SHAWSHANK REDEMPTION**.\n",
    "\n",
    "little wonder it has been rated the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION?**\n",
    "\n",
    "Which user has rated the most movies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:10.399535Z",
     "iopub.status.busy": "2022-05-01T13:15:10.399284Z",
     "iopub.status.idle": "2022-05-01T13:15:11.16816Z",
     "shell.execute_reply": "2022-05-01T13:15:11.167366Z",
     "shell.execute_reply.started": "2022-05-01T13:15:10.399506Z"
    }
   },
   "outputs": [],
   "source": [
    "frequent_user = train.groupby(['userId'])\\\n",
    "                            .agg({'rating':'mean', 'movieId':'count'}).reset_index()\\\n",
    "                            .rename(columns={'rating':'ave_rating', 'movieId':'movies_count'})\\\n",
    "                            .sort_values(['movies_count', 'ave_rating'], ascending=False)\n",
    "\n",
    "most_ratings_20 = frequent_user.head(20).reset_index().drop('index',axis=1)\n",
    "\n",
    "most_ratings_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:11.170254Z",
     "iopub.status.busy": "2022-05-01T13:15:11.169377Z",
     "iopub.status.idle": "2022-05-01T13:15:11.688303Z",
     "shell.execute_reply": "2022-05-01T13:15:11.687428Z",
     "shell.execute_reply.started": "2022-05-01T13:15:11.170202Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(most_ratings_20['userId'], most_ratings_20['movies_count'], color='darkblue',\n",
    "                 order=most_ratings_20.sort_values('movies_count',ascending = False).userId)\n",
    "\n",
    "for i, p in enumerate(ax.patches):\n",
    "    ratings = '{:.2f}'.format(round(most_ratings_20['ave_rating'].tolist()[i],2))\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(ratings, (x, y),ha='right', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User __72315__ has rated __12, 952__ movies, with an average rating of __3.09__. <h2>Stellar!<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:11.690913Z",
     "iopub.status.busy": "2022-05-01T13:15:11.689737Z",
     "iopub.status.idle": "2022-05-01T13:15:11.758783Z",
     "shell.execute_reply": "2022-05-01T13:15:11.758067Z",
     "shell.execute_reply.started": "2022-05-01T13:15:11.690862Z"
    }
   },
   "outputs": [],
   "source": [
    "train[train.userId == 72315]['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This user has rated more movies a __3.0__ and __3.5__ than other scales combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we have dived into the world of ratings and have answered a couple of interesting questions.\n",
    "\n",
    "\n",
    "And for the final show of the Exploratory Data Analysis, we will look into the tags dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. TAGS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:11.760907Z",
     "iopub.status.busy": "2022-05-01T13:15:11.760358Z",
     "iopub.status.idle": "2022-05-01T13:15:11.775919Z",
     "shell.execute_reply": "2022-05-01T13:15:11.775013Z",
     "shell.execute_reply.started": "2022-05-01T13:15:11.760857Z"
    }
   },
   "outputs": [],
   "source": [
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:11.778369Z",
     "iopub.status.busy": "2022-05-01T13:15:11.777835Z",
     "iopub.status.idle": "2022-05-01T13:15:11.863663Z",
     "shell.execute_reply": "2022-05-01T13:15:11.862444Z",
     "shell.execute_reply.started": "2022-05-01T13:15:11.778314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Descriptive Statistics\n",
    "tags.describe(include='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __73,050__ unique tags in this dataframe, with Sci-fi taking the lead with __8,330__ occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:11.865375Z",
     "iopub.status.busy": "2022-05-01T13:15:11.864964Z",
     "iopub.status.idle": "2022-05-01T13:15:11.876506Z",
     "shell.execute_reply": "2022-05-01T13:15:11.87572Z",
     "shell.execute_reply.started": "2022-05-01T13:15:11.865339Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"There are {tags.userId.nunique()} unique users in the dataframe\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:11.878224Z",
     "iopub.status.busy": "2022-05-01T13:15:11.877819Z",
     "iopub.status.idle": "2022-05-01T13:15:11.892358Z",
     "shell.execute_reply": "2022-05-01T13:15:11.891676Z",
     "shell.execute_reply.started": "2022-05-01T13:15:11.878192Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"There are {tags.movieId.nunique()} unique movies in the dataframe\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:15:11.894544Z",
     "iopub.status.busy": "2022-05-01T13:15:11.894037Z",
     "iopub.status.idle": "2022-05-01T13:15:15.524908Z",
     "shell.execute_reply": "2022-05-01T13:15:15.524168Z",
     "shell.execute_reply.started": "2022-05-01T13:15:11.894508Z"
    }
   },
   "outputs": [],
   "source": [
    "tags_freq = {}\n",
    "\n",
    "for tag in tags['tag']:\n",
    "    if tag in tags_freq:\n",
    "        tags_freq[tag] += 1\n",
    "    else:\n",
    "        tags_freq[tag] = 1\n",
    "\n",
    "word_cloud(tags_freq, 'tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see how frequently each tag occurs; the bigger the tag, the more frequent it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes the `Exploratory Data Analysis` Section and it has been a long ride. Here, we have been able to drilldown into our datasets and unearth some details about the features they contain and how that might affect the **Movie Recommendation Systems** we want to develop.\n",
    "\n",
    "As all these datasets cannot be used individually to make recommendations, they have to be merged into one dataframe, taking only the key features that are of importance to our algorithms and discarding the rest. We may also be required to engineer new features. With that said, we will go into `Feature Engineering` next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n",
    "</div>\n",
    "<div id='FE'>\n",
    "    <h2 style='text-transform: uppercase;'>Feature Engineering</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering or feature extraction is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. The motivation is to use these extra features to improve the quality of results from a machine learning process, compared with supplying only the raw data to the machine learning process. - wikipedia\n",
    "\n",
    "Feature engineering is a machine learning technique that leverages data to create new variables that aren't in the training set. It can produce new features for both supervised and unsupervised learning, with the goal of simplifying and speeding up data transformations while also enhancing model accuracy. - TowardsDataScience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first point of call will be to determine which of Imdb data and meta_data should we use, or what features from both datasets we can combine.\n",
    "\n",
    "We have seen that meta_data has more observations than imdb data. Meta_data has 42,277 unique movies compared to 27,778 movies in the Imdb data. Both datasets have important features that will ben of immense help.\n",
    "\n",
    "So, we will merge both dataframes, only picking our features that interest us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:08.301973Z",
     "iopub.status.busy": "2022-05-01T13:21:08.30157Z",
     "iopub.status.idle": "2022-05-01T13:21:08.307266Z",
     "shell.execute_reply": "2022-05-01T13:21:08.306677Z",
     "shell.execute_reply.started": "2022-05-01T13:21:08.30193Z"
    }
   },
   "outputs": [],
   "source": [
    "# print imdb_data columns\n",
    "print(list(imdb_copy.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:08.531247Z",
     "iopub.status.busy": "2022-05-01T13:21:08.530422Z",
     "iopub.status.idle": "2022-05-01T13:21:08.535577Z",
     "shell.execute_reply": "2022-05-01T13:21:08.534756Z",
     "shell.execute_reply.started": "2022-05-01T13:21:08.531185Z"
    }
   },
   "outputs": [],
   "source": [
    "# print meta_data columns\n",
    "print(list(meta_copy.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:08.699049Z",
     "iopub.status.busy": "2022-05-01T13:21:08.698252Z",
     "iopub.status.idle": "2022-05-01T13:21:08.7047Z",
     "shell.execute_reply": "2022-05-01T13:21:08.703744Z",
     "shell.execute_reply.started": "2022-05-01T13:21:08.699003Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_columns_of_interest = ['movieId', 'title_cast', 'director', 'runtime', 'plot_keywords']\n",
    "meta_columns_of_interest = ['imdbId', 'title', 'spoken_languages', 'overview', 'popularity',\n",
    "                            'production_companies', 'production_countries',\n",
    "                            'tagline', 'vote_average', 'vote_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are seemingly no similar columns between the 2 datasets and as such, merging will prove very difficult to do. Luckily for us, we have a data of links to assist us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:09.856051Z",
     "iopub.status.busy": "2022-05-01T13:21:09.855665Z",
     "iopub.status.idle": "2022-05-01T13:21:09.868439Z",
     "shell.execute_reply": "2022-05-01T13:21:09.867574Z",
     "shell.execute_reply.started": "2022-05-01T13:21:09.855979Z"
    }
   },
   "outputs": [],
   "source": [
    "links.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The links dataset has 3 columns, all of which are 'Primary Keys' to other tables. We are only interested in __movieId__ and __imdbId__.\n",
    "\n",
    "First, we will merge meta_data with links, then merge the output with the imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:10.824477Z",
     "iopub.status.busy": "2022-05-01T13:21:10.823432Z",
     "iopub.status.idle": "2022-05-01T13:21:10.933449Z",
     "shell.execute_reply": "2022-05-01T13:21:10.93254Z",
     "shell.execute_reply.started": "2022-05-01T13:21:10.824395Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_link = links.merge(meta_copy[meta_columns_of_interest], on='imdbId')\n",
    "meta_link.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:11.333421Z",
     "iopub.status.busy": "2022-05-01T13:21:11.333027Z",
     "iopub.status.idle": "2022-05-01T13:21:11.54375Z",
     "shell.execute_reply": "2022-05-01T13:21:11.542596Z",
     "shell.execute_reply.started": "2022-05-01T13:21:11.333362Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_link['movie_description'] = meta_link['overview'].str.lower() + \" \" + meta_link['tagline'].str.lower()\n",
    "meta_link['title'] =meta_link['title'].str.lower()\n",
    "\n",
    "columns_to_drop = ['tmdbId', 'overview', 'tagline']\n",
    "\n",
    "meta_link.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:11.872527Z",
     "iopub.status.busy": "2022-05-01T13:21:11.871601Z",
     "iopub.status.idle": "2022-05-01T13:21:11.896456Z",
     "shell.execute_reply": "2022-05-01T13:21:11.895809Z",
     "shell.execute_reply.started": "2022-05-01T13:21:11.872453Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_link.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:12.051022Z",
     "iopub.status.busy": "2022-05-01T13:21:12.050488Z",
     "iopub.status.idle": "2022-05-01T13:21:12.122018Z",
     "shell.execute_reply": "2022-05-01T13:21:12.121204Z",
     "shell.execute_reply.started": "2022-05-01T13:21:12.050978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge meta_link and imdb dataframes\n",
    "\n",
    "imdb_meta = meta_link.merge(imdb_copy[imdb_columns_of_interest], how='left', on='movieId')\n",
    "\n",
    "imdb_meta.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:45:20.738912Z",
     "iopub.status.busy": "2022-05-01T13:45:20.738504Z",
     "iopub.status.idle": "2022-05-01T13:45:20.85053Z",
     "shell.execute_reply": "2022-05-01T13:45:20.849683Z",
     "shell.execute_reply.started": "2022-05-01T13:45:20.738862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge imdb_meta and movies dataframes\n",
    "\n",
    "all_movies = movies_copy.merge(imdb_meta.drop('title', axis=1), on='movieId')\n",
    "\n",
    "all_movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:45:30.620279Z",
     "iopub.status.busy": "2022-05-01T13:45:30.619904Z",
     "iopub.status.idle": "2022-05-01T13:45:30.653579Z",
     "shell.execute_reply": "2022-05-01T13:45:30.652174Z",
     "shell.execute_reply.started": "2022-05-01T13:45:30.620237Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop imdbId\n",
    "all_movies.drop('imdbId', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring in tags. \n",
    "\n",
    "We will work with Genome Scores, to get the relevant tags at a certain threshold, Genome tags to the tags and possibly Tags df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:45:38.439997Z",
     "iopub.status.busy": "2022-05-01T13:45:38.438535Z",
     "iopub.status.idle": "2022-05-01T13:45:41.227583Z",
     "shell.execute_reply": "2022-05-01T13:45:41.226632Z",
     "shell.execute_reply.started": "2022-05-01T13:45:38.439929Z"
    }
   },
   "outputs": [],
   "source": [
    "# group tag df on movie id, squeezing tag into a list\n",
    "\n",
    "tags_grouped = tags.drop('userId',axis=1).groupby(['movieId'])['tag'].apply(list).reset_index()\n",
    "\n",
    "tags_grouped.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:45:51.240236Z",
     "iopub.status.busy": "2022-05-01T13:45:51.239905Z",
     "iopub.status.idle": "2022-05-01T13:45:51.355012Z",
     "shell.execute_reply": "2022-05-01T13:45:51.354035Z",
     "shell.execute_reply.started": "2022-05-01T13:45:51.240201Z"
    }
   },
   "outputs": [],
   "source": [
    "# make tag column a string of tags separated by a space (' ')\n",
    "\n",
    "def tag_to_str(tag_list):\n",
    "    \n",
    "    try:\n",
    "        tag_str = ' '.join(tag_list)\n",
    "    except (TypeError, ValueError):\n",
    "        tag_str = str(tag_list)\n",
    "        \n",
    "    return tag_str\n",
    "\n",
    "tags_grouped['tag'] = tags_grouped['tag'].apply(tag_to_str)\n",
    "tags_grouped.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:17.55084Z",
     "iopub.status.busy": "2022-05-01T13:21:17.550191Z",
     "iopub.status.idle": "2022-05-01T13:21:17.834292Z",
     "shell.execute_reply": "2022-05-01T13:21:17.833321Z",
     "shell.execute_reply.started": "2022-05-01T13:21:17.550794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set relevance threshold for genome tags\n",
    "\n",
    "threshold = 0.80 # 80% relevance at least\n",
    "\n",
    "relevant_genomes = genome_scores[genome_scores.relevance >= threshold]\n",
    "scores_tags = genome_tags.merge(relevant_genomes, on='tagId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:19.10467Z",
     "iopub.status.busy": "2022-05-01T13:21:19.103922Z",
     "iopub.status.idle": "2022-05-01T13:21:19.114795Z",
     "shell.execute_reply": "2022-05-01T13:21:19.114067Z",
     "shell.execute_reply.started": "2022-05-01T13:21:19.104603Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_count = scores_tags.groupby('tag')['movieId'].count()\\\n",
    "                .reset_index().rename(columns={'movieId':'movieId_counts'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole process takes a long time to complete, so we have a saved copy of the output that has been loaded already. let's just bring that here. \n",
    "\n",
    "To run the code, nonetheless, just uncomment the block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:21:19.291776Z",
     "iopub.status.busy": "2022-05-01T13:21:19.290605Z",
     "iopub.status.idle": "2022-05-01T13:33:01.174043Z",
     "shell.execute_reply": "2022-05-01T13:33:01.172936Z",
     "shell.execute_reply.started": "2022-05-01T13:21:19.291726Z"
    }
   },
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz\n",
    "# from fuzzywuzzy import process\n",
    "\n",
    "# \"\"\"\n",
    "# Fuzzywuzzy is a library used for string matching. Fuzzy string matching is the process of finding strings that match a given pattern.\n",
    "# Basically, it uses Levenshtein distance to calculate the differences between sequences.\n",
    "# \"\"\"\n",
    "\n",
    "# match_list = []\n",
    "# ratio_list = []\n",
    "\n",
    "# bad_tags = scores_tags.tag.values\n",
    "# good_tags = movie_count.tag[movie_count.movieId_counts > 100].values\n",
    "\n",
    "# threshold = 80\n",
    "\n",
    "# for b_tag in bad_tags:\n",
    "#     process_extract = process.extractOne(b_tag, good_tags, scorer=fuzz.token_sort_ratio)\n",
    "#     match_list.append(process_extract[0])\n",
    "#     ratio_list.append(process_extract[1])\n",
    "\n",
    "# scores_tags['matches'] = match_list\n",
    "# scores_tags['match_ratio'] = ratio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tags = scores_n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:33:01.176474Z",
     "iopub.status.busy": "2022-05-01T13:33:01.17622Z",
     "iopub.status.idle": "2022-05-01T13:33:01.191292Z",
     "shell.execute_reply": "2022-05-01T13:33:01.19043Z",
     "shell.execute_reply.started": "2022-05-01T13:33:01.176443Z"
    }
   },
   "outputs": [],
   "source": [
    "print(scores_tags.shape)\n",
    "scores_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:45:00.961341Z",
     "iopub.status.busy": "2022-05-01T13:45:00.961012Z",
     "iopub.status.idle": "2022-05-01T13:45:11.868561Z",
     "shell.execute_reply": "2022-05-01T13:45:11.867462Z",
     "shell.execute_reply.started": "2022-05-01T13:45:00.961307Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_tags_threshold = scores_tags[scores_tags.match_ratio >= threshold]\n",
    "adjusted_tags = pd.merge(scores_tags[['movieId', 'tag']], \\\n",
    "                         filtered_tags_threshold[['tag', 'matches']], on='tag')\n",
    "cleaned_tags = adjusted_tags.groupby(['movieId', 'matches'])['tag'].count()\\\n",
    "                            .reset_index().rename(columns={'tag':'tag_count', 'matches':'tag'})\n",
    "\n",
    "cleaned_tags = cleaned_tags.groupby('movieId')['tag'].agg(list).reset_index()\n",
    "\n",
    "cleaned_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:46:17.046249Z",
     "iopub.status.busy": "2022-05-01T13:46:17.045888Z",
     "iopub.status.idle": "2022-05-01T13:46:17.071213Z",
     "shell.execute_reply": "2022-05-01T13:46:17.070289Z",
     "shell.execute_reply.started": "2022-05-01T13:46:17.04621Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_tags['tag'] = cleaned_tags['tag'].apply(tag_to_str)\n",
    "cleaned_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:46:27.287851Z",
     "iopub.status.busy": "2022-05-01T13:46:27.287462Z",
     "iopub.status.idle": "2022-05-01T13:46:27.352063Z",
     "shell.execute_reply": "2022-05-01T13:46:27.350924Z",
     "shell.execute_reply.started": "2022-05-01T13:46:27.287809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge cleaned tags and tag_grouped\n",
    "\n",
    "tags_merged = tags_grouped.merge(cleaned_tags, how='left', on='movieId')\n",
    "tags_merged = tags_merged.fillna('')\n",
    "tags_merged['tag'] = tags_merged['tag_x'] + \" \" + tags_merged['tag_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:46:33.676243Z",
     "iopub.status.busy": "2022-05-01T13:46:33.675254Z",
     "iopub.status.idle": "2022-05-01T13:46:33.698606Z",
     "shell.execute_reply": "2022-05-01T13:46:33.697844Z",
     "shell.execute_reply.started": "2022-05-01T13:46:33.67619Z"
    }
   },
   "outputs": [],
   "source": [
    "tags_merged.drop(['tag_x', 'tag_y'], axis=1, inplace=True)\n",
    "tags_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will merge the train dataset and all_movies. First, we are going to group the train dataset based on movies and get the average rating of each movie and the number of ratings the movie received. We have done this in the <a href=#eda>EDA</a> section, but we will redo it here.\n",
    "\n",
    "Finally merge with tage_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:46:45.703712Z",
     "iopub.status.busy": "2022-05-01T13:46:45.7032Z",
     "iopub.status.idle": "2022-05-01T13:46:46.115559Z",
     "shell.execute_reply": "2022-05-01T13:46:46.11441Z",
     "shell.execute_reply.started": "2022-05-01T13:46:45.703668Z"
    }
   },
   "outputs": [],
   "source": [
    "ave_ratings = train.groupby(['movieId'])\\\n",
    "                            .agg({'rating':'mean', 'userId':'count'}).reset_index()\\\n",
    "                            .rename(columns={'rating':'ave_rating', 'userId':'rating_count'})\n",
    "print(ave_ratings.shape)\n",
    "ave_ratings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:46:54.00813Z",
     "iopub.status.busy": "2022-05-01T13:46:54.007361Z",
     "iopub.status.idle": "2022-05-01T13:46:54.17344Z",
     "shell.execute_reply": "2022-05-01T13:46:54.17245Z",
     "shell.execute_reply.started": "2022-05-01T13:46:54.008066Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge ave_ratings and all_movies\n",
    "\n",
    "full_movie = ave_ratings.merge(all_movies, how='right', on='movieId')\n",
    "\n",
    "# rearrange dataset\n",
    "columns = ['movieId', 'title', 'genres', 'title_cast', 'director', 'production_companies',\n",
    "           'production_countries', 'movie_description', 'plot_keywords', 'spoken_languages', \n",
    "           'year', 'runtime', 'ave_rating', 'rating_count', 'vote_average', \n",
    "           'vote_count', 'popularity']\n",
    "full_movie = full_movie[columns]\n",
    "\n",
    "full_movie.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:47:31.161095Z",
     "iopub.status.busy": "2022-05-01T13:47:31.160289Z",
     "iopub.status.idle": "2022-05-01T13:47:31.273408Z",
     "shell.execute_reply": "2022-05-01T13:47:31.272515Z",
     "shell.execute_reply.started": "2022-05-01T13:47:31.161034Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge full movies and tags\n",
    "\n",
    "full_movies = full_movie.merge(tags_merged, how='left', on='movieId')\n",
    "full_movies['tag'] = full_movies['tag'].fillna('').str.lower()\n",
    "full_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will lemmatize and stem the **movie-descriptions**, **plot_keywords** and **tags**. First, What are Lemmatization and Stemming?\n",
    "\n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word.\n",
    "\n",
    "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words chocolates, chocolatey, choco to the root word, chocolate and retrieval, retrieved, retrieves reduce to the stem retrieve. This is very important because we want similar words grouped, to give our algorithm a better accuracy\n",
    "\n",
    "Both Lemming and Stemming both take as inputs, tokenized words.\n",
    "\n",
    "Tokenization is a process of converting a string of words into a list of separated individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:33:13.27454Z",
     "iopub.status.busy": "2022-05-01T13:33:13.274291Z",
     "iopub.status.idle": "2022-05-01T13:33:13.281322Z",
     "shell.execute_reply": "2022-05-01T13:33:13.280426Z",
     "shell.execute_reply.started": "2022-05-01T13:33:13.274508Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets define our tokenizer\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "# Lemming and stemming function\n",
    "\n",
    "def transform(text):\n",
    "    # tokenize words\n",
    "    words = tokenize(text)\n",
    "    \n",
    "    # define both Lemmatizer and stemmer\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    \n",
    "    # lemmatize and stem words\n",
    "    lemmatized = [lemmer.lemmatize(x) for x in words]\n",
    "    stemmed = [stemmer.stem(x) for x in lemmatized]\n",
    "    \n",
    "    return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:47:51.523673Z",
     "iopub.status.busy": "2022-05-01T13:47:51.522856Z",
     "iopub.status.idle": "2022-05-01T13:48:58.693371Z",
     "shell.execute_reply": "2022-05-01T13:48:58.69218Z",
     "shell.execute_reply.started": "2022-05-01T13:47:51.523605Z"
    }
   },
   "outputs": [],
   "source": [
    "full_movies[['movie_description', 'plot_keywords', 'tag']] = full_movies[['movie_description', 'plot_keywords', 'tag']].fillna('')\n",
    "full_movies['movie_description'] = full_movies['movie_description'].apply(transform)\n",
    "full_movies['plot_keywords'] = full_movies['plot_keywords'].apply(transform)\n",
    "full_movies['tag'] = full_movies['tag'].apply(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:48:58.695522Z",
     "iopub.status.busy": "2022-05-01T13:48:58.695248Z",
     "iopub.status.idle": "2022-05-01T13:48:58.820665Z",
     "shell.execute_reply": "2022-05-01T13:48:58.819726Z",
     "shell.execute_reply.started": "2022-05-01T13:48:58.695489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check number and percentage of missing data\n",
    "\n",
    "# Extract the number of missing data and the percentage\n",
    "# of missing data and concatenate into one dataframe\n",
    "full_movies_missing = pd.concat([full_movies.isnull().sum(), round(full_movies.isnull().sum()/full_movies.shape[0] * 100)], axis=1)\n",
    "full_movies_missing.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "full_movies_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:34:23.573034Z",
     "iopub.status.busy": "2022-05-01T13:34:23.57217Z",
     "iopub.status.idle": "2022-05-01T13:34:23.580031Z",
     "shell.execute_reply": "2022-05-01T13:34:23.579059Z",
     "shell.execute_reply.started": "2022-05-01T13:34:23.57299Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_na(df):\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # impute based on the type of column\n",
    "        if df[column].dtype == 'object':\n",
    "            # fill categorical columns with blanks\n",
    "            df[column] = df[column].fillna('')\n",
    "        elif df[column].dtype == 'float32' or df[column].dtype == 'float64':\n",
    "            # fill numerical columns with the mean value of the column\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:48:58.822552Z",
     "iopub.status.busy": "2022-05-01T13:48:58.82229Z",
     "iopub.status.idle": "2022-05-01T13:48:58.975356Z",
     "shell.execute_reply": "2022-05-01T13:48:58.974357Z",
     "shell.execute_reply.started": "2022-05-01T13:48:58.82252Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply fill_na function\n",
    "full_movies = fill_na(full_movies)\n",
    "\n",
    "full_movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:34:23.740841Z",
     "iopub.status.busy": "2022-05-01T13:34:23.740599Z",
     "iopub.status.idle": "2022-05-01T13:34:23.870814Z",
     "shell.execute_reply": "2022-05-01T13:34:23.8698Z",
     "shell.execute_reply.started": "2022-05-01T13:34:23.740812Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if fill_na worked.\n",
    "full_movies_missing = pd.concat([full_movies.isnull().sum(), round(full_movies.isnull().sum()/full_movies.shape[0] * 100)], axis=1)\n",
    "full_movies_missing.columns = ['missing_count', 'missing_percentage'] # rename columns\n",
    "full_movies_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill_na worked a charm. The categorical columns are filled with blanks(''), while the numerical columns are filled with the mean value of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will remove stopwords from 'movie_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:48:58.97853Z",
     "iopub.status.busy": "2022-05-01T13:48:58.977991Z",
     "iopub.status.idle": "2022-05-01T13:48:59.728128Z",
     "shell.execute_reply": "2022-05-01T13:48:59.727089Z",
     "shell.execute_reply.started": "2022-05-01T13:48:58.978479Z"
    }
   },
   "outputs": [],
   "source": [
    "full_movies['movie_description'] = full_movies['movie_description'].apply(remove_stopwords)\n",
    "full_movies['tag'] = full_movies['tag'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's downcast the columns of the dataset, to reduce the size of the dataset, using 'convert_columns' custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:34:24.603594Z",
     "iopub.status.busy": "2022-05-01T13:34:24.603319Z",
     "iopub.status.idle": "2022-05-01T13:34:25.301487Z",
     "shell.execute_reply": "2022-05-01T13:34:25.300583Z",
     "shell.execute_reply.started": "2022-05-01T13:34:24.603552Z"
    }
   },
   "outputs": [],
   "source": [
    "full_movies = convert_columns(full_movies)\n",
    "full_movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come to the end of feature engineering. Here we drop features, engineered a new one, merged dataframes and finally downcasted columns to reduce the size of the final dataframe. Now we can go ahead an create our recommender systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:48:59.729897Z",
     "iopub.status.busy": "2022-05-01T13:48:59.729572Z",
     "iopub.status.idle": "2022-05-01T13:48:59.739154Z",
     "shell.execute_reply": "2022-05-01T13:48:59.737877Z",
     "shell.execute_reply.started": "2022-05-01T13:48:59.72986Z"
    }
   },
   "outputs": [],
   "source": [
    "del cleaned_tags, tags_merged, all_movies, full_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n",
    "</div>\n",
    "<div id='recommender'>\n",
    "    <h2>RECOMMENDER SYSTEMS</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief introduction into recommender systems and the types there are was given in <a href='#introduction'>Introduction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB TOP 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive in, let's get the IMDB top 250 movies, based on a calculation developed by IMDB.\n",
    "\n",
    "$$ \n",
    "Weighted Rating(WR) = \\frac{v}{v + m} * R + \\frac{m}{v + m} * C\n",
    "$$\n",
    "\n",
    "Where;\n",
    " - v = Number of votes for the movie\n",
    " - m = minimum votes required to be listed in the Top 250 (currently 25000)\n",
    " - R = average rating for the movie (mean rating)\n",
    " - C = mean vote across the whole report (currently 7.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:49:11.512975Z",
     "iopub.status.busy": "2022-05-01T13:49:11.512575Z",
     "iopub.status.idle": "2022-05-01T13:49:11.522753Z",
     "shell.execute_reply": "2022-05-01T13:49:11.521925Z",
     "shell.execute_reply.started": "2022-05-01T13:49:11.51293Z"
    }
   },
   "outputs": [],
   "source": [
    "full_movies['imdb_wr'] = ((full_movies['vote_count']/(full_movies['vote_count']+25000)) * full_movies['ave_rating'] +\\\n",
    "                        (25000/(full_movies['vote_count']+25000)) * 7.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:49:15.623788Z",
     "iopub.status.busy": "2022-05-01T13:49:15.622835Z",
     "iopub.status.idle": "2022-05-01T13:49:15.668096Z",
     "shell.execute_reply": "2022-05-01T13:49:15.666868Z",
     "shell.execute_reply.started": "2022-05-01T13:49:15.623734Z"
    }
   },
   "outputs": [],
   "source": [
    "top_250_movies = full_movies.sort_values(by='imdb_wr').head(250)\n",
    "top_250_movies = top_250_movies.title.tolist()\n",
    "print(top_250_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HIGHEST RATED 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:35:36.571708Z",
     "iopub.status.busy": "2022-05-01T13:35:36.571323Z",
     "iopub.status.idle": "2022-05-01T13:35:36.591377Z",
     "shell.execute_reply": "2022-05-01T13:35:36.59076Z",
     "shell.execute_reply.started": "2022-05-01T13:35:36.571662Z"
    }
   },
   "outputs": [],
   "source": [
    "top_rated_100 = full_movies.sort_values(by='ave_rating', ascending=False).head(100)\n",
    "top_rated_100 = top_rated_100['title'].tolist()\n",
    "print(top_rated_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MOST POPULAR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:35:37.321526Z",
     "iopub.status.busy": "2022-05-01T13:35:37.320984Z",
     "iopub.status.idle": "2022-05-01T13:35:37.339316Z",
     "shell.execute_reply": "2022-05-01T13:35:37.338593Z",
     "shell.execute_reply.started": "2022-05-01T13:35:37.321468Z"
    }
   },
   "outputs": [],
   "source": [
    "most_popular_100 = full_movies.sort_values(by='popularity', ascending=False).head(100)\n",
    "most_popular_100 = most_popular_100['title'].tolist()\n",
    "print(most_popular_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CONTENT BASED RECOMMENDER SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:42:40.273832Z",
     "iopub.status.busy": "2022-05-01T13:42:40.273306Z",
     "iopub.status.idle": "2022-05-01T13:42:40.280974Z",
     "shell.execute_reply": "2022-05-01T13:42:40.279594Z",
     "shell.execute_reply.started": "2022-05-01T13:42:40.273782Z"
    }
   },
   "outputs": [],
   "source": [
    "# a handle function to create cosine similarities\n",
    "\n",
    "def get_similarity(series):\n",
    "    tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 1))\n",
    "    tfidf_matrix = tf.fit_transform(series)\n",
    "\n",
    "    tfidf_matrix.shape\n",
    "    tfidf_matrix = tfidf_matrix.astype('float32')\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:42:46.418944Z",
     "iopub.status.busy": "2022-05-01T13:42:46.417817Z",
     "iopub.status.idle": "2022-05-01T13:42:46.427271Z",
     "shell.execute_reply": "2022-05-01T13:42:46.426451Z",
     "shell.execute_reply.started": "2022-05-01T13:42:46.418896Z"
    }
   },
   "outputs": [],
   "source": [
    "def content_model(category):\n",
    "    cosine_sim = get_similarity(category)\n",
    "    return cosine_sim\n",
    "\n",
    "def get_recommendations(title, category, n=10):\n",
    "    \n",
    "    cosine_sim = content_model(category)\n",
    "    movies = full_movies[['movieId', 'title']]\n",
    "    movies  = movies.reset_index()\n",
    "    titles = movies['title']\n",
    "    indices = pd.Series(movies.index, index=movies['title'])\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:31]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return titles.iloc[movie_indices].astype('object').tolist()[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the cast and crew of the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:49:59.566496Z",
     "iopub.status.busy": "2022-05-01T13:49:59.566096Z",
     "iopub.status.idle": "2022-05-01T13:49:59.617164Z",
     "shell.execute_reply": "2022-05-01T13:49:59.615954Z",
     "shell.execute_reply.started": "2022-05-01T13:49:59.566455Z"
    }
   },
   "outputs": [],
   "source": [
    "cast_n_crew = full_movies['title_cast'].astype('O') + \" \" + full_movies['director'].astype('O').apply(lambda x: ' '.join([x, x, x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:50:03.217416Z",
     "iopub.status.busy": "2022-05-01T13:50:03.217076Z",
     "iopub.status.idle": "2022-05-01T13:50:05.898088Z",
     "shell.execute_reply": "2022-05-01T13:50:05.897037Z",
     "shell.execute_reply.started": "2022-05-01T13:50:03.217381Z"
    }
   },
   "outputs": [],
   "source": [
    "get_recommendations('dark knight, the', cast_n_crew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on plot keywords and movie description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:54:52.316002Z",
     "iopub.status.busy": "2022-05-01T13:54:52.315598Z",
     "iopub.status.idle": "2022-05-01T13:54:52.335738Z",
     "shell.execute_reply": "2022-05-01T13:54:52.334994Z",
     "shell.execute_reply.started": "2022-05-01T13:54:52.315957Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords = full_movies['plot_keywords'].astype('object') + \" \" + full_movies['movie_description'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('dark knight, the', keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:52:07.393015Z",
     "iopub.status.busy": "2022-05-01T13:52:07.39241Z",
     "iopub.status.idle": "2022-05-01T13:52:07.398812Z",
     "shell.execute_reply": "2022-05-01T13:52:07.397858Z",
     "shell.execute_reply.started": "2022-05-01T13:52:07.392973Z"
    }
   },
   "outputs": [],
   "source": [
    "tags = full_movies['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:52:29.073483Z",
     "iopub.status.busy": "2022-05-01T13:52:29.072436Z",
     "iopub.status.idle": "2022-05-01T13:52:46.405456Z",
     "shell.execute_reply": "2022-05-01T13:52:46.404333Z",
     "shell.execute_reply.started": "2022-05-01T13:52:29.073422Z"
    }
   },
   "outputs": [],
   "source": [
    "get_recommendations('dark knight, the', tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on production companies and countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:53:12.430071Z",
     "iopub.status.busy": "2022-05-01T13:53:12.42969Z",
     "iopub.status.idle": "2022-05-01T13:53:12.455681Z",
     "shell.execute_reply": "2022-05-01T13:53:12.454746Z",
     "shell.execute_reply.started": "2022-05-01T13:53:12.430033Z"
    }
   },
   "outputs": [],
   "source": [
    "production = full_movies['production_companies'].astype('object') + \" \" + full_movies['production_countries'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T13:53:45.565158Z",
     "iopub.status.busy": "2022-05-01T13:53:45.564766Z",
     "iopub.status.idle": "2022-05-01T13:53:58.454996Z",
     "shell.execute_reply": "2022-05-01T13:53:58.453878Z",
     "shell.execute_reply.started": "2022-05-01T13:53:45.565111Z"
    }
   },
   "outputs": [],
   "source": [
    "get_recommendations('dark knight, the', production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T17:56:37.417812Z",
     "iopub.status.busy": "2022-04-30T17:56:37.417508Z",
     "iopub.status.idle": "2022-04-30T17:56:37.423511Z",
     "shell.execute_reply": "2022-04-30T17:56:37.422711Z",
     "shell.execute_reply.started": "2022-04-30T17:56:37.417781Z"
    },
    "id": "iE-qn_PPDwCQ"
   },
   "outputs": [],
   "source": [
    "def split_text(text, sep):\n",
    "    split_text = text.split(sep)\n",
    "    return split_text\n",
    "\n",
    "def join_lists(list_):\n",
    "    return ' '.join(list_)\n",
    "\n",
    "def stringify(num):\n",
    "    num = str(num)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T17:53:45.04567Z",
     "iopub.status.busy": "2022-04-30T17:53:45.044799Z",
     "iopub.status.idle": "2022-04-30T17:53:45.049135Z",
     "shell.execute_reply": "2022-04-30T17:53:45.048522Z",
     "shell.execute_reply.started": "2022-04-30T17:53:45.045617Z"
    }
   },
   "outputs": [],
   "source": [
    "reader = Reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:10:41.363493Z",
     "iopub.status.busy": "2022-04-30T18:10:41.363183Z",
     "iopub.status.idle": "2022-04-30T18:10:41.757489Z",
     "shell.execute_reply": "2022-04-30T18:10:41.75662Z",
     "shell.execute_reply.started": "2022-04-30T18:10:41.363462Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "\n",
    "filtered_train = train[train['rating'] >= threshold]\n",
    "filtered_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:11:11.559807Z",
     "iopub.status.busy": "2022-04-30T18:11:11.559524Z",
     "iopub.status.idle": "2022-04-30T18:11:19.158664Z",
     "shell.execute_reply": "2022-04-30T18:11:19.157916Z",
     "shell.execute_reply.started": "2022-04-30T18:11:11.559776Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_train['userId'] = filtered_train['userId'].apply(stringify)\n",
    "filtered_train['movieId'] = filtered_train['movieId'].apply(stringify)\n",
    "# train['Id'] = train['userId'] + \"_\" + train['movieId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:11:40.638199Z",
     "iopub.status.busy": "2022-04-30T18:11:40.637405Z",
     "iopub.status.idle": "2022-04-30T18:11:51.199054Z",
     "shell.execute_reply": "2022-04-30T18:11:51.198424Z",
     "shell.execute_reply.started": "2022-04-30T18:11:40.638146Z"
    }
   },
   "outputs": [],
   "source": [
    "data = Dataset.load_from_df(filtered_train[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:13:25.516233Z",
     "iopub.status.busy": "2022-04-30T18:13:25.515704Z",
     "iopub.status.idle": "2022-04-30T18:13:25.520286Z",
     "shell.execute_reply": "2022-04-30T18:13:25.519574Z",
     "shell.execute_reply.started": "2022-04-30T18:13:25.516181Z"
    }
   },
   "outputs": [],
   "source": [
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': False  # compute  similarities between items\n",
    "               }\n",
    "# knn =KNNBasic(sim_options=sim_options)\n",
    "svd = NMF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:15:40.914446Z",
     "iopub.status.busy": "2022-04-30T18:15:40.914108Z",
     "iopub.status.idle": "2022-04-30T18:16:13.398651Z",
     "shell.execute_reply": "2022-04-30T18:16:13.397651Z",
     "shell.execute_reply.started": "2022-04-30T18:15:40.914406Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:16:13.400941Z",
     "iopub.status.busy": "2022-04-30T18:16:13.400615Z",
     "iopub.status.idle": "2022-04-30T18:26:44.12946Z",
     "shell.execute_reply": "2022-04-30T18:26:44.128371Z",
     "shell.execute_reply.started": "2022-04-30T18:16:13.400888Z"
    }
   },
   "outputs": [],
   "source": [
    "svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:26:44.131915Z",
     "iopub.status.busy": "2022-04-30T18:26:44.131561Z",
     "iopub.status.idle": "2022-04-30T18:27:20.388983Z",
     "shell.execute_reply": "2022-04-30T18:27:20.38827Z",
     "shell.execute_reply.started": "2022-04-30T18:26:44.131869Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = svd.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:27:20.39053Z",
     "iopub.status.busy": "2022-04-30T18:27:20.390214Z",
     "iopub.status.idle": "2022-04-30T18:27:23.046245Z",
     "shell.execute_reply": "2022-04-30T18:27:23.045361Z",
     "shell.execute_reply.started": "2022-04-30T18:27:20.390481Z"
    }
   },
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T18:10:05.4068Z",
     "iopub.status.busy": "2022-04-30T18:10:05.406477Z",
     "iopub.status.idle": "2022-04-30T18:10:09.234196Z",
     "shell.execute_reply": "2022-04-30T18:10:09.233416Z",
     "shell.execute_reply.started": "2022-04-30T18:10:05.406762Z"
    }
   },
   "outputs": [],
   "source": [
    "test['userId'] = test['userId'].apply(stringify)\n",
    "test['movieId'] = test['movieId'].apply(stringify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import dump\n",
    "import os\n",
    "\n",
    "model_filename = \"/kaggle/working/model.pickle\"\n",
    "\n",
    "print(\">>> starting dump\")\n",
    "file_name = os.path.expanduser(model_filename)\n",
    "dump.dump(file_name, algo=svd)\n",
    "print('dump done')\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_rating(user, item):\n",
    "    uid = str(user)\n",
    "    iid = str(item)\n",
    "    model = svd\n",
    "    prediction = model.predict(user, item, verbose=False)\n",
    "    rating = round(prediction.est, 1)\n",
    "    \n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ratings'] = test.apply(lambda row: item_rating(row['userId'], row['movieId']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
