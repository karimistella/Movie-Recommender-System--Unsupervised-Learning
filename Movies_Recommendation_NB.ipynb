{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align:center; font-size:45px; color: teal; letter-spacing: .1em;\">\n    MOVIES RECOMMENDATION SYSTEMS\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id='tbl-contents'>\n    <h2>TABLE OF CONTENTS</h2>\n    <ol>\n        <li>\n            <h4>\n                <a href='#introduction'>Introduction</a>\n            </h4>\n        </li>\n        <li>\n            <h4>\n                <a href='#load-dependencies'>Load Dependencies</a>\n            </h4>\n        </li>\n        <li>\n            <h4>\n                <a href='#load-data'>Load Data</a>\n            </h4>\n        </li> \n        <li>\n            <h4>\n                <a href='#sneak-peek'>Sneak Peek into Loaded Data</a>\n            </h4>\n        </li> \n        <li>\n            <h4>\n                <a href='#data-cleaning'>Data Cleaning</a>\n            </h4>\n        </li>  \n    </ol>\n</div>\n\n","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='introduction'>\n    <h2> INTRODUCTION </h2>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"Intelligent algorithms can help viewers sift through tens of thousands of titles to find the best ones. Recommender systems are both socially and economically important in ensuring that people can make informed decisions about the content they consume on a daily basis. This is particularly true in the case of movie recommendations.\n\nProviding an accurate and robust solution to this challenge has enormous economic potential, with users of the system receiving personalized recommendations, thereby enhancing platform affinity for the streaming services that best facilitate their audience's viewing.","metadata":{}},{"cell_type":"markdown","source":"### RECOMMENDER SYSTEM","metadata":{}},{"cell_type":"markdown","source":"A recommendation system is an information filtering system whose main goal is to predict the rating or preference a user might give to an item. This helps create personalized content and better product search experience. One popular use is recommending to users which movie to watch. This is because significant dependencies exist between users and item centric activity. For example a user who is interested in s historical documentary is more likely to be interested in another historical documentary or an educational program, rather than in an action movie.\n\nA recommendation system can use either of these two techniques:\n\n- Content based filtering\n- Collaborative filtering\n\nIn content based filtering, the algorithm seeks to make recommendations based on how similar the properties or features of an item are to other items.\n\nIn collaborative filtering, we use similarities between users and items simultaneously to provide recommendations. This allows for serendipitous recommendations; that is, collaborative filtering models can recommend an item to user A based on the interests of a similar user B.\n\nHere we are going to explore both methods and assess which recommendation system gives us the best results. Increasing sales is the primary goal of a recommender system. By recommending carefully selected items to users, recommender systems bring relevant items to the attention of users. This increases the sales volumes and profits to the merchants.","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='load-dependencies'>\n    <h2>LOAD DEPENDENCIES</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"import os # for os operations on kaggle\n\n# for pattern searching and extraction \nimport re\n\n# libraries for data analysis and manipulation\nimport pandas as pd\nimport numpy as np\n\n# libraries for numerical efficiencies\nimport scipy as sp\nfrom scipy import stats\n\n# libraries for data visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import figure\n\n# libraries used during sorting procedures.\nimport operator\nimport heapq\n\n# library to evaluate strings containing python literals\nfrom ast import literal_eval\n\n# libraries for natural language processing\nfrom nltk.corpus import wordnet\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# libraries for building and analyzing recommender systems that deal with explicit rating data.\nfrom surprise import Reader, Dataset, SVD\nfrom surprise import KNNBasic, BaselineOnly, SVDpp\n\n# libraries for entity featurization and similarity computation\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom surprise.model_selection import cross_validate, train_test_split\n\n# to ignore whatever warnings that may arise\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (15, 10) # defaulting all plots to a fixed size\nplt.style.use('ggplot')\nsns.set_palette(sns.dark_palette(\"#69d\"))\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"id":"jJoKzyWPDpVd","execution":{"iopub.status.busy":"2022-04-27T19:52:51.395484Z","iopub.execute_input":"2022-04-27T19:52:51.396142Z","iopub.status.idle":"2022-04-27T19:52:52.231860Z","shell.execute_reply.started":"2022-04-27T19:52:51.395997Z","shell.execute_reply":"2022-04-27T19:52:52.230947Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='load-dependencies'>\n    <h2>LOAD DATASETS</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"def convert_columns(data):\n    \"\"\"\n    This function takes in a dataset and converts the \n    dtype of each column to a lesser version to reduce\n    the size of the dataset for further operations.\n    \"\"\"\n    \n    for col in data.columns: # iterate over the columns in the dataset\n        \n        if data[col].dtype == 'object':\n            data[col] = data[col].astype('category') # convert objects to categories\n        \n        if data[col].dtype == 'int64':\n            data[col] = data[col].astype('int32') # convert int64 to int32\n        \n        if data[col].dtype == 'float64':\n            data[col] = data[col].astype('float32') # convert float64 to float32\n        \n    return data # return converted data\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:52:52.233622Z","iopub.execute_input":"2022-04-27T19:52:52.233888Z","iopub.status.idle":"2022-04-27T19:52:52.242342Z","shell.execute_reply.started":"2022-04-27T19:52:52.233857Z","shell.execute_reply":"2022-04-27T19:52:52.241413Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"imdb = pd.read_csv('/kaggle/input/movie-recommendation-data/imdb_data.csv')\nmovies = pd.read_csv('/kaggle/input/movie-recommendation-data/movies.csv')\nmeta_data = pd.read_csv('/kaggle/input/movies-metadata/movies_metadata.csv')\ngenome_scores = pd.read_csv('/kaggle/input/movie-recommendation-data/genome_scores.csv')\ngenome_scores = convert_columns(genome_scores)\ngenome_tags = pd.read_csv('/kaggle/input/movie-recommendation-data/genome_tags.csv')\ngenome_tags= convert_columns(genome_tags)\ntrain = pd.read_csv('/kaggle/input/movie-recommendation-data/train.csv')\ntrain = convert_columns(train)\ntest = pd.read_csv('/kaggle/input/movie-recommendation-data/test.csv')\ntest = convert_columns(test)\nlinks = pd.read_csv('/kaggle/input/movie-recommendation-data/links.csv')\nlinks = convert_columns(links)\ntags = pd.read_csv('/kaggle/input/movie-recommendation-data/tags.csv')\ntags = convert_columns(tags)\n\n# movies = convert_columns(movies)\nsample_submission = pd.read_csv('/kaggle/input/movie-recommendation-data/sample_submission.csv')","metadata":{"id":"AX_ZbyJ6DyQG","execution":{"iopub.status.busy":"2022-04-27T19:52:52.375451Z","iopub.execute_input":"2022-04-27T19:52:52.375775Z","iopub.status.idle":"2022-04-27T19:53:10.839554Z","shell.execute_reply.started":"2022-04-27T19:52:52.375736Z","shell.execute_reply":"2022-04-27T19:53:10.838683Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='sneak-peek'>\n    <h2 style='text-transform: uppercase;'>Sneak Peak into Loaded Data</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### 1. IMDB DATA","metadata":{}},{"cell_type":"code","source":"# imdb data\nimdb.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:10.841358Z","iopub.execute_input":"2022-04-27T19:53:10.842358Z","iopub.status.idle":"2022-04-27T19:53:10.859260Z","shell.execute_reply.started":"2022-04-27T19:53:10.842306Z","shell.execute_reply":"2022-04-27T19:53:10.858666Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"From the above output, we see that the IMDB Data is made up of __6__ columns - __movieId__, __title_cast__, __director__, __runtime__, __budget__, __plot_keywords__.\n\nWe can observe that the __title_cast__ and __plot_keywords__ columns are separated by a pipe - '|'. This makes each row in these columns seem to be one(1) long complicated word, which will make further analysis difficult. We will treat this problem in the `Data Cleaning` section.\n\nAlso, data in the __budget__ column, which is meant to be a numerical column, are prepended with currency symbols and separated by commas(,). This is bad format and needs to be taken care of in the `Data Cleaning` section.","metadata":{}},{"cell_type":"code","source":"# get the dimensions of the data\nimdb.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:10.860272Z","iopub.execute_input":"2022-04-27T19:53:10.861026Z","iopub.status.idle":"2022-04-27T19:53:10.867576Z","shell.execute_reply.started":"2022-04-27T19:53:10.860963Z","shell.execute_reply":"2022-04-27T19:53:10.866782Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"IMDB data is made up of __27,278__ rows and __6__ columns.\n\nHow about some information about the data? ","metadata":{}},{"cell_type":"code","source":"imdb.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:10.869913Z","iopub.execute_input":"2022-04-27T19:53:10.870573Z","iopub.status.idle":"2022-04-27T19:53:10.898839Z","shell.execute_reply.started":"2022-04-27T19:53:10.870525Z","shell.execute_reply":"2022-04-27T19:53:10.897847Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"The IMDB data has __1__ column of dtype `float64` - __runtime__, __1__ column of dtype `int64` - __movieId__ and __4__ columns of dtype `object` - __title_cast, director, budget & plot_keywords__. \n\nThe __budget__ column is meant to be numerical to aid aggregation. This will be taken care of in the `Data Cleaning` section.\n\nWe also have a case of missing data in all columns bar __movieId__. Let's see by how much.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\nimdb_missing_data = pd.concat([imdb.isnull().sum(), round(imdb.isnull().sum()/imdb.shape[0] * 100)], axis=1)\nimdb_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\nimdb_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:10.900290Z","iopub.execute_input":"2022-04-27T19:53:10.900840Z","iopub.status.idle":"2022-04-27T19:53:10.937458Z","shell.execute_reply.started":"2022-04-27T19:53:10.900803Z","shell.execute_reply":"2022-04-27T19:53:10.936636Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"`Budget` has the highest missing data with __19,372__! rows of missing data, making up __71%__! of the entire column - that is a huge amount!.\n\n`Runtime` has the second highest missing values at __44%__, followed closely by `plot_keywords` at __41%__. `title_cast` and `director` also record missing values at __37%__ and __36%__ respectively.\n\nThese look like a lot of missing data and have to come up with creative ways to deal with this problem in `Feature Engineering` section.","metadata":{}},{"cell_type":"markdown","source":"Next, Movies data","metadata":{}},{"cell_type":"markdown","source":"#### 2. MOVIES DATA","metadata":{}},{"cell_type":"code","source":"# movies data\nmovies.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:10.938685Z","iopub.execute_input":"2022-04-27T19:53:10.938951Z","iopub.status.idle":"2022-04-27T19:53:10.948908Z","shell.execute_reply.started":"2022-04-27T19:53:10.938921Z","shell.execute_reply":"2022-04-27T19:53:10.948089Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that the Movies data is made up of __3__ columns - __movieId__, __title__, __genres__.\n\nSimilar to IMDB's __title_cast__ and __plot_keywords__, data in the __genres__ column are separated distinctly by a '|' symbol. As stated earlier, this will need to be taken care of in the `Data Cleaning` section.\n\nThe __title__ column holds both the _title_ of the movie and the _year of release_, like co-joined twins they need to be separated in the theatre of `Feature Engineering`.\n\n\nNext, we will look at the dimensions of the data using `.shape` attribute of a Dataframe","metadata":{}},{"cell_type":"code","source":"# get the dimensions of the data\nmovies.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:10.950360Z","iopub.execute_input":"2022-04-27T19:53:10.951036Z","iopub.status.idle":"2022-04-27T19:53:10.961443Z","shell.execute_reply.started":"2022-04-27T19:53:10.951003Z","shell.execute_reply":"2022-04-27T19:53:10.960512Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The movies dataset has __62,423__ rows of data and __3__ columns of features.\n\nFine, let's drill down a bit on the data by columns to gain a slightly better understanding using `.info` ","metadata":{}},{"cell_type":"code","source":"# get more information about the data\nmovies.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:10.962663Z","iopub.execute_input":"2022-04-27T19:53:10.963262Z","iopub.status.idle":"2022-04-27T19:53:10.994433Z","shell.execute_reply.started":"2022-04-27T19:53:10.963227Z","shell.execute_reply":"2022-04-27T19:53:10.993404Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"There are __2__ objects columns - __title__ and __genres__ and __1__ numerical(_int64_) column.\n\nWe can safely say there are no missing values in any of the columns, judging from the shape of the dataset and the number of _non-null count_ for each column.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\nmovies_missing_data = pd.concat([movies.isnull().sum(), round(movies.isnull().sum()/movies.shape[0] * 100)], axis=1)\nmovies_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\nmovies_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:10.995893Z","iopub.execute_input":"2022-04-27T19:53:10.996482Z","iopub.status.idle":"2022-04-27T19:53:11.040452Z","shell.execute_reply.started":"2022-04-27T19:53:10.996432Z","shell.execute_reply":"2022-04-27T19:53:11.039633Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Our intuition was right afterall...\n\nNext on the list, Meta_data!","metadata":{}},{"cell_type":"markdown","source":"#### 3. META_DATA DATA","metadata":{}},{"cell_type":"code","source":"meta_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.043234Z","iopub.execute_input":"2022-04-27T19:53:11.043482Z","iopub.status.idle":"2022-04-27T19:53:11.071290Z","shell.execute_reply.started":"2022-04-27T19:53:11.043450Z","shell.execute_reply":"2022-04-27T19:53:11.070403Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Wow! the meta_data dataset seem to be a more robust, upgraded and an agglomerated version of the IMDB dataset and Movies dataset with a lot more information about a movie. This will be very useful for our recommendation systems.\n\nA drawback of note is that the meta_data does not have a \"movieId\", while we may use the \"id\" column instead, it doesn't map correctly with \"movieId\" of other datasets. \n\nWhat to do? let's keep that pending while we continue exploring the dataset.\n\nLet's look at the dimensions of the data next.","metadata":{}},{"cell_type":"code","source":"# get the dimension of the data\nmeta_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.072434Z","iopub.execute_input":"2022-04-27T19:53:11.072672Z","iopub.status.idle":"2022-04-27T19:53:11.077989Z","shell.execute_reply.started":"2022-04-27T19:53:11.072642Z","shell.execute_reply":"2022-04-27T19:53:11.077434Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"This dataset has __45,466__ rows and __24__ columns.\n\nMore information please! `.info`","metadata":{}},{"cell_type":"code","source":"# get more information about the dataset\nmeta_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.079263Z","iopub.execute_input":"2022-04-27T19:53:11.079458Z","iopub.status.idle":"2022-04-27T19:53:11.193086Z","shell.execute_reply.started":"2022-04-27T19:53:11.079434Z","shell.execute_reply":"2022-04-27T19:53:11.191701Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We have a lot more columns than we do for both `movies` and `IMDB` datasets combined. \n\nQuestion is, are they all useful for what we are trying to achieve? \n\nThere are columns with missing values. It is difficult to know by how much, so let's break it down.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\nmeta_data_missing_data = pd.concat([meta_data.isnull().sum(), round(meta_data.isnull().sum()/meta_data.shape[0] * 100)], axis=1)\nmeta_data_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\nmeta_data_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.194230Z","iopub.execute_input":"2022-04-27T19:53:11.194506Z","iopub.status.idle":"2022-04-27T19:53:11.398362Z","shell.execute_reply.started":"2022-04-27T19:53:11.194475Z","shell.execute_reply":"2022-04-27T19:53:11.397437Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"An astonishing __90%__! of data are missing in the __belongs_to_collection__ column, but that can be very misleading because not all the movies are part of a franchise or collection, meaning they don't have sequels. Also, __budget__ column appears to not have any missing value but from the initial sneak peek, we can see that there are movies with **Zero (0)** budget. This is practically not possible and need to be dealt with.\n\n__homepage__ on the other hand, which has __83%__ of its data missing is not useful to us in the particular context of a recommender system. Therefore, it will be removed during `Feature engineering`\n\n__tagline__, while have approximately half of its data missing may be of value us and cannot be discarded so easily.\n\nHonorable mentions in the missing data category include; __overview__ - __2%__, __poster_path__ - __1%__ and __runtime__ - __1%__\n\nUp Next, Genome_scores.","metadata":{}},{"cell_type":"markdown","source":"#### 4. GENOME SCORES DATA","metadata":{}},{"cell_type":"code","source":"genome_scores.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.399636Z","iopub.execute_input":"2022-04-27T19:53:11.399990Z","iopub.status.idle":"2022-04-27T19:53:11.410632Z","shell.execute_reply.started":"2022-04-27T19:53:11.399957Z","shell.execute_reply":"2022-04-27T19:53:11.409790Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"This dataset is made up of __3__ columns, namely; __movieId__, __tagId__ and __relevance__.\n\nRight now, we can only assume that __relevance__ indicates by how much a tag is of importance to a movie.\n\nLet's look at the shape of the dataset","metadata":{}},{"cell_type":"code","source":"genome_scores.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.411840Z","iopub.execute_input":"2022-04-27T19:53:11.412069Z","iopub.status.idle":"2022-04-27T19:53:11.424392Z","shell.execute_reply.started":"2022-04-27T19:53:11.412041Z","shell.execute_reply":"2022-04-27T19:53:11.423420Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"There are over __15 Million__ rows of data.","metadata":{}},{"cell_type":"code","source":"genome_scores.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.426074Z","iopub.execute_input":"2022-04-27T19:53:11.426617Z","iopub.status.idle":"2022-04-27T19:53:11.439928Z","shell.execute_reply.started":"2022-04-27T19:53:11.426517Z","shell.execute_reply":"2022-04-27T19:53:11.438904Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"There's no information on the number of non-null rows.\n\nTher are __2__ *int32* columns and __1__ *float32* column, indicating it's an all-numeric dataset\n\nLet's see if there are any missing data","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\ngs_missing_data = pd.concat([genome_scores.isnull().sum(), round(genome_scores.isnull().sum()/genome_scores.shape[0] * 100)], axis=1)\ngs_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\ngs_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.441768Z","iopub.execute_input":"2022-04-27T19:53:11.442084Z","iopub.status.idle":"2022-04-27T19:53:11.590520Z","shell.execute_reply.started":"2022-04-27T19:53:11.442041Z","shell.execute_reply":"2022-04-27T19:53:11.589636Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"There are no missing data.\n\nNext, we look at Genome Tags.","metadata":{}},{"cell_type":"markdown","source":"#### 5. GENOME TAGS DATA ","metadata":{}},{"cell_type":"code","source":"genome_tags.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.591885Z","iopub.execute_input":"2022-04-27T19:53:11.592694Z","iopub.status.idle":"2022-04-27T19:53:11.603646Z","shell.execute_reply.started":"2022-04-27T19:53:11.592650Z","shell.execute_reply":"2022-04-27T19:53:11.602784Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"This dataset has __2__ columns; __tagId__ and __tag__. \n\nMovie tags are another way to relate movies to each other.\n\nNext, The dimensions of the dataset","metadata":{}},{"cell_type":"code","source":"genome_tags.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.604995Z","iopub.execute_input":"2022-04-27T19:53:11.605757Z","iopub.status.idle":"2022-04-27T19:53:11.612260Z","shell.execute_reply.started":"2022-04-27T19:53:11.605673Z","shell.execute_reply":"2022-04-27T19:53:11.611151Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"There are __1,128__ rows of data.","metadata":{}},{"cell_type":"code","source":"genome_tags.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.613763Z","iopub.execute_input":"2022-04-27T19:53:11.614261Z","iopub.status.idle":"2022-04-27T19:53:11.630548Z","shell.execute_reply.started":"2022-04-27T19:53:11.614219Z","shell.execute_reply":"2022-04-27T19:53:11.629593Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The dataset consists of __1__ categorical or text data column, much like 'object' and __1__ numerical column ('int32')\n\nAnd there are no missing values, but let's double check.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\ngt_missing_data = pd.concat([genome_tags.isnull().sum(), round(genome_tags.isnull().sum()/genome_tags.shape[0] * 100)], axis=1)\ngt_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\ngt_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.632104Z","iopub.execute_input":"2022-04-27T19:53:11.632409Z","iopub.status.idle":"2022-04-27T19:53:11.646215Z","shell.execute_reply.started":"2022-04-27T19:53:11.632374Z","shell.execute_reply":"2022-04-27T19:53:11.645674Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Our suspicion was correct afterall. Let's trust our gut feelings next time. :-)\n\nUp next, we will be sneak peaking into the train dataset. stay tuned!","metadata":{}},{"cell_type":"markdown","source":"#### 6. TRAIN DATA","metadata":{}},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.647075Z","iopub.execute_input":"2022-04-27T19:53:11.647815Z","iopub.status.idle":"2022-04-27T19:53:11.666301Z","shell.execute_reply.started":"2022-04-27T19:53:11.647763Z","shell.execute_reply":"2022-04-27T19:53:11.665501Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"At a glance, we see that the train dataset is made up of __4__ columns; __userId__, __movieId__, __rating__ and __timestamp__.\n\nHere we have the rating each user gives a movie and also a timestamp of when such rating occured.\n\nWe will look at the shape of the data next","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.667443Z","iopub.execute_input":"2022-04-27T19:53:11.667859Z","iopub.status.idle":"2022-04-27T19:53:11.678074Z","shell.execute_reply.started":"2022-04-27T19:53:11.667815Z","shell.execute_reply":"2022-04-27T19:53:11.677228Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"The dataset is made up of about __10 Million__ rows of data. Pretty large.\n\nLet's extract more information.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.679350Z","iopub.execute_input":"2022-04-27T19:53:11.680136Z","iopub.status.idle":"2022-04-27T19:53:11.695283Z","shell.execute_reply.started":"2022-04-27T19:53:11.680096Z","shell.execute_reply":"2022-04-27T19:53:11.694406Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"There are __4__ columns, all of which are numerical, consisting of __3__ columns of dtype *int32* and __1__ column of dtype *float32*. I am tempted to say there are no missing values and trust my gut feelings, but just to double check again...","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\ntrain_missing_data = pd.concat([train.isnull().sum(), round(train.isnull().sum()/train.shape[0] * 100)], axis=1)\ntrain_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\ntrain_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.696617Z","iopub.execute_input":"2022-04-27T19:53:11.697270Z","iopub.status.idle":"2022-04-27T19:53:11.825625Z","shell.execute_reply.started":"2022-04-27T19:53:11.697224Z","shell.execute_reply":"2022-04-27T19:53:11.824756Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Sorry gut feelings!","metadata":{}},{"cell_type":"markdown","source":"#### 7. LINKS DATA","metadata":{}},{"cell_type":"code","source":"links.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:22:46.718124Z","iopub.execute_input":"2022-04-27T20:22:46.719115Z","iopub.status.idle":"2022-04-27T20:22:46.730084Z","shell.execute_reply.started":"2022-04-27T20:22:46.719073Z","shell.execute_reply":"2022-04-27T20:22:46.729094Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"Links, as the name suggests, contains only primary keys (columns with unique identity for each data point)  to other datasets. \n\nIt is made up of **3** columns; __movieId__, __imdbId__ & __tmdbId__","metadata":{}},{"cell_type":"code","source":"links.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:25:13.503333Z","iopub.execute_input":"2022-04-27T20:25:13.503682Z","iopub.status.idle":"2022-04-27T20:25:13.510735Z","shell.execute_reply.started":"2022-04-27T20:25:13.503649Z","shell.execute_reply":"2022-04-27T20:25:13.510088Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"There are __62,423__ rows in the dataset","metadata":{}},{"cell_type":"code","source":"links.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:26:01.450293Z","iopub.execute_input":"2022-04-27T20:26:01.450631Z","iopub.status.idle":"2022-04-27T20:26:01.464780Z","shell.execute_reply.started":"2022-04-27T20:26:01.450586Z","shell.execute_reply":"2022-04-27T20:26:01.463750Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"All columns are numeric columns; __2__ _int32_ and __1__ _float32_ column(s) respectively. \n\n__tmdbId__ seems to have missing values, let's check by how much","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\nlinks_missing_data = pd.concat([links.isnull().sum(), round(links.isnull().sum()/links.shape[0] * 100, 3)], axis=1)\nlinks_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\nlinks_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:28:45.486970Z","iopub.execute_input":"2022-04-27T20:28:45.487873Z","iopub.status.idle":"2022-04-27T20:28:45.504479Z","shell.execute_reply.started":"2022-04-27T20:28:45.487823Z","shell.execute_reply":"2022-04-27T20:28:45.503569Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"The number of missing data here is very negligible at __0.171%__, which I believe won't create significant problems","metadata":{}},{"cell_type":"markdown","source":"For our final show in this section, we will take a sneak peek into the tags dataset","metadata":{}},{"cell_type":"markdown","source":"#### 8. TAGS DATA","metadata":{}},{"cell_type":"code","source":"tags.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.827094Z","iopub.execute_input":"2022-04-27T19:53:11.827597Z","iopub.status.idle":"2022-04-27T19:53:11.841168Z","shell.execute_reply.started":"2022-04-27T19:53:11.827553Z","shell.execute_reply":"2022-04-27T19:53:11.840212Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"This dataset also has __4__ columns; __userId__, __movieId__, __tag__ and __timestamp__.\n\n__tag__ also features here as it did in `genome_tags` dataset. Is there a difference? or are they the same? This we will explore in the `Exploratory Data Analysis` section.\n\nNext, we will take a look at the dimensions of the dataset","metadata":{}},{"cell_type":"code","source":"tags.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.842637Z","iopub.execute_input":"2022-04-27T19:53:11.842958Z","iopub.status.idle":"2022-04-27T19:53:11.853592Z","shell.execute_reply.started":"2022-04-27T19:53:11.842925Z","shell.execute_reply":"2022-04-27T19:53:11.852774Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"There are over __1 Million__ rows of data in this dataset.\n\nLet's get some more information.","metadata":{}},{"cell_type":"code","source":"tags.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.857128Z","iopub.execute_input":"2022-04-27T19:53:11.857373Z","iopub.status.idle":"2022-04-27T19:53:11.933234Z","shell.execute_reply.started":"2022-04-27T19:53:11.857342Z","shell.execute_reply":"2022-04-27T19:53:11.932254Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"There are __3__ numerical columns and __1__ categorical column which is the __tag__ column. \n\nThe tag column also seem to be missing some values. Let's confirm this.","metadata":{}},{"cell_type":"code","source":"# Extract the number of missing data and the percentage\n# of missing data and concatenate into one dataframe\ntags_missing_data = pd.concat([tags.isnull().sum(), round(tags.isnull().sum()/tags.shape[0] * 100, 3)], axis=1)\ntags_missing_data.columns = ['missing_count', 'missing_percentage'] # rename columns\ntags_missing_data","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.934477Z","iopub.execute_input":"2022-04-27T19:53:11.934759Z","iopub.status.idle":"2022-04-27T19:53:11.962721Z","shell.execute_reply.started":"2022-04-27T19:53:11.934701Z","shell.execute_reply":"2022-04-27T19:53:11.962050Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"There's a very very negligible number of missing data at __0.001%__. That won't do us much harm.","metadata":{}},{"cell_type":"markdown","source":"And with that, we've come to the end of the `Sneak Peek into Loaded Data Section`. \n\nHere, we had a brief overview of the datasets we intend to work with and what needs to be done to get our data ready for further analysis and modeling.\n\n\nDuring the sneak peeking, we noticed that there are some columns that aren't just right. In the next section, we will be making use of a bunch of techniques to prepare the data into the right and useable formats in a process known as `Data Cleaning`","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4><a href='#tbl-contents'>Back to table of contents</a></h4>\n</div>\n<div id='data-cleaning'>\n    <h2 style='text-transform: uppercase;'>data cleaning</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### WHAT IS DATA CLEANING?\n\nThe process of repairing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data from a dataset is known as `Data Cleaning`. There are numerous opportunities for data to be duplicated or mislabeled when combining multiple data sources. If the data is incorrect, the results and algorithms are untrustworthy, even if they appear to be correct.\n\nThis will be done for every datasets we intend to work with.","metadata":{}},{"cell_type":"markdown","source":"#### 1. IMDB DATA","metadata":{}},{"cell_type":"markdown","source":"First, let's make a copy of the dataset","metadata":{}},{"cell_type":"code","source":"# make a copy of the dataset\nimdb_copy = imdb.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.963768Z","iopub.execute_input":"2022-04-27T19:53:11.964198Z","iopub.status.idle":"2022-04-27T19:53:11.969216Z","shell.execute_reply.started":"2022-04-27T19:53:11.964150Z","shell.execute_reply":"2022-04-27T19:53:11.968651Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"...then let's remind ourselves what messy data we have on our hands","metadata":{}},{"cell_type":"code","source":"imdb_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:53:11.970296Z","iopub.execute_input":"2022-04-27T19:53:11.970977Z","iopub.status.idle":"2022-04-27T19:53:11.988412Z","shell.execute_reply.started":"2022-04-27T19:53:11.970926Z","shell.execute_reply":"2022-04-27T19:53:11.987789Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"For the *title_cast* column, we will split each row on '|' and we want to keep the firstname and lastname of the actors together so we join the first names and last names with an underscore('_'), same for the directors\n\nFor *plot_keyword*, we will be replacing '|' with a space ' '\n\nFor the *budget* column, we will remove the commas(',') and extract the digits into a separate column and the currency symbols into another","metadata":{}},{"cell_type":"code","source":"# check for duplicated data\n\nimdb_copy.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:11:47.238560Z","iopub.execute_input":"2022-04-27T20:11:47.239305Z","iopub.status.idle":"2022-04-27T20:11:47.281583Z","shell.execute_reply.started":"2022-04-27T20:11:47.239262Z","shell.execute_reply":"2022-04-27T20:11:47.280931Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"There are no duplicated data in the IMDB Dataset","metadata":{}},{"cell_type":"markdown","source":"#### Agenda\n1. Fill null values with ''(blank) for both *title_cast* and *plot_keywords*\n2. Split *title_cast* on '|', join the first and last names of each actor with '_'\n3. Replace '|' with ' '(a space) in *plot_keywords*\n4. Replace ',' in _budget_ column with ''(nothing)\n5. Extract currency symbol into another column called 'symbol' and amount into 'budget_amount'\n6. Convert the dtype of budget to 'float32'","metadata":{}},{"cell_type":"code","source":"# Agenda 1: Fill null values with ''(blank)\n\nimdb_copy['title_cast'] = imdb_copy['title_cast'].fillna('')\nimdb_copy['director'] = imdb_copy['director'].fillna('')\nimdb_copy['plot_keywords'] = imdb_copy['plot_keywords'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:01:12.779514Z","iopub.execute_input":"2022-04-27T20:01:12.779885Z","iopub.status.idle":"2022-04-27T20:01:12.801813Z","shell.execute_reply.started":"2022-04-27T20:01:12.779846Z","shell.execute_reply":"2022-04-27T20:01:12.800867Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Agenda 2: Split title_cast on '|', join the first and last names of each actor with '_'\n\ndef clean_text(text):\n    \n    # split text on '|'\n    text_split = text.split('|')\n    \n    # replace the space between the actors first name and\n    # lastname with an underscore, convert to lowercase\n    # and then join into a string.\n    text_replace = ' '.join([x.replace(' ', '_') if len(x) > 0 else '' for x in text_split]).lower()\n    \n    # return transformed text\n    return text_replace\n\n# apply clean_text function to each row\nimdb_copy['title_cast'] = imdb_copy['title_cast'].apply(clean_text) \n\n# replace the space between the directors' first name and \n# last names with an underscore, and convert to lowercase\nimdb_copy['director'] = imdb_copy['director'].apply(lambda row: row.replace(' ', '_').lower())","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:01:16.961730Z","iopub.execute_input":"2022-04-27T20:01:16.962190Z","iopub.status.idle":"2022-04-27T20:01:17.032983Z","shell.execute_reply.started":"2022-04-27T20:01:16.962158Z","shell.execute_reply":"2022-04-27T20:01:17.031822Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Agenda 3: Replace '|' with ' '(a space) in plot_keywords\n\n# select column and use .apply() with the lambda function to replace \"|\" character\n# with a space.\nimdb_copy['plot_keywords'] = imdb_copy['plot_keywords']\\\n                            .apply(lambda row: row.replace('|', ' '))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:01:27.179863Z","iopub.execute_input":"2022-04-27T20:01:27.180183Z","iopub.status.idle":"2022-04-27T20:01:27.200226Z","shell.execute_reply.started":"2022-04-27T20:01:27.180146Z","shell.execute_reply":"2022-04-27T20:01:27.199549Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Agenda 4: Replace ',' in budget column with ''(nothing)\n\n# replace commas in budget amount with blanks\n# excluding rows with values np.Nan\nimdb_copy['budget'] = imdb_copy['budget']\\\n                                .apply(lambda row: row.replace(',', '')\\\n                                      if type(row) == str else row)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:02:03.941057Z","iopub.execute_input":"2022-04-27T20:02:03.941367Z","iopub.status.idle":"2022-04-27T20:02:03.961152Z","shell.execute_reply.started":"2022-04-27T20:02:03.941336Z","shell.execute_reply":"2022-04-27T20:02:03.960000Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"Removing the commas before using regex to extract amounts and currency symbols is important because trying to extract the digits first before removing commas will result in extracting only the first few digits found before a comma, leaving us with only the million figure without the zeros (i.e 35 instead of 35000000)","metadata":{}},{"cell_type":"code","source":"# Agenda 5: Extract currency symbol into another column called 'symbol' and amount into 'budget_amount'\n\npattern = '[0-9]+' # pattern to search for digits\nsymbol = '[$A-Za-z]+' # patter to search for alpha characters\n\n# extract the budget amount from the budget column \n# and put it in `budget_amount column \n# excluding rows with values np.Nan\nimdb_copy['budget_amount'] = imdb_copy['budget']\\\n                            .apply(lambda row: re.search(pattern, row)\\\n                                   .group() if type(row) == str else row)\n\n# extract the currency symbol from the budget column\n# and put it in `symbol` column\n# excluding rows with values np.Nan\nimdb_copy['symbol'] = imdb_copy['budget']\\\n                            .apply(lambda row: re.search(symbol, row)\\\n                                   .group() if type(row) == str else row)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:02:42.736248Z","iopub.execute_input":"2022-04-27T20:02:42.736875Z","iopub.status.idle":"2022-04-27T20:02:42.796081Z","shell.execute_reply.started":"2022-04-27T20:02:42.736835Z","shell.execute_reply":"2022-04-27T20:02:42.795193Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Agenda 6: Convert the dtype of budget to 'float32'\n\nimdb_copy['budget_amount'] = imdb_copy['budget_amount'].astype('float32')","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:02:46.298520Z","iopub.execute_input":"2022-04-27T20:02:46.299331Z","iopub.status.idle":"2022-04-27T20:02:46.310132Z","shell.execute_reply.started":"2022-04-27T20:02:46.299278Z","shell.execute_reply":"2022-04-27T20:02:46.309064Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"imdb_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:03:07.521193Z","iopub.execute_input":"2022-04-27T20:03:07.521623Z","iopub.status.idle":"2022-04-27T20:03:07.541547Z","shell.execute_reply.started":"2022-04-27T20:03:07.521566Z","shell.execute_reply":"2022-04-27T20:03:07.540224Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"So far, we have been able to clean the imdb data to a certain degree and I am currently happy with the result. \n\nThe redundant columns will be handled during `Feature Engineering`\n\nNext we will be cleaning the Movies Dataset","metadata":{}},{"cell_type":"markdown","source":"#### 2. MOVIES DATA","metadata":{}},{"cell_type":"code","source":"# make a copy\n\nmovies_copy = movies.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:08:39.209767Z","iopub.execute_input":"2022-04-27T20:08:39.210114Z","iopub.status.idle":"2022-04-27T20:08:39.217783Z","shell.execute_reply.started":"2022-04-27T20:08:39.210079Z","shell.execute_reply":"2022-04-27T20:08:39.216764Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"movies_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:08:51.009463Z","iopub.execute_input":"2022-04-27T20:08:51.010624Z","iopub.status.idle":"2022-04-27T20:08:51.022053Z","shell.execute_reply.started":"2022-04-27T20:08:51.010560Z","shell.execute_reply":"2022-04-27T20:08:51.021313Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# check for duplicated data\n\nmovies.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:13:24.659807Z","iopub.execute_input":"2022-04-27T20:13:24.660069Z","iopub.status.idle":"2022-04-27T20:13:24.695188Z","shell.execute_reply.started":"2022-04-27T20:13:24.660042Z","shell.execute_reply":"2022-04-27T20:13:24.694214Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"There are no duplicated data","metadata":{}},{"cell_type":"markdown","source":"The Agenda for this dataset will be;\n\n1. Split the title into 2 columns, title and year_released\n2. Replace '|' with ' '(space) in the genres column","metadata":{}},{"cell_type":"code","source":"# Agenda 1: Split the title into 2 columns, title and year_released\n\nmovies['year'] = movies['title'].apply(lambda x: x[-7:].replace('(', '').replace(')', ''))\nmovies['title'] = movies['title'].apply(lambda x: x[:-7].strip().lower())","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:16:56.311876Z","iopub.execute_input":"2022-04-27T20:16:56.312235Z","iopub.status.idle":"2022-04-27T20:16:56.394741Z","shell.execute_reply.started":"2022-04-27T20:16:56.312196Z","shell.execute_reply":"2022-04-27T20:16:56.394090Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Agenda 2: Replace '|' with ' '(space) in the genres column\n\nmovies['genres'] = movies['genres'].apply(lambda row: row.replace('|', ' ').lower())","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:18:37.392474Z","iopub.execute_input":"2022-04-27T20:18:37.392827Z","iopub.status.idle":"2022-04-27T20:18:37.435850Z","shell.execute_reply.started":"2022-04-27T20:18:37.392792Z","shell.execute_reply":"2022-04-27T20:18:37.434766Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"movies.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:19:10.158622Z","iopub.execute_input":"2022-04-27T20:19:10.159004Z","iopub.status.idle":"2022-04-27T20:19:10.171083Z","shell.execute_reply.started":"2022-04-27T20:19:10.158965Z","shell.execute_reply":"2022-04-27T20:19:10.169912Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"Everything seems to have worked out fine, and I am happy with the results\n\n\nNext, Meta_data","metadata":{}},{"cell_type":"markdown","source":"#### 3. META DATA","metadata":{}},{"cell_type":"markdown","source":"Recall that meta_data is a robust agglomerated version of the IMDB and Movies Datasets. The meta_data meanwhile, does not contain values for some of the data points that are available in Movies and IMDB datasets, for instance, some movie budgets data available in the IMDB dataset are not available in the Meta dataset.\n\nAfter cleaning, we will take a look at how to maximally utilise all datasets involved","metadata":{}},{"cell_type":"code","source":"meta_copy = meta_data.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:37:30.150457Z","iopub.execute_input":"2022-04-27T20:37:30.150849Z","iopub.status.idle":"2022-04-27T20:37:30.173999Z","shell.execute_reply.started":"2022-04-27T20:37:30.150809Z","shell.execute_reply":"2022-04-27T20:37:30.173166Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"meta_copy.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:37:41.039820Z","iopub.execute_input":"2022-04-27T20:37:41.040497Z","iopub.status.idle":"2022-04-27T20:37:41.066686Z","shell.execute_reply.started":"2022-04-27T20:37:41.040458Z","shell.execute_reply":"2022-04-27T20:37:41.065792Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# displaying the truncated columns\nmeta_copy.loc[:, 'overview': 'release_date'].head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:47:26.307893Z","iopub.execute_input":"2022-04-27T20:47:26.308415Z","iopub.status.idle":"2022-04-27T20:47:26.326513Z","shell.execute_reply.started":"2022-04-27T20:47:26.308362Z","shell.execute_reply":"2022-04-27T20:47:26.325897Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"This is a very messy data. Let's get to work","metadata":{}},{"cell_type":"markdown","source":"Let's set our cleaning agenda for this dataset;\n\n1. Column *Belongs_to_collection* should be converted to a boolean field (True or False)\n2. Extract the genres from column _genres_ as they are store in dictionaries in a list\n3. Extract the digits from the *imdb_id* column while renaming the column to _imdbId_\n4. Compare *original_title* and *title*\n5. Convert *popularity* to 'float32'\n6. Extract *production_companies* and *production_countries*\n7. Extract _year_ from *release_data*\n8. Extract _language_ from *spoken_languages*\n9. Compare _language_ with *original_language*","metadata":{}},{"cell_type":"code","source":"# Agenda 1: Column Belongs_to_collection should be converted to a boolean field (True or False)\n\nmeta_copy['belongs_to_collection'] = meta_copy['belongs_to_collection']\\\n                                        .apply(lambda row: True if type(row) != float else False )","metadata":{"execution":{"iopub.status.busy":"2022-04-27T21:01:12.510319Z","iopub.execute_input":"2022-04-27T21:01:12.510870Z","iopub.status.idle":"2022-04-27T21:01:12.545051Z","shell.execute_reply.started":"2022-04-27T21:01:12.510808Z","shell.execute_reply":"2022-04-27T21:01:12.544031Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# engineer a_list_actors, b_list_actors, c_list_actors\n\ndef classify(df, col, columns=[], threshold=[]):\n    most_featured = feature_importance(df[col])\n    most_featured_df = pd.DataFrame.from_dict(most_featured.items())\n    most_featured_df.columns = columns\n\n    print('feature extraction complete')\n\n    a_list_actors = most_featured_df[columns[0]][most_featured_df[columns[1]] >= threshold[0]].tolist()\n    b_list_actors = most_featured_df[columns[0]][(most_featured_df[columns[1]] >= threshold[1]) & (most_featured_df[columns[1]] < threshold[0])].tolist()\n    c_list_actors = most_featured_df[columns[0]][most_featured_df[columns[1]] < threshold[1]].tolist()\n\n    a_list_count = []\n    b_list_count = []\n    c_list_count = []\n\n    for row in df[col]:\n        a_counter = 0\n        b_counter = 0\n        c_counter = 0\n\n        for item in row:\n            if item in a_list_actors:\n                a_counter += 1\n            if item in b_list_actors:\n                b_counter += 1\n            if item in c_list_actors:\n                c_counter += 1\n\n        a_list_count.append(a_counter)\n        b_list_count.append(b_counter)\n        c_list_count.append(c_counter)\n\n    return (a_list_count, b_list_count, c_list_count)\n\ndef feature_importance(series):\n    items = aggregate(series)\n    frequency = {}\n\n    for item in items:\n        if item in frequency:\n            frequency[item] += 1\n        else:\n            frequency[item] = 1\n    \n    sorted_freq = {k: v for k, v in sorted(frequency.items(), reverse=True, key=lambda item: item[1])}\n\n    return sorted_freq\n\ndef aggregate(series):\n    aggregate = []\n    for i in series:\n        for j in i:\n            aggregate.append(j)\n  \n    return aggregate\n\ndef split_text(text, sep):\n    split_text = text.split(sep)\n    return split_text\n\ndef join_lists(list_):\n    return ' '.join(list_)\n\ndef stringify(num):\n    num = str(num)\n    return num\n\ndef fill_na(df, cols=[]):\n    for col in cols:\n        df[col] = df[col].fillna(df[col].median())\n  \n    return df","metadata":{"id":"iE-qn_PPDwCQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Based On: Director and Title Cast | Genres and Plot Keywords","metadata":{}},{"cell_type":"code","source":"# genre, director, title_cast\npkt = movies.merge(imdb, how='left', on='movieId')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pkt[['plot_keywords', 'genres', 'title_cast']] = pkt[['plot_keywords', 'genres', 'title_cast']].fillna('')\npkt['plot_keywords'] = pkt['plot_keywords'].apply(split_text, args=('|', ))\npkt['genres'] = pkt['genres'].apply(split_text, args=('|', ))\npkt['title_cast'] = pkt['title_cast'].apply(split_text, args=('|', ))\npkt['director'] = pkt['director'].fillna('')\npkt['director'] = pkt['director'].apply(lambda x: [x, x, x])\npkt[['plot_keywords', 'genres', 'title_cast']] = pkt[['plot_keywords', 'genres', 'title_cast']].fillna('')\npkt['plot_keywords'] = pkt['plot_keywords'].apply(lambda x: \" \".join(x).lower() if len(x) > 0 else '')\npkt['title_cast'] = pkt['title_cast'].apply(lambda x: \" \".join(x).lower() if len(x) > 0 else '')\npkt['genres'] = pkt['genres'].apply(lambda x: \" \".join(x).lower() if len(x) > 0 else '')\npkt['director'] = pkt['director'].apply(lambda x: \" \".join(x).lower())\npkt['plot_keywords'] = pkt['plot_keywords'].fillna('')\npkt['soup'] = pkt['title_cast'] + \" \" + pkt['director'] + pkt['plot_keywords']\npkt.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Title Cast and Director","metadata":{}},{"cell_type":"code","source":"pkt = pkt[['soup', 'movieId']]\npkt = convert_columns(pkt)\npkt_movies = pkt.merge(movies[['movieId', 'title']], on='movieId')\npkt_movies.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pkt_movies['title'] = pkt_movies['title'].apply(lambda x: x[:-7])\npkt_movies.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pkt_movies['soup'] = pkt_movies['soup'].astype('O').fillna('')\npkt_movies = convert_columns(pkt_movies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 1),min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(pkt_movies['soup'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix = tfidf_matrix.astype('float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pkt_movies  = pkt_movies.reset_index()\ntitles = pkt_movies['title']\nindices = pd.Series(pkt_movies.index, index=pkt_movies['title'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cosine_sim.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_recommendations(title):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:31]\n    movie_indices = [i[0] for i in sim_scores]\n    return titles.iloc[movie_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_recommendations('Balto').head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.title[movies.title.str.startswith('B')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collaborative Filtering","metadata":{}},{"cell_type":"code","source":"reader = Reader()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['userId'] = train['userId'].apply(stringify)\ntrain['movieId'] = train['movieId'].apply(stringify)\n# train['Id'] = train['userId'] + \"_\" + train['movieId']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], reader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sim_options = {'name': 'cosine',\n               'user_based': False  # compute  similarities between items\n               }\n# knn =KNNBasic(sim_options=sim_options)\nsvd = SVDpp()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset, testset = train_test_split(data, test_size=.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svd.fit(trainset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = svd.test(testset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from surprise import accuracy\n\naccuracy.rmse(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['userId'] = test['userId'].apply(stringify)\ntest['movieId'] = test['movieId'].apply(stringify)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from surprise import dump\nimport os\n\nmodel_filename = \"/kaggle/working/model.pickle\"\n\nprint(\">>> starting dump\")\nfile_name = os.path.expanduser(model_filename)\ndump.dump(file_name, algo=svd)\nprint('dump done')\nprint(file_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def item_rating(user, item):\n    uid = str(user)\n    iid = str(item)\n    model = svd\n    prediction = model.predict(user, item, verbose=False)\n    rating = round(prediction.est, 1)\n    \n    return rating","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['ratings'] = test.apply(lambda row: item_rating(row['userId'], row['movieId']), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get relevant tags and merge with genome_tags\n\nthreshold = 0.80 # get genomes with relevance 80% and above\n\nrelevant_genomes = genome_scores[genome_scores.relevance >= threshold]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_tags = genome_tags.merge(relevant_genomes, on='tagId')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie_count = scores_tags.groupby('tag')['movieId'].count().reset_index().rename(columns={'movieId':'movieId_counts'})","metadata":{"id":"N_rNmxxYFvFW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process\n\nmatch_list = []\nratio_list = []\n\nbad_tags = scores_tags.tag.values\ngood_tags = movie_count.tag[movie_count.movieId_counts > 100].values\n\nthreshold = 80\n\nfor b_tag in bad_tags:\n    process_extract = process.extractOne(b_tag, good_tags, scorer=fuzz.token_sort_ratio)\n    match_list.append(process_extract[0])\n    ratio_list.append(process_extract[1])\n\nscores_tags['matches'] = match_list\nscores_tags['match_ratio'] = ratio_list","metadata":{"id":"6pkyMrNCF8ZP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(scores_tags.shape)\nscores_tags.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_tags_threshold = scores_tags[scores_tags.match_ratio >= threshold]\nadjusted_tags = pd.merge(scores_tags[['movieId', 'tag']], filtered_tags_threshold[['tag', 'matches']], on='tag')\ncleaned_tags = adjusted_tags.groupby(['movieId', 'matches'])['tag'].count().reset_index().rename(columns={'tag':'tag_count', 'matches':'tag'})\n\nprint(cleaned_tags.shape)\ncleaned_tags.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_tags = cleaned_tags.groupby('movieId')['tag'].agg(list).reset_index()\n\ncleaned_tags.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### clean imdb data","metadata":{}},{"cell_type":"code","source":"imdb_data = movies.merge(imdb, on='movieId')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data.budget = imdb_data.budget.apply(lambda x: x.replace(',', '') if type(x) == str else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\npattern = '[0-9]+'\nsymbol = '[$A-Za-z]+'\nimdb_data['budget_amount'] = imdb_data.budget.apply(lambda x: re.search(pattern, x).group() if type(x) == str else x)\nimdb_data['symbol'] = imdb_data.budget.apply(lambda x: re.search(symbol, x).group() if type(x) == str else x)\nimdb_data['year'] = imdb_data['title'].apply(lambda x: x[-7:].replace('(', '').replace(')', ''))\nimdb_data['title'] = imdb_data['title'].apply(lambda x: x[:-7].strip())\n# imdb_data['year'] = imdb_data['year'].apply(lambda x: int(x) if x.isnumeric() else np.NaN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data.year = pd.to_numeric(imdb_data.year, errors='coerce')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data.budget_amount = imdb_data.budget_amount.astype('float')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country2curr = pd.read_csv('/kaggle/input/countries-currency/4680482-b61a5bdf5f3d5c69399f9d9e592c4896fd0dc53c/country-code-to-currency-code-mapping.csv')\ncurr_conv = pd.read_csv('/kaggle/input/exchange-rate-2020-cleaned/API_PA.NUS.FCRF_DS2_en_csv_v2_3930759.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_list = [np.nan, 'ITL', 'BGL','TRL', 'FRF', 'VEB', 'BEF', 'PTE', 'DEM', 'ESP', '$', 'RUR', 'SIT', 'NLG', 'FIM', 'ATS']\nmap_list = [np.nan,'Italy', 'Bulgaria', 'Turkey', 'France', 'Venezuela', 'Belgium', 'Portugal', 'Germany', 'Spain', 'USA', 'Russia', 'Slovenia', 'Netherlands', 'Finland', 'Austria']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data['country'] = imdb_data['symbol'].map(dict(zip(country2curr.Code.tolist()+missing_list, country2curr.Country.tolist()+map_list)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace(text):\n    if text == 'Saint Pierre and Miquelon':\n        st = 'Europe'\n    else:\n        st = text\n    return st","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data['country'] = imdb_data['country'].apply(replace)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directors = list(imdb_data['director'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\ngroup = []\nfor k in directors:\n    sub_data = imdb_data[imdb_data.director == k]\n    sub_data['budget_amount'].fillna(sub_data['budget_amount'].dropna().mean(), inplace=True)\n    mode = stats.mode(sub_data['country'].dropna().tolist())[0][0] if len(sub_data['country'].dropna().tolist()) > 0 else np.nan\n    sub_data['country'].fillna(mode, inplace=True)\n    group.append(sub_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade = pd.concat(group).sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.isnull().sum()/imdb_data_remade.shape[0] * 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb.shape, imdb_data.shape, imdb_data_remade.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.drop(['symbol', 'plot_keywords', 'budget', 'title'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade['budget_amount'].fillna(np.median(imdb_data_remade['budget_amount'].dropna().tolist()), inplace=True)\nimdb_data_remade['country'].fillna(stats.mode(imdb_data_remade['country'].dropna().tolist())[0][0], inplace=True)\nimdb_data_remade['runtime'].fillna(np.mean(imdb_data_remade['budget_amount'].dropna().tolist()), inplace=True)\nimdb_data_remade['year'].fillna(np.median(imdb_data_remade['year'].dropna().tolist()), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade['conversion_rate/USD'] = imdb_data_remade['country'].map(dict(zip(curr_conv['Country Name'].tolist()+['USA'], curr_conv['2020'].tolist()+[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade['conversion_rate/USD'].fillna(0.88, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade['usd_budget'] = imdb_data_remade['budget_amount'] / imdb_data_remade['conversion_rate/USD']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.drop(['budget_amount', 'conversion_rate/USD'], axis=1, inplace=True)\nimdb_data_remade.drop(imdb_data_remade.runtime[imdb_data_remade.runtime > 420].index, inplace=True)\nimdb_data_remade.drop(imdb_data_remade[imdb_data_remade.usd_budget > 300000000].index, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.usd_budget = imdb_data_remade.usd_budget.apply(lambda x: \\\n                                                                1000000 if x == 0 else \\\n                                                               x*1000000 if x <= 10 else \\\n                                                               x*100000 if x <= 100 else \\\n                                                               x*10000 if x <= 1000 else \\\n                                                               x*1000 if x <= 10000 else \\\n                                                               x*100 if x <= 100000 else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.drop(list(imdb_data_remade[imdb_data_remade.runtime<20].index), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade['title_cast'] = imdb_data_remade['title_cast'].apply(split_text, args=(\"|\", ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade['genres'] = imdb_data_remade['genres'].apply(split_text, args=(\"|\", ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a list of our conditions\nconditions = [\n    (imdb_data_remade['usd_budget'] > imdb_data_remade['usd_budget'].quantile(.75)),\n    (imdb_data_remade['usd_budget'] >= imdb_data_remade['usd_budget'].quantile(.25)),\n    (imdb_data_remade['usd_budget'] < imdb_data_remade['usd_budget'].quantile(.25)),\n    ]\n\n# create a list of the values we want to assign for each condition\nvalues = ['blockbuster', 'average', 'low_budget']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\nimdb_data_remade['level'] = np.select(conditions, values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a, b, c = classify(imdb_data_remade, 'title_cast', columns=['actor', 'appearance'], threshold=[30, 15])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade['alist'], imdb_data_remade['blist'], imdb_data_remade['clist'] = a, b, c","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.drop('title_cast', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_remade.drop('index',axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\ncount_vector=cv.fit_transform(' '.join(x) for x in imdb_data_remade.genres)\ncount_vect_df = pd.DataFrame(count_vector.todense(), columns = cv.get_feature_names_out())\nimdb_data_rem = pd.concat([imdb_data_remade,count_vect_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_rem.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_rem.drop('genres', axis=1, inplace=True)\nimdb_data_rem.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exclude_list = ['movieId', 'country', 'usd_budget', 'level', 'director', 'year']\ninclude_list = [x for x in imdb_data_rem.columns if x not in exclude_list]\n\nimdb_data_rem[include_list] = imdb_data_rem[include_list].astype('int8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_rem = convert_columns(imdb_data_rem)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_rem.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"uQvzZ2AP1_Uh"},"execution_count":null,"outputs":[]}]}